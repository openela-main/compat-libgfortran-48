Backport of trunk revisions: r214898, r221047, r223367, r223368, r223369, r223393, r223395, r223396, r223397, r223398, r223399, r223400, r223403, r224227, r224867, r224868, r224869, r224870, r224871, r224872, r224873, r224874, r226671, r226672, r227058, r227635, r227636, r227637, r227780, r231153, r231154, r231155, r231156, r231157, r231158, r231159, r231809, r232972, r232973, r233548, r233549, r233550, r233552, r233553, r233554, r233555, r233556, r233623, r236067

2016-05-10  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.md ("*vec_cmp<insn_cmp>df_cconly")
	("*fixuns_truncdfdi2_z13")
	("*fixuns_trunc<FP:mode><GPR:mode>2_z196")
	("*fix_truncdfdi2_bfp_z13", "*floatunsdidf2_z13")
	("*extendsfdf2_z13"): Replace TARGET_Z13 with TARGET_VX.

2016-02-23  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* gcc.target/s390/md/movstr-2.c: Move and rename to ...
	* gcc.target/s390/vector/stpcpy-1.c: ... this one.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vector.md: Add missing commutative operand markers
	to the patterns which qualify for one.
	* config/s390/vx-builtins.md: Likewise.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vector.md (VI, VI_QHS): Add single element vector
	types to mode iterators.
	(vec_double): ... and mode attribute.
	* config/s390/vx-builtins.md (non_vec_int): Likewise.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vector.md ("<ti*>add<mode>3", "<ti*>sub<mode>3"):
	Change the predicate of op2 from nonimmediate to general and let
	reload fix it if necessary.

	* gcc.target/s390/vector/int128-1.c: New test.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vecintrin.h (vec_sub_u128): Define missing macro.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_expand_vcond): Use the compare operand
	mode.

	* gcc.target/s390/vector/vec-vcond-1.c: New test.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390-protos.h: Add s390_expand_vec_movstr prototype.
	* config/s390/s390.c (s390_expand_vec_movstr): New function.
	* config/s390/s390.md ("movstr<P:mode>"): Call
	s390_expand_vec_movstr.

	* gcc.target/s390/md/movstr-2.c: New test.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.md: Add missing output modifier for operand 1
	to print it as address properly.

2016-02-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/2827.md: Rename ooo_* insn attributes to zEC12_*.
	* config/s390/2964.md: New file.
	* config/s390/s390.c (s390_get_sched_attrmask): Use the right set
	of insn grouping attributes depending on the CPU level.
	(s390_get_unit_mask): New function.
	(s390_sched_score): Remove the OOO from the scheduling macros.
	Add loop to calculate a score for the instruction mix.
	(s390_sched_reorder): Likewise plus improve debug output.
	(s390_sched_variable_issue): Rename macros as above.  Calculate
	the unit distances after actually scheduling an insn.  Improve
	debug output.
	(s390_sched_init): Clear last_scheduled_unit_distance array.
	* config/s390/s390.md: Include 2964.md.

2016-01-29  Dominik Vogt  <vogt@linux.vnet.ibm.com>

	* config/s390/s390-c.c (s390_resolve_overloaded_builtin): Format
	declaration name with %qs and print it in both error messages.
	Also fix indentation.

2016-01-29  Dominik Vogt  <vogt@linux.vnet.ibm.com>

	PR other/69006
	* config/s390/s390-c.c (s390_resolve_overloaded_builtin): Remove
	trailing blank line from error message.

2015-12-18  Robin Dapp  <rdapp@linux.vnet.ibm.com>

	* config/s390/predicates.md: Change and rename
	constm1_operand to all_ones_operand
	* config/s390/s390.c (s390_expand_vcond): Use all_ones_operand
	* config/s390/vector.md: Likewise

2015-12-02  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/predicates.md (const_mask_operand): New predicate.
	* config/s390/s390-builtins.def: Set a smaller bitmask for a few builtins.
	* config/s390/vector.md: Change predicate from immediate_operand
	to either const_int_operand or const_mask_operand.  Add special
	insn conditions on patterns which have to exclude certain values.
	* config/s390/vx-builtins.md: Likewise.

2015-12-02  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vector.md ("*vec_set<mode>"): Change shift count
	mode from DI to SI.

2015-12-02  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390-builtin-types.def: New builtin types added.
	* config/s390/s390-builtins.def: Add s390_vec_splat_* definitions.
	* config/s390/s390.c (s390_expand_builtin): Always truncate
	constants to the mode in the pattern.
	* config/s390/vecintrin.h: Let the vec_splat_* macros point to the
	respective builtin __builtin_s390_vec_splat_*.

	* gcc.target/s390/zvector/vec-splat-2.c: New test.

2015-12-02  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390-builtin-types.def: Sort builtin types.

2015-12-02  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390-c.c (s390_get_vstring_flags): Invert the
	condition for the RT flag.

2015-12-02  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/constraints.md ("jKK"): New constraint.
	* config/s390/s390.c (tm-constrs.h): Include for
	satisfies_constraint_*.
	(s390_legitimate_constant_p): Allow jKK constants.  Use
	satisfies_constraint_* also for the others.
	(legitimate_reload_vector_constant_p): Likewise.
	(print_operand): Allow h output modifier on vectors.
	* config/s390/vector.md ("mov<mode>"): Add vrepi.

	* gcc.target/s390/vector/vec-vrepi-1.c: New test.

2015-12-02  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vector.md ("*vec_splats<mode>"): Fix constraint
	latter I->K.

	* gcc.target/s390/zvector/vec-splat-1.c: New test.

2015-09-15  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

        * config/s390/s390.c (s390_const_operand_ok): Add missing
	brackets.

2015-09-10  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_contiguous_bitmask_vector_p): Reject if
	the vector element is bigger than 64 bit.

	* gcc.target/s390/vector/vec-genbytemask-1.c: Add check for V1TI
	initialization with a byte mask.  No change expected here.
	* gcc.target/s390/vector/vec-genmask-1.c: Fix whitespace.
	* gcc.target/s390/vector/vec-genmask-2.c: Add check for V1TI
	initialization with contigious bitmask.  Literal pool is expectd
	to be used here.

2015-09-10  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vx-builtins.md ("vec_vmal<mode>", "vec_vmah<mode>")
	("vec_vmalh<mode>"): Change mode iterator from VI_HW to VI_HW_QHS.

2015-09-10  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c: Add V1TImode to constant pool modes.

2015-08-21  Dominik Vogt  <vogt@linux.vnet.ibm.com>

	* config/s390/s390-builtins.def: Fix value range of vec_load_bndry.

	* gcc.target/s390/zvector/vec-load_bndry-1.c: New test.

2015-08-06  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_expand_tbegin): Expand either
	tbegin_1_z13 or tbegin_1 depending on VX flag.
	* config/s390/s390.md ("tbegin_1_z13"): New expander.

	* gcc.target/s390/htm-builtins-z13-1.c: New test.

2015-08-06  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.opt: Clarify description for -mzvector
	* doc/invoke.texi: Add documentation for -mhtm, -mvx, and
	-mzvector.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vx-builtins.md
	("vec_scatter_element<mode>_<non_vec_int>")
	("vec_scatter_element<V_HW_64:mode>_SI"): Replace gf mode
	attribute with bhfgq.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390-builtins.def: Fix vpopct instruction comments.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390-builtin-types.def: Add flag to indicate the
	options under which the function type is needed.
	* config/s390/s390-builtins.def: Add flag to indicate the options
	under which the builtin is enabled.
	* config/s390/s390-builtins.h: Add flags parameter to macro
	definitions.
	(bflags_for_builtin): New function.
	(flags_for_builtin): Renamed to ...
	(opflags_for_builtin): ... this.
	* config/s390/s390-c.c (s390_resolve_overloaded_builtin): Rename
	flags_for_builtin to bflags_for_builtin and
	flags_overloaded_builtin_var to opflags_overloaded_builtin_var.
	* config/s390/s390.c: Add initialization of bflags_builtin and
	opflags_builtin arrays.
	Remove code for flags_builtin.
	(s390_init_builtins): Only create builtin function types if one of
	their flags is active.
	Only create builtins if all of their flags are active.
	(s390_expand_builtin): Rename flags_for_builtin to
	opflags_for_builtin.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/vecintrin.h: Remove internal builtins.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_secondary_reload): Fix check for
	GENERAL_REGS register class.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_support_vector_misalignment): Call
	default implementation for !TARGET_VX.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_legitimate_constant_p): Add
	TARGET_VX check.

2015-06-24  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_vector_abi): New variable definition.
	(s390_check_type_for_vector_abi): New function.
	(TARGET_ASM_FILE_END): New macro definition.
	(s390_asm_file_end): New function.
	(s390_function_arg): Call s390_check_type_for_vector_abi.
	(s390_gimplify_va_arg): Likewise.
	* configure: Regenerate.
	* configure.ac: Check for .gnu_attribute Binutils feature.

	* gcc.target/s390/vector/vec-abi-1.c: Add gnu attribute check.
	* gcc.target/s390/vector/vec-abi-attr-1.c: New test.
	* gcc.target/s390/vector/vec-abi-attr-2.c: New test.
	* gcc.target/s390/vector/vec-abi-attr-3.c: New test.
	* gcc.target/s390/vector/vec-abi-attr-4.c: New test.
	* gcc.target/s390/vector/vec-abi-attr-5.c: New test.
	* gcc.target/s390/vector/vec-abi-attr-6.c: New test.

2015-06-08  Jakub Jelinek  <jakub@redhat.com>

	* genattrtab.c (insn_alternatives): Change type from int *
	to uint64_t *.
	(check_attr_test): Shift ((uint64_t) 1) instead of 1 up.
	(get_attr_value): Change type of num_alt to uint64_t.
	(compute_alternative_mask): Change return type from
	int to uint64_t, shift ((uint64_t) 1) instead of 1 up.
	(make_alternative_compare, mk_attr_alt): Change argument type
	from int to uint64_t.
	(simplify_test_exp): Change type of i from int to uint64_t.
	Shift ((uint64_t) 1) instead of 1 up.
	(main): Adjust oballocvec first argument from int to uint64_t.
	Shift ((uint64_t) 1) instead of 1 up.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* lib/target-supports.exp: Vector do not always have natural
	alignment on s390*.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* gcc.dg/tree-ssa/gen-vect-11b.c: Disable vector instructions on
	s390*.
	* gcc.dg/tree-ssa/gen-vect-11c.c: Likewise.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* gcc.target/s390/zvector/vec-dbl-math-compile-1.c: New test.
	* gcc.target/s390/zvector/vec-genbytemask-1.c: New test.
	* gcc.target/s390/zvector/vec-genmask-1.c: New test.
	* gcc.target/s390/zvector/vec-lcbb-1.c: New test.
	* gcc.target/s390/zvector/vec-overloading-1.c: New test.
	* gcc.target/s390/zvector/vec-overloading-2.c: New test.
	* gcc.target/s390/zvector/vec-overloading-3.c: New test.
	* gcc.target/s390/zvector/vec-overloading-4.c: New test.
	* gcc.target/s390/zvector/vec-test-mask-1.c: New test.
	* gcc.target/s390/zvector/vec-elem-1.c: New test.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config.gcc: Add vecintrin.h to extra_headers.  Add s390-c.o to
	c_target_objs and cxx_target_objs.  Add t-s390 to tmake_file.
	* config/s390/s390-builtin-types.def: New file.
	* config/s390/s390-builtins.def: New file.
	* config/s390/s390-builtins.h: New file.
	* config/s390/s390-c.c: New file.
	* config/s390/s390-modes.def: Add modes CCVEQANY, CCVH,
	CCVHANY, CCVHU, CCVHUANY, CCVFHANY, CCVFHEANY.
	* config/s390/s390-protos.h (s390_expand_vec_compare_cc)
	(s390_cpu_cpp_builtins, s390_register_target_pragmas): Add
	prototypes.
	* config/s390/s390.c (s390-builtins.h, s390-builtins.def):
	Include.
	(flags_builtin, flags_overloaded_builtin_var, s390_builtin_types)
	(s390_builtin_fn_types, s390_builtin_decls, code_for_builtin): New
	variable definitions.
	(s390_const_operand_ok): New function.
	(s390_expand_builtin): Rewrite.
	(s390_init_builtins): New function.
	(s390_handle_vectorbool_attribute): New function.
	(s390_attribute_table): Add s390_vector_bool attribute.
	(s390_match_ccmode_set): Handle new cc modes CCVH, CCVHU.
	(s390_branch_condition_mask): Generate masks for new modes.
	(s390_expand_vec_compare_cc): New function.
	(s390_mangle_type): Add mangling for vector bool types.
	(enum s390_builtin): Remove.
	(s390_atomic_assign_expand_fenv): Rename constants for sfpc and
	efpc builtins.
	* config/s390/s390.h (TARGET_CPU_CPP_BUILTINS): Call
	s390_cpu_cpp_builtins.
	(REGISTER_TARGET_PRAGMAS): New macro.
	* config/s390/s390.md: Define more UNSPEC_VEC_* constants.
	(insn_cmp mode attribute): Add new CC modes.
	(s390_sfpc, s390_efpc): Rename patterns to sfpc and efpc.
	(lcbb): New pattern definition.
	* config/s390/s390intrin.h: Include vecintrin.h.
	* config/s390/t-s390: New file.
	* config/s390/vecintrin.h: New file.
	* config/s390/vector.md: Include vx-builtins.md.
	* config/s390/vx-builtins.md: New file.S/390 zvector builtin
	support.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390-modes.def: Add new modes CCVEQ, CCVFH, and
	CCVFHE.
	* config/s390/s390.c (s390_match_ccmode_set): Handle new modes.
	(s390_select_ccmode): Likewise.
	(s390_canonicalize_comparison): Swap operands if necessary.
	(s390_expand_vec_compare_scalar): Expand DFmode compare using
	single element vector instructions.
	(s390_emit_compare): Call s390_expand_vec_compare_scalar.
	(s390_branch_condition_mask): Generate CC masks for the new modes.
	* config/s390/s390.md (v0, vf, vd): New mode attributes.
	(VFCMP, asm_fcmp, insn_cmp): New mode iterator and attributes.
	(*vec_cmp<insn_cmp>df_cconly, *fixuns_truncdfdi2_z13)
	(*fix_trunc<BFP:mode><GPR:mode>2_bfp, *floatunsdidf2_z13)
	(*floatuns<GPR:mode><FP:mode>2, *extendsfdf2_z13)
	(*extend<DSF:mode><BFP:mode>2): New insn definition.
	(fix_trunc<BFP:mode><GPR:mode>2_bfp, loatuns<GPR:mode><FP:mode>2)
	(extend<DSF:mode><BFP:mode>2): Turn into expander.
	(floatdi<mode>2, truncdfsf2, add<mode>3, sub<mode>3, mul<mode>3)
	(div<mode>3, *neg<mode>2, *abs<mode>2, *negabs<mode>2)
	(sqrt<mode>2): Add vector instruction.

	* gcc.target/s390/vector/vec-scalar-cmp-1.c: New test.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* gcc.target/s390/s390.exp
	(check_effective_target_vector): New check.
	* gcc.target/s390/vector/vec-abi-1.c: New test.
	* gcc.target/s390/vector/vec-abi-2.c: New test.
	* gcc.target/s390/vector/vec-abi-3.c: New test.
	* gcc.target/s390/vector/vec-abi-4.c: New test.
	* gcc.target/s390/vector/vec-abi-align-1.c: New test.
	* gcc.target/s390/vector/vec-abi-single-1.c: New test.
	* gcc.target/s390/vector/vec-abi-single-2.c: New test.
	* gcc.target/s390/vector/vec-abi-struct-1.c: New test.
	* gcc.target/s390/vector/vec-abi-vararg-1.c: New test.
	* gcc.target/s390/vector/vec-abi-vararg-2.c: New test.
	* gcc.target/s390/vector/vec-clobber-1.c: New test.
	* gcc.target/s390/vector/vec-cmp-1.c: New test.
	* gcc.target/s390/vector/vec-cmp-2.c: New test.
	* gcc.target/s390/vector/vec-dbl-math-compile-1.c: New test.
	* gcc.target/s390/vector/vec-genbytemask-1.c: New test.
	* gcc.target/s390/vector/vec-genbytemask-2.c: New test.
	* gcc.target/s390/vector/vec-genmask-1.c: New test.
	* gcc.target/s390/vector/vec-genmask-2.c: New test.
	* gcc.target/s390/vector/vec-init-1.c: New test.
	* gcc.target/s390/vector/vec-int-math-compile-1.c: New test.
	* gcc.target/s390/vector/vec-shift-1.c: New test.
	* gcc.target/s390/vector/vec-sub-1.c: New test.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/constraints.md (j00, jm1, jxx, jyy, v): New
	constraints.
	* config/s390/predicates.md (const0_operand, constm1_operand)
	(constable_operand): Accept vector operands.
	* config/s390/s390-modes.def: Add supported vector modes.
	* config/s390/s390-protos.h (s390_cannot_change_mode_class)
	(s390_function_arg_vector, s390_contiguous_bitmask_vector_p)
	(s390_bytemask_vector_p, s390_expand_vec_strlen)
	(s390_expand_vec_compare, s390_expand_vcond)
	(s390_expand_vec_init): Add prototypes.
	* config/s390/s390.c (VEC_ARG_NUM_REG): New macro.
	(s390_vector_mode_supported_p): New function.
	(s390_contiguous_bitmask_p): Mask out the irrelevant bits.
	(s390_contiguous_bitmask_vector_p): New function.
	(s390_bytemask_vector_p): New function.
	(s390_split_ok_p): Vector regs don't work either.
	(regclass_map): Add VEC_REGS.
	(s390_legitimate_constant_p): Handle vector constants.
	(s390_cannot_force_const_mem): Handle CONST_VECTOR.
	(legitimate_reload_vector_constant_p): New function.
	(s390_preferred_reload_class): Handle CONST_VECTOR.
	(s390_reload_symref_address):  Likewise.
	(s390_secondary_reload): Vector memory instructions only support
	short displacements.  Rename reload*_nonoffmem* to reload*_la*.
	(s390_emit_ccraw_jump): New function.
	(s390_expand_vec_strlen): New function.
	(s390_expand_vec_compare): New function.
	(s390_expand_vcond): New function.
	(s390_expand_vec_init): New function.
	(s390_dwarf_frame_reg_mode): New function.
	(print_operand): Handle addresses with 'O' and 'R' constraints.
	(NR_C_MODES, constant_modes): Add vector modes.
	(s390_output_pool_entry): Handle vector constants.
	(s390_hard_regno_mode_ok): Handle vector registers.
	(s390_class_max_nregs): Likewise.
	(s390_cannot_change_mode_class): New function.
	(s390_invalid_arg_for_unprototyped_fn): New function.
	(s390_function_arg_vector): New function.
	(s390_function_arg_float): Remove size variable.
	(s390_pass_by_reference): Handle vector arguments.
	(s390_function_arg_advance): Likewise.
	(s390_function_arg): Likewise.
	(s390_return_in_memory): Vector values are returned in a VR if
	possible.
	(s390_function_and_libcall_value): Handle vector arguments.
	(s390_gimplify_va_arg): Likewise.
	(s390_call_saved_register_used): Consider the arguments named.
	(s390_conditional_register_usage): Disable v16-v31 for non-vec
	targets.
	(s390_preferred_simd_mode): New function.
	(s390_support_vector_misalignment): New function.
	(s390_vector_alignment): New function.
	(TARGET_STRICT_ARGUMENT_NAMING, TARGET_DWARF_FRAME_REG_MODE)
	(TARGET_VECTOR_MODE_SUPPORTED_P)
	(TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN)
	(TARGET_VECTORIZE_PREFERRED_SIMD_MODE)
	(TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT)
	(TARGET_VECTOR_ALIGNMENT): Define target macro.
	* config/s390/s390.h (FUNCTION_ARG_PADDING): Define macro.
	(FIRST_PSEUDO_REGISTER): Increase value.
	(VECTOR_NOFP_REGNO_P, VECTOR_REGNO_P, VECTOR_NOFP_REG_P)
	(VECTOR_REG_P): Define macros.
	(FIXED_REGISTERS, CALL_USED_REGISTERS)
	(CALL_REALLY_USED_REGISTERS, REG_ALLOC_ORDER)
	(HARD_REGNO_CALL_PART_CLOBBERED, REG_CLASS_NAMES)
	(FUNCTION_ARG_REGNO_P, FUNCTION_VALUE_REGNO_P, REGISTER_NAMES):
	Add vector registers.
	(CANNOT_CHANGE_MODE_CLASS): Call C function.
	(enum reg_class): Add VEC_REGS, ADDR_VEC_REGS, GENERAL_VEC_REGS.
	(SECONDARY_MEMORY_NEEDED): Allow SF<->SI mode moves without
	memory.
	(DBX_REGISTER_NUMBER, FIRST_VEC_ARG_REGNO, LAST_VEC_ARG_REGNO)
	(SHORT_DISP_IN_RANGE, VECTOR_STORE_FLAG_VALUE): Define macro.
	* config/s390/s390.md (UNSPEC_VEC_*): New constants.
	(VR*_REGNUM): New constants.
	(ALL): New mode iterator.
	(INTALL): Remove mode iterator.
	Include vector.md.
	(movti): Implement TImode moves for VRs.
	Disable TImode splitter for VR targets.
	Implement splitting TImode GPR<->VR moves.
	(reload*_tomem_z10, reload*_toreg_z10): Replace INTALL with ALL.
	(reload<mode>_nonoffmem_in, reload<mode>_nonoffmem_out): Rename to
	reload<mode>_la_in, reload<mode>_la_out.
	(*movdi_64, *movsi_zarch, *movhi, *movqi, *mov<mode>_64dfp)
	(*mov<mode>_64, *mov<mode>_31): Add vector instructions.
	(TD/TF mode splitter): Enable for GPRs only (formerly !FP).
	(mov<mode> SF SD): Prefer lder, lde for loading.
	Add lrl and strl instructions.
	Add vector instructions.
	(strlen<mode>): Rename old strlen<mode> to strlen_srst<mode>.
	Call s390_expand_vec_strlen on z13.
	(*cc_to_int): Change predicate to nonimmediate_operand.
	(addti3): Rename to *addti3.  New expander.
	(subti3): Rename to *subti3.  New expander.
	* config/s390/vector.md: New file.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* common/config/s390/s390-common.c (processor_flags_table): Add
	z13.
	* config.gcc: Add z13.
	* config/s390/s390-opts.h (enum processor_type): Add
	PROCESSOR_2964_Z13.
	* config/s390/s390.c (s390_adjust_priority): Check for
	PROCESSOR_2964_Z13.
	(s390_reorg): Likewise.
	(s390_sched_reorder): Likewise.
	(s390_sched_variable_issue): Likewise.
	(s390_loop_unroll_adjust): Likewise.
	(s390_option_override): Likewise. Default to -mvx when available.
	* config/s390/s390.h (enum processor_flags): Add PF_Z13 and PF_VX.
	(TARGET_CPU_Z13, TARGET_CPU_VX, TARGET_Z13, TARGET_VX)
	(TARGET_VX_ABI): Define macros.
	macros.
	(TARGET_DEFAULT): Add MASK_OPT_VX.
	* config/s390/s390.md ("cpu" attribute): Add z13.
	("cpu_facility" attribute): Add vec.
	* config/s390/s390.opt (processor_type): Add z13.
	(mvx): New options.
	* doc/invoke.texi: Add z13 option for -march.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* optabs.c (expand_vec_perm): Don't re-use SEL as target operand.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* config/s390/s390.c (s390_secondary_reload): Fix check for
	load/store relative.

2015-05-19  Andreas Krebbel  <krebbel@linux.vnet.ibm.com>

	* recog.h: Increase MAX_RECOG_ALTERNATIVES.  Change type of
	alternative_mask to uint64_t.

2015-02-27  Andreas Krebbel  <Andreas.Krebbel@de.ibm.com>

	* config/s390/s390.c (enum s390_builtin):
	Add S390_BUILTIN_S390_SFPC and S390_BUILTIN_S390_EFPC.
	(code_for_builtin): Add CODE_FOR_s390_sfpc and CODE_FOR_s390_efpc.
	(s390_init_builtins): Generate new builtin functions.
	* config/s390/s390.md (UNSPECV_SFPC, UNSPECV_EFPC): New constants.
	(s390_sfpc, s390_efpc): New pattern definitions.

2014-09-03  Matthew Fortune  <matthew.fortune@imgtec.com>

	* target.def (TARGET_DWARF_FRAME_REG_MODE): New target hook.
	* targhooks.c (default_dwarf_frame_reg_mode): New function.
	* targhooks.h (default_dwarf_frame_reg_mode): New prototype.
	* doc/tm.texi.in (TARGET_DWARF_FRAME_REG_MODE): Document.
	* doc/tm.texi: Regenerate.
	* dwarf2cfi.c (expand_builtin_init_dwarf_reg_sizes): Abstract mode
	selection logic to default_dwarf_frame_reg_mode.

--- gcc/common/config/s390/s390-common.c	2013-08-14 13:55:13.000000000 +0200
+++ gcc/common/config/s390/s390-common.c	2016-05-11 15:53:24.000000000 +0200
@@ -42,7 +42,10 @@ EXPORTED_CONST int processor_flags_table
     /* z196 */   PF_IEEE_FLOAT | PF_ZARCH | PF_LONG_DISPLACEMENT
                  | PF_EXTIMM | PF_DFP | PF_Z10 | PF_Z196,
     /* zEC12 */  PF_IEEE_FLOAT | PF_ZARCH | PF_LONG_DISPLACEMENT
+                 | PF_EXTIMM | PF_DFP | PF_Z10 | PF_Z196 | PF_ZEC12 | PF_TX,
+    /* z13 */    PF_IEEE_FLOAT | PF_ZARCH | PF_LONG_DISPLACEMENT
                  | PF_EXTIMM | PF_DFP | PF_Z10 | PF_Z196 | PF_ZEC12 | PF_TX
+                 | PF_Z13 | PF_VX
   };
 
 /* Change optimizations to be performed, depending on the
--- gcc/config/s390/2827.md	2015-06-18 17:09:04.000000000 +0200
+++ gcc/config/s390/2827.md	2016-05-11 18:03:45.000000000 +0200
@@ -18,20 +18,19 @@
 ;; along with GCC; see the file COPYING3.  If not see
 ;; <http://www.gnu.org/licenses/>.
 
-
-(define_attr "ooo_cracked" ""
+(define_attr "zEC12_cracked" ""
   (cond [(eq_attr "mnemonic" "cgdbr,clfxtr,cdgtr,celfbr,cxgtr,clfebr,clc,lngfr,cs,cfxbr,xc,clfdbr,basr,ex,cxlgtr,clfdtr,srdl,lpgfr,cdlgbr,cgxtr,cxlftr,nc,cxftr,cdfbr,clfxbr,cdftr,clgxbr,cgdtr,cxlgbr,mvc,clgdtr,cegbr,cfebr,cdlftr,sldl,cdlgtr,csg,chhsi,clgebr,cxgbr,cxfbr,cdlfbr,cgebr,lzxr,oc,cdgbr,brasl,cgxbr,cxlfbr,clgxtr,exrl,cfdbr,celgbr,clgdbr,lxr,cpsdr,lcgfr,bras,srda,cefbr") (const_int 1)]
         (const_int 0)))
 
-(define_attr "ooo_expanded" ""
+(define_attr "zEC12_expanded" ""
   (cond [(eq_attr "mnemonic" "dlr,dsgr,d,dsgf,stam,dsgfr,dlgr,dsg,cds,dr,stm,mvc,dl,cdsg,stmy,dlg,stmg,lam") (const_int 1)]
         (const_int 0)))
 
-(define_attr "ooo_endgroup" ""
+(define_attr "zEC12_endgroup" ""
   (cond [(eq_attr "mnemonic" "ipm") (const_int 1)]
         (const_int 0)))
 
-(define_attr "ooo_groupalone" ""
+(define_attr "zEC12_groupalone" ""
   (cond [(eq_attr "mnemonic" "lnxbr,madb,ltxtr,clc,axtr,msebr,slbgr,xc,alcr,lpxbr,slbr,maebr,mlg,mfy,lxdtr,maeb,lxeb,nc,mxtr,sxtr,dxbr,alc,msdbr,ltxbr,lxdb,madbr,lxdbr,lxebr,mvc,m,mseb,mlr,mlgr,slb,tcxb,msdb,sqxbr,alcgr,oc,flogr,alcg,mxbr,dxtr,axbr,mr,sxbr,slbg,ml,lcxbr,bcr_flush") (const_int 1)]
         (const_int 0)))
 
--- gcc/config/s390/2964.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/2964.md	2016-05-11 18:03:45.000000000 +0200
@@ -0,0 +1,232 @@
+;; Scheduling description for z13.
+;;   Copyright (C) 2016 Free Software Foundation, Inc.
+;;   Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com)
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it under
+;; the terms of the GNU General Public License as published by the Free
+;; Software Foundation; either version 3, or (at your option) any later
+;; version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+;; WARRANTY; without even the implied warranty of MERCHANTABILITY or
+;; FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+;; for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+
+; generator options: vector_ecycs=12 cracked_ecycs=6 scale_ecycs=5
+
+(define_attr "z13_cracked" ""
+  (cond [(eq_attr "mnemonic" "celgbr,vscef,vsceg,exrl,clfebr,cefbr,chhsi,\
+vgef,vgeg,cdlftr,lcgfr,cfdbr,cgdbr,lzxr,cfxbr,rnsbg,cgdtr,cegbr,rxsbg,ex,\
+cgxtr,clfxtr,cdlgtr,brasl,efpc,cfebr,tbeginc,celfbr,clgxbr,vsteb,vsteh,\
+clfdtr,cdfbr,lngfr,clgebr,stpq,cs,lpgfr,cdlgbr,lpq,cdgtr,d,cgxbr,cdftr,\
+rosbg,clgdbr,cdgbr,bras,tbegin,clfdbr,cdlfbr,cgebr,clfxbr,lxr,csy,csg,clgdtr,\
+clgxtr") (const_int 1)]
+        (const_int 0)))
+
+(define_attr "z13_expanded" ""
+  (cond [(eq_attr "mnemonic" "cxlftr,cdsg,cdsy,stam,lam,dsgf,lmg,cxlgtr,\
+dl,cxftr,sldl,dsg,cxlfbr,cxgtr,stmg,stmy,stm,lm,cds,lmy,cxfbr,cxlgbr,srda,\
+srdl,cxgbr,dlg") (const_int 1)]
+        (const_int 0)))
+
+(define_attr "z13_groupalone" ""
+  (cond [(eq_attr "mnemonic" "mvc,dxbr,lxebr,axtr,cxtr,alcr,lxdb,lxeb,mxtr,\
+mfy,cxbr,dsgr,lcxbr,slb,mr,dr,alc,slbr,maebr,mlgr,dsgfr,sxtr,tdcxt,tabort,\
+msebr,lxdtr,ltxtr,slbg,ml,mxbr,maeb,oc,dxtr,msdb,sqxbr,mseb,xc,m,clc,mlg,\
+mlr,fixbra,alcgr,nc,sfpc,dlgr,fixbr,slbgr,fixtr,lpxbr,axbr,lxdbr,ltxbr,\
+tcxb,dlr,lnxbr,sxbr,flogr,alcg,tend,madb,bcr_flush") (const_int 1)]
+        (const_int 0)))
+
+(define_attr "z13_endgroup" ""
+  (cond [(eq_attr "mnemonic" "ipm") (const_int 1)]
+        (const_int 0)))
+
+(define_attr "z13_unit_lsu" ""
+  (cond [(eq_attr "mnemonic" "vlbb,mvc,llgc,llc,llhrl,vl,llghrl,vlrepf,\
+vlrepg,vlreph,lde,ldy,tabort,l,llh,ld,lg,ly,vlrepb,vllezb,vllezf,vllezg,\
+vllezh,oc,xc,clc,lrl,ear,nc,lgrl,sfpc,llgf,llgfrl,llgh,llgt,lcbb,vll,sar") (const_int 1)]
+        (const_int 0)))
+
+(define_attr "z13_unit_fxu" ""
+  (cond [(eq_attr "mnemonic" "s,lcgr,x,nop,oiy,ppa,ng,msy,sgrk,vstl,aghik,\
+msgf,ipm,mvi,stocg,rll,srlg,cghsi,clgit,srlk,alrk,sg,sh,sl,st,sy,vst,ark,\
+xgr,agsi,tm,nrk,shy,llhr,agf,alcr,slgfr,sr,clgrt,laa,lder,sgf,lan,llilf,\
+llilh,ag,llill,lay,al,n,laxg,ar,ahi,sgr,ntstg,ay,stcy,nopr,mfy,ngrk,lbr,\
+br,dsgr,stdy,ork,ldgr,lcr,cg,ch,lgfrl,cl,stoc,cr,agfr,stgrl,cy,alfi,xg,\
+cgfi,xi,clfhsi,cgfr,xr,slb,mghi,clfi,slg,clhhsi,agfi,clfit,sly,mr,ldr,nihf,\
+nihh,algfi,dr,nihl,algf,algfr,algr,clgf,clgr,clgt,aghi,alc,alg,locg,alr,\
+locr,cghi,aly,alghsik,slbr,clgfrl,mhy,cit,nr,ny,xiy,mlgr,sthy,cly,dsgfr,\
+rllg,cgit,lgb,lgf,clgrl,lgh,lrvgr,cliy,cgrl,lgr,slrk,clrt,icy,laog,og,agr,\
+mvhi,lhrl,or,lhr,vlvgp,lhy,nilf,oy,nilh,nill,lcdfr,mviy,tmhh,tmhl,sthrl,\
+ltgf,ltgr,srk,clghrl,ahy,vstef,vsteg,ah,vlgvb,llgcr,tmh,tml,clmy,slr,cfi,\
+stc,std,ste,stg,sth,locgr,slbg,sty,tmlh,la,lb,mvghi,lh,risbgn,lrvg,lr,asi,\
+lt,ahik,lrvr,cgf,cgh,cgr,clhrl,lzdr,tmll,mh,ml,vlvgb,ms,lrv,vlvgf,xgrk,\
+vlvgg,llgfr,vlvgh,slfi,chi,chy,mhi,lzer,alhsik,ni,ltgfr,loc,icm,oi,cgfrl,\
+agrk,lgat,oilh,llghr,lghrl,oill,xihf,lpgr,cgrt,clrl,sgfr,lpr,lgbr,strl,\
+algrk,alsi,srak,slgf,a,c,slgr,m,o,algsi,icmh,srag,iilf,ogrk,clg,icmy,\
+cli,clm,clr,clt,slgrk,mlg,lao,mlr,risbg,mvhhi,lat,etnd,lax,iihf,sra,alcgr,\
+msgr,clghsi,stey,ngr,xilf,laag,oihf,oihh,oihl,ltg,ltr,niy,lgfi,dlgr,lgfr,\
+slgfi,llcr,slbgr,chrl,lgdr,pfpo,lang,basr,sllg,sllk,lghi,lghr,vlgvf,vlgvg,\
+vlgvh,vlr,chsi,lngr,cghrl,srl,lhi,oilf,crl,crt,afi,xrk,llgtr,llihf,llihh,\
+llihl,dlr,msgfi,msgfr,msg,flogr,xy,msr,clgfi,clgfr,ogr,popcnt,alcg,lndfr,\
+larl,sll,tmy,msfi,ic,lpdfr,tend,lnr") (const_int 1)]
+        (const_int 0)))
+
+(define_attr "z13_unit_vfu" ""
+  (cond [(eq_attr "mnemonic" "seb,vcksm,vfadb,vleib,vchgs,vleif,vleig,vleih,\
+vgbm,verimb,vone,verimf,verimg,verimh,dxbr,verllvb,lpebr,verllvf,verllvg,\
+verllvh,vfeneb,wcdgb,vfenef,vfeneh,vchhs,vctzb,vctzf,vctzg,vctzh,vlcb,aeb,\
+vlcf,vlcg,vlch,vfmsdb,vgfmab,ltebr,vgfmaf,vgfmag,vgfmah,vmaeh,vsb,vsf,vsg,\
+vsh,vsl,vsq,lxebr,cdtr,fiebr,vupllb,vupllf,vupllh,vmrhb,madbr,vtm,vmrhf,\
+vmrhg,vmrhh,axtr,fiebra,vleb,cxtr,vlef,vleg,vleh,vpkf,vpkg,vpkh,vmlob,vmlof,\
+vmloh,lxdb,ldeb,mdtr,vceqfs,adb,wflndb,lxeb,vn,vo,vchlb,vx,mxtr,vchlf,vchlg,\
+vchlh,vfcedbs,vfcedb,vceqgs,cxbr,msdbr,vcdgb,debr,vceqhs,meeb,lcxbr,vavglb,\
+vavglf,vavglg,vavglh,wfcedbs,vmrlb,vmrlf,vmrlg,vmrlh,wfchedbs,vmxb,tcdb,\
+vmahh,vsrlb,wcgdb,lcdbr,vistrbs,vrepb,wfmdb,vrepf,vrepg,vreph,ler,wcdlgb,\
+ley,vistrb,vistrf,vistrh,tceb,wfsqdb,sqeb,vsumqf,vsumqg,vesrlb,vfeezbs,\
+maebr,vesrlf,vesrlg,vesrlh,vmeb,vmef,vmeh,meebr,vflcdb,wfmadb,vperm,sxtr,\
+vclzf,vgm,vgmb,vgmf,vgmg,vgmh,tdcxt,vzero,msebr,veslb,veslf,veslg,vfenezb,\
+vfenezf,vfenezh,vistrfs,vchf,vchg,vchh,vmhb,vmhf,vmhh,cdb,veslvb,ledbr,\
+veslvf,veslvg,veslvh,wclgdb,vfmdb,vmnlb,vmnlf,vmnlg,vmnlh,vclzb,vfeezfs,\
+vclzg,vclzh,mdb,vmxlb,vmxlf,vmxlg,vmxlh,ltdtr,vsbcbiq,ceb,wfddb,sebr,vistrhs,\
+lxdtr,lcebr,vab,vaf,vag,vah,ltxtr,vlpf,vlpg,vsegb,vaq,vsegf,vsegh,wfchdbs,\
+sdtr,cdbr,vfeezhs,le,wldeb,vfmadb,vchlbs,vacccq,vmaleb,vsel,vmalef,vmaleh,\
+vflndb,mdbr,vmlb,wflpdb,ldetr,vpksfs,vpksf,vpksg,vpksh,sqdb,mxbr,sqdbr,\
+vmaeb,veslh,vmaef,vpklsf,vpklsg,vpklsh,verllb,vchb,ddtr,verllf,verllg,verllh,\
+wfsdb,maeb,vclgdb,vftcidb,vpksgs,vmxf,vmxg,vmxh,fidbra,vmnb,vmnf,vmng,vfchedbs,\
+lnebr,vfidb,dxtr,ddb,msdb,vmalhb,vfddb,vmalhf,vmalhh,vpkshs,vfsdb,sqxbr,\
+vmalhw,ltdbr,vmob,vmof,vmoh,deb,vchlfs,mseb,vcdlgb,vlpb,wfmsdb,vlph,vmahb,\
+vldeb,vmahf,vgfmb,fidbr,vfsqdb,aebr,wledb,vchlgs,vesravb,vfchdbs,cebr,vesravf,\
+vesravg,vesravh,vcgdb,fixbra,vrepib,vrepif,vrepig,vrepih,tdcdt,vchlhs,vceqb,\
+vscbib,vceqf,vceqg,vscbif,vscbig,vscbih,vmlhw,vscbiq,vuphb,vuphf,vuphh,\
+vfchedb,tdcet,vslb,vpklsfs,adbr,sqebr,vfchdb,fixbr,vpklsgs,vsldb,vmleb,\
+vmlef,vmleh,cpsdr,vmalb,vmalf,vavgb,vmlf,vavgf,vavgg,vavgh,vgfmf,vgfmg,\
+vgfmh,fidtr,vpklshs,lndbr,vno,lpdbr,vacq,vledb,vchbs,vfeeb,vfeef,vfeeh,\
+fixtr,vaccb,wfadb,vaccf,vaccg,vacch,vnot,vmalob,vaccq,vmalof,vmaloh,lpxbr,\
+ledtr,vuplb,vuplf,axbr,lxdbr,ltxbr,vpopct,vpdi,vmlhb,vmlhf,vmlhh,sdbr,vnc,\
+vsumb,vsrab,vsumh,vmaob,vmaof,vmaoh,vesrlvb,vesrlvf,vesrlvg,vesrlvh,tcxb,\
+vceqbs,vceqh,lnxbr,sxbr,vesrab,wflcdb,vesraf,vesrag,vesrah,vflpdb,vmnh,\
+vsbiq,adtr,vsra,vsrl,vuplhb,sdb,vuplhf,vuplhh,vsumgf,vsumgh,ldebr,vuplhw,\
+vchfs,madb,ddbr") (const_int 1)]
+        (const_int 0)))
+
+(define_insn_reservation "z13_0" 0
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "s,lcgr,x,nop,oiy,vlbb,ppa,ng,sgrk,vstl,aghik,\
+mvc,ipm,llgc,mvi,stocg,rll,jg,srlg,cghsi,clgit,srlk,alrk,sg,sh,sl,st,sy,\
+vst,ark,xgr,agsi,tm,nrk,shy,llhr,agf,alcr,slgfr,sr,clgrt,llc,laa,lder,sgf,\
+lan,llhrl,llilf,llilh,ag,llill,lay,al,n,laxg,ar,ahi,sgr,ntstg,ay,stcy,vl,\
+nopr,ngrk,lbr,br,stdy,ork,ldgr,lcr,cg,ch,llghrl,lgfrl,cl,stoc,cr,agfr,stgrl,\
+cy,alfi,xg,cgfi,xi,vlrepf,vlrepg,vlreph,clfhsi,cgfr,xr,slb,mghi,clfi,slg,\
+lde,clhhsi,agfi,clfit,sly,ldr,ldy,nihf,nihh,algfi,nihl,algf,algfr,algr,\
+clgf,clgr,clgt,aghi,alc,alg,locg,alr,locr,cghi,aly,alghsik,slbr,clgfrl,\
+mhy,cit,nr,ny,xiy,sthy,cly,rllg,cgit,lgb,lgf,clgrl,lgh,lrvgr,cliy,cgrl,\
+lgr,slrk,clrt,icy,laog,og,agr,mvhi,lhrl,or,lhr,vlvgp,lhy,nilf,oy,nilh,tabort,\
+nill,lcdfr,mviy,tmhh,tmhl,sthrl,ltgf,ltgr,srk,clghrl,ahy,vstef,vsteg,ah,\
+vlgvb,llgcr,tmh,tml,clmy,slr,cfi,stc,std,ste,stg,sth,l,locgr,llh,slbg,sty,\
+tmlh,la,lb,ld,mvghi,lg,lh,risbgn,lrvg,lr,asi,lt,ahik,ly,lrvr,vlrepb,vllezb,\
+cgf,cgh,vllezf,vllezg,vllezh,cgr,clhrl,lzdr,tmll,mh,vlvgb,lrv,vlvgf,xgrk,\
+vlvgg,llgfr,vlvgh,slfi,chi,chy,mhi,lzer,alhsik,ni,ltgfr,loc,icm,oc,oi,cgfrl,\
+agrk,lgat,oilh,llghr,lghrl,oill,xihf,lpgr,cgrt,clrl,sgfr,lpr,lgbr,strl,\
+algrk,alsi,srak,brcl,slgf,xc,a,c,slgr,j,o,algsi,icmh,srag,iilf,ogrk,clc,\
+clg,icmy,cli,clm,clr,clt,slgrk,lrl,lao,risbg,mvhhi,lat,etnd,lax,iihf,sra,\
+alcgr,clghsi,ear,nc,lgrl,stey,ngr,xilf,laag,oihf,oihh,oihl,ltg,ltr,niy,\
+lgfi,sfpc,lgfr,slgfi,llcr,llgf,llgfrl,llgh,slbgr,llgt,chrl,lgdr,pfpo,lang,\
+basr,lcbb,sllg,sllk,lghi,vll,lghr,vlgvf,vlgvg,vlgvh,vlr,chsi,lngr,cghrl,\
+srl,sar,lhi,oilf,crl,crt,afi,xrk,llgtr,llihf,llihh,llihl,xy,clgfi,clgfr,\
+ogr,popcnt,alcg,lndfr,larl,sll,tmy,ic,lpdfr,tend,lnr,bcr_flush")) "nothing")
+
+(define_insn_reservation "z13_1" 1
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "celgbr,vscef,vsceg,msy,msgf,cxlftr,cdsg,cdsy,\
+exrl,clfebr,cefbr,chhsi,stam,vgef,vgeg,cdlftr,lam,mfy,lcgfr,cfdbr,dsgf,\
+cgdbr,lzxr,lmg,cfxbr,rnsbg,cxlgtr,mr,dl,cxftr,sldl,cgdtr,cegbr,rxsbg,ex,\
+cgxtr,clfxtr,mlgr,cdlgtr,brasl,dsg,efpc,cfebr,tbeginc,celfbr,clgxbr,vsteb,\
+vsteh,cxlfbr,clfdtr,cxgtr,stmg,stmy,stm,lm,cds,cdfbr,ml,ms,lngfr,clgebr,\
+stpq,lmy,cs,lpgfr,cdlgbr,lpq,cxfbr,cxlgbr,cdgtr,d,m,mlg,mlr,cgxbr,cdftr,\
+msgr,rosbg,clgdbr,cdgbr,srda,bras,srdl,tbegin,clfdbr,cdlfbr,cxgbr,cgebr,\
+dlg,clfxbr,lxr,csy,msgfi,msgfr,msg,flogr,msr,csg,msfi,clgdtr,clgxtr")) "nothing")
+
+(define_insn_reservation "z13_2" 2
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "seb,vcksm,vfadb,vleib,vchgs,vleif,vleig,vleih,\
+vgbm,verimb,vone,verimf,verimg,verimh,verllvb,lpebr,verllvf,verllvg,verllvh,\
+vfeneb,wcdgb,vfenef,vfeneh,vchhs,vctzb,vctzf,vctzg,vctzh,vlcb,aeb,vlcf,\
+vlcg,vlch,vfmsdb,vgfmab,ltebr,vgfmaf,vgfmag,vgfmah,vmaeh,vsb,vsf,vsg,vsh,\
+vsl,vsq,lxebr,cdtr,fiebr,vupllb,vupllf,vupllh,vmrhb,madbr,vtm,vmrhf,vmrhg,\
+vmrhh,axtr,fiebra,vleb,cxtr,vlef,vleg,vleh,vpkf,vpkg,vpkh,vmlob,vmlof,vmloh,\
+lxdb,ldeb,vceqfs,adb,wflndb,lxeb,vn,vo,vchlb,vx,vchlf,vchlg,vchlh,vfcedbs,\
+vfcedb,vceqgs,cxbr,msdbr,vcdgb,vceqhs,meeb,lcxbr,vavglb,vavglf,vavglg,vavglh,\
+wfcedbs,vmrlb,vmrlf,vmrlg,vmrlh,wfchedbs,vmxb,tcdb,vmahh,vsrlb,wcgdb,lcdbr,\
+vistrbs,vrepb,wfmdb,vrepf,vrepg,vreph,ler,wcdlgb,ley,vistrb,vistrf,vistrh,\
+tceb,vsumqf,vsumqg,vesrlb,vfeezbs,maebr,vesrlf,vesrlg,vesrlh,vmeb,vmef,\
+vmeh,meebr,vflcdb,wfmadb,vperm,sxtr,vclzf,vgm,vgmb,vgmf,vgmg,vgmh,tdcxt,\
+vzero,msebr,veslb,veslf,veslg,vfenezb,vfenezf,vfenezh,vistrfs,vchf,vchg,\
+vchh,vmhb,vmhf,vmhh,cdb,veslvb,ledbr,veslvf,veslvg,veslvh,wclgdb,vfmdb,\
+vmnlb,vmnlf,vmnlg,vmnlh,vclzb,vfeezfs,vclzg,vclzh,mdb,vmxlb,vmxlf,vmxlg,\
+vmxlh,ltdtr,vsbcbiq,ceb,sebr,vistrhs,lxdtr,lcebr,vab,vaf,vag,vah,ltxtr,\
+vlpf,vlpg,vsegb,vaq,vsegf,vsegh,wfchdbs,sdtr,cdbr,vfeezhs,le,wldeb,vfmadb,\
+vchlbs,vacccq,vmaleb,vsel,vmalef,vmaleh,vflndb,mdbr,vmlb,wflpdb,ldetr,vpksfs,\
+vpksf,vpksg,vpksh,vmaeb,veslh,vmaef,vpklsf,vpklsg,vpklsh,verllb,vchb,verllf,\
+verllg,verllh,wfsdb,maeb,vclgdb,vftcidb,vpksgs,vmxf,vmxg,vmxh,fidbra,vmnb,\
+vmnf,vmng,vfchedbs,lnebr,vfidb,msdb,vmalhb,vmalhf,vmalhh,vpkshs,vfsdb,vmalhw,\
+ltdbr,vmob,vmof,vmoh,vchlfs,mseb,vcdlgb,vlpb,wfmsdb,vlph,vmahb,vldeb,vmahf,\
+vgfmb,fidbr,aebr,wledb,vchlgs,vesravb,vfchdbs,cebr,vesravf,vesravg,vesravh,\
+vcgdb,fixbra,vrepib,vrepif,vrepig,vrepih,tdcdt,vchlhs,vceqb,vscbib,vceqf,\
+vceqg,vscbif,vscbig,vscbih,vmlhw,vscbiq,vuphb,vuphf,vuphh,vfchedb,tdcet,\
+vslb,vpklsfs,adbr,vfchdb,fixbr,vpklsgs,vsldb,vmleb,vmlef,vmleh,cpsdr,vmalb,\
+vmalf,vavgb,vmlf,vavgf,vavgg,vavgh,vgfmf,vgfmg,vgfmh,fidtr,vpklshs,lndbr,\
+vno,lpdbr,vacq,vledb,vchbs,vfeeb,vfeef,vfeeh,fixtr,vaccb,wfadb,vaccf,vaccg,\
+vacch,vnot,vmalob,vaccq,vmalof,vmaloh,lpxbr,vuplb,vuplf,axbr,lxdbr,ltxbr,\
+vpopct,vpdi,vmlhb,vmlhf,vmlhh,sdbr,vnc,vsumb,vsrab,vsumh,vmaob,vmaof,vmaoh,\
+vesrlvb,vesrlvf,vesrlvg,vesrlvh,tcxb,vceqbs,vceqh,lnxbr,sxbr,vesrab,wflcdb,\
+vesraf,vesrag,vesrah,vflpdb,vmnh,vsbiq,adtr,vsra,vsrl,vuplhb,sdb,vuplhf,\
+vuplhh,vsumgf,vsumgh,ldebr,vuplhw,vchfs,madb")) "nothing")
+
+(define_insn_reservation "z13_3" 3
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "ledtr")) "nothing")
+
+(define_insn_reservation "z13_4" 4
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "dr,mxbr,dlr")) "nothing")
+
+(define_insn_reservation "z13_6" 6
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "debr,sqeb,deb,sqebr")) "nothing")
+
+(define_insn_reservation "z13_7" 7
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "mdtr")) "nothing")
+
+(define_insn_reservation "z13_8" 8
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "wfddb,ddb,vfddb,ddbr")) "nothing")
+
+(define_insn_reservation "z13_9" 9
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "dsgr,wfsqdb,dsgfr,sqdb,sqdbr,vfsqdb")) "nothing")
+
+(define_insn_reservation "z13_13" 13
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "mxtr,ddtr")) "nothing")
+
+(define_insn_reservation "z13_16" 16
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "sqxbr")) "nothing")
+
+(define_insn_reservation "z13_17" 17
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "dxtr")) "nothing")
+
+(define_insn_reservation "z13_20" 20
+  (and (eq_attr "cpu" "z13")
+       (eq_attr "mnemonic" "dxbr,dlgr")) "nothing")
+
--- gcc/config/s390/constraints.md	2013-01-21 16:11:50.000000000 +0100
+++ gcc/config/s390/constraints.md	2016-05-11 18:40:20.880008612 +0200
@@ -29,7 +29,15 @@
 ;;    c -- Condition code register 33.
 ;;    d -- Any register from 0 to 15.
 ;;    f -- Floating point registers.
+;;    j -- Multiple letter constraint for constant scalar and vector values
+;;         j00: constant zero scalar or vector
+;;         jm1: constant scalar or vector with all bits set
+;;         jxx: contiguous bitmask of 0 or 1 in all vector elements
+;;         jyy: constant consisting of byte chunks being either 0 or 0xff
+;;         jKK: constant vector with all elements having the same value and
+;;              matching K constraint
 ;;    t -- Access registers 36 and 37.
+;;    v -- Vector registers v0-v31.
 ;;    C -- A signed 8-bit constant (-128..127)
 ;;    D -- An unsigned 16-bit constant (0..65535)
 ;;    G -- Const double zero operand
@@ -109,6 +117,11 @@
    Access registers 36 and 37")
 
 
+(define_register_constraint "v"
+  "VEC_REGS"
+  "Vector registers v0-v31")
+
+
 ;;
 ;;  General constraints for constants.
 ;;
@@ -374,6 +387,33 @@
        (match_test "s390_O_constraint_str ('n', ival)")))
 
 
+;;
+;; Vector constraints follow.
+;;
+
+(define_constraint "j00"
+  "Zero scalar or vector constant"
+  (match_test "op == CONST0_RTX (GET_MODE (op))"))
+
+(define_constraint "jm1"
+  "All one bit scalar or vector constant"
+  (match_test "op == CONSTM1_RTX (GET_MODE (op))"))
+
+(define_constraint "jxx"
+  "@internal"
+  (and (match_code "const_vector")
+       (match_test "s390_contiguous_bitmask_vector_p (op, NULL, NULL)")))
+
+(define_constraint "jyy"
+  "@internal"
+  (and (match_code "const_vector")
+       (match_test "s390_bytemask_vector_p (op, NULL)")))
+
+(define_constraint "jKK"
+  "@internal"
+  (and (and (match_code "const_vector")
+	    (match_test "s390_const_vec_duplicate_p (op)"))
+       (match_test "satisfies_constraint_K (XVECEXP (op, 0, 0))")))
 
 
 ;;
--- gcc/config/s390/predicates.md	2013-08-14 13:55:12.000000000 +0200
+++ gcc/config/s390/predicates.md	2016-05-11 18:17:42.508662564 +0200
@@ -24,16 +24,26 @@
 
 ;; operands --------------------------------------------------------------
 
-;; Return true if OP a (const_int 0) operand.
-
+;; Return true if OP a const 0 operand (int/float/vector).
 (define_predicate "const0_operand"
-  (and (match_code "const_int, const_double")
+  (and (match_code "const_int,const_double,const_vector")
        (match_test "op == CONST0_RTX (mode)")))
 
+;; Return true if OP an all ones operand (int/vector).
+(define_predicate "all_ones_operand"
+  (and (match_code "const_int, const_double, const_vector")
+       (match_test "INTEGRAL_MODE_P (GET_MODE (op))")
+       (match_test "op == CONSTM1_RTX (mode)")))
+
+;; Return true if OP is a 4 bit mask operand
+(define_predicate "const_mask_operand"
+  (and (match_code "const_int")
+       (match_test "UINTVAL (op) < 16")))
+
 ;; Return true if OP is constant.
 
 (define_special_predicate "consttable_operand"
-  (and (match_code "symbol_ref, label_ref, const, const_int, const_double")
+  (and (match_code "symbol_ref, label_ref, const, const_int, const_double, const_vector")
        (match_test "CONSTANT_P (op)")))
 
 ;; Return true if OP is a valid S-type operand.
--- gcc/config/s390/s390-builtins.def	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/s390-builtins.def	2016-05-11 17:53:57.000000000 +0200
@@ -0,0 +1,2488 @@
+/* Builtin definitions for IBM S/390 and zSeries
+   Copyright (C) 2015 Free Software Foundation, Inc.
+
+   Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com).
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#undef O_U1
+#undef O_U2
+#undef O_U3
+#undef O_U4
+#undef O_U5
+#undef O_U8
+#undef O_U12
+#undef O_U16
+#undef O_U32
+
+#undef O_S2
+#undef O_S3
+#undef O_S4
+#undef O_S5
+#undef O_S8
+#undef O_S12
+#undef O_S16
+#undef O_S32
+#undef O_ELEM
+#undef O_LIT
+
+#undef O1_U1
+#undef O2_U1
+#undef O3_U1
+#undef O4_U1
+
+#undef O1_U2
+#undef O2_U2
+#undef O3_U2
+#undef O4_U2
+
+#undef O1_U3
+#undef O2_U3
+#undef O3_U3
+#undef O4_U3
+
+#undef O1_U4
+#undef O2_U4
+#undef O3_U4
+#undef O4_U4
+
+#undef O1_U5
+#undef O2_U5
+#undef O3_U5
+#undef O4_U5
+
+#undef O1_U8
+#undef O2_U8
+#undef O3_U8
+#undef O4_U8
+
+#undef O1_U12
+#undef O2_U12
+#undef O3_U12
+#undef O4_U12
+
+#undef O1_U16
+#undef O2_U16
+#undef O3_U16
+#undef O4_U16
+
+#undef O1_U32
+#undef O2_U32
+#undef O3_U32
+#undef O4_U32
+
+#undef O1_S2
+#undef O2_S2
+#undef O3_S2
+#undef O4_S2
+
+#undef O1_S3
+#undef O2_S3
+#undef O3_S3
+#undef O4_S3
+
+#undef O1_S4
+#undef O2_S4
+#undef O3_S4
+#undef O4_S4
+
+#undef O1_S5
+#undef O2_S5
+#undef O3_S5
+#undef O4_S5
+
+#undef O1_S8
+#undef O2_S8
+#undef O3_S8
+#undef O4_S8
+
+#undef O1_S12
+#undef O2_S12
+#undef O3_S12
+#undef O4_S12
+
+#undef O1_S16
+#undef O2_S16
+#undef O3_S16
+#undef O4_S16
+
+#undef O1_S32
+#undef O2_S32
+#undef O3_S32
+#undef O4_S32
+
+#undef O1_ELEM
+#undef O2_ELEM
+#undef O3_ELEM
+#undef O4_ELEM
+
+#undef O1_LIT
+#undef O2_LIT
+#undef O3_LIT
+#undef O4_LIT
+
+#undef O_SHIFT
+#undef O_IMM_P
+#undef O_UIMM_P
+#undef O_SIMM_P
+
+#define O_U1   1 /* unsigned  1 bit literal */
+#define O_U2   2 /* unsigned  2 bit literal */
+#define O_U3   3 /* unsigned  3 bit literal */
+#define O_U4   4 /* unsigned  4 bit literal */
+#define O_U5   5 /* unsigned  5 bit literal */
+#define O_U8   6 /* unsigned  8 bit literal */
+#define O_U12  7 /* unsigned 16 bit literal */
+#define O_U16  8 /* unsigned 16 bit literal */
+#define O_U32  9 /* unsigned 32 bit literal */
+
+#define O_S2  10 /* signed  2 bit literal */
+#define O_S3  11 /* signed  3 bit literal */
+#define O_S4  12 /* signed  4 bit literal */
+#define O_S5  13 /* signed  5 bit literal */
+#define O_S8  14 /* signed  8 bit literal */
+#define O_S12 15 /* signed 12 bit literal */
+#define O_S16 16 /* signed 16 bit literal */
+#define O_S32 17 /* signed 32 bit literal */
+
+#define O_ELEM  18 /* Element selector requiring modulo arithmetic. */
+#define O_LIT   19 /* Operand must be a literal fitting the target type.  */
+
+#define O_SHIFT 5
+
+#define O_UIMM_P(X) ((X) >= O_U1 && (X) <= O_U32)
+#define O_SIMM_P(X) ((X) >= O_S2 && (X) <= O_S32)
+#define O_IMM_P(X) ((X) == O_LIT || ((X) >= O_U1 && (X) <= O_S32))
+
+#define O1_U1 O_U1
+#define O2_U1 (O_U1 << O_SHIFT)
+#define O3_U1 (O_U1 << (2 * O_SHIFT))
+#define O4_U1 (O_U1 << (3 * O_SHIFT))
+
+#define O1_U2 O_U2
+#define O2_U2 (O_U2 << O_SHIFT)
+#define O3_U2 (O_U2 << (2 * O_SHIFT))
+#define O4_U2 (O_U2 << (3 * O_SHIFT))
+
+#define O1_U3 O_U3
+#define O2_U3 (O_U3 << O_SHIFT)
+#define O3_U3 (O_U3 << (2 * O_SHIFT))
+#define O4_U3 (O_U3 << (3 * O_SHIFT))
+
+#define O1_U4 O_U4
+#define O2_U4 (O_U4 << O_SHIFT)
+#define O3_U4 (O_U4 << (2 * O_SHIFT))
+#define O4_U4 (O_U4 << (3 * O_SHIFT))
+
+#define O1_U5 O_U5
+#define O2_U5 (O_U5 << O_SHIFT)
+#define O3_U5 (O_U5 << (2 * O_SHIFT))
+#define O4_U5 (O_U5 << (3 * O_SHIFT))
+
+#define O1_U8 O_U8
+#define O2_U8 (O_U8 << O_SHIFT)
+#define O3_U8 (O_U8 << (2 * O_SHIFT))
+#define O4_U8 (O_U8 << (3 * O_SHIFT))
+
+#define O1_U12 O_U12
+#define O2_U12 (O_U12 << O_SHIFT)
+#define O3_U12 (O_U12 << (2 * O_SHIFT))
+#define O4_U12 (O_U12 << (3 * O_SHIFT))
+
+#define O1_U16 O_U16
+#define O2_U16 (O_U16 << O_SHIFT)
+#define O3_U16 (O_U16 << (2 * O_SHIFT))
+#define O4_U16 (O_U16 << (3 * O_SHIFT))
+
+#define O1_U32 O_U32
+#define O2_U32 (O_U32 << O_SHIFT)
+#define O3_U32 (O_U32 << (2 * O_SHIFT))
+#define O4_U32 (O_U32 << (3 * O_SHIFT))
+
+
+#define O1_S2 O_S2
+#define O2_S2 (O_S2 << O_SHIFT)
+#define O3_S2 (O_S2 << (2 * O_SHIFT))
+#define O4_S2 (O_S2 << (3 * O_SHIFT))
+
+#define O1_S3 O_S3
+#define O2_S3 (O_S3 << O_SHIFT)
+#define O3_S3 (O_S3 << (2 * O_SHIFT))
+#define O4_S3 (O_S3 << (3 * O_SHIFT))
+
+#define O1_S4 O_S4
+#define O2_S4 (O_S4 << O_SHIFT)
+#define O3_S4 (O_S4 << (2 * O_SHIFT))
+#define O4_S4 (O_S4 << (3 * O_SHIFT))
+
+#define O1_S5 O_S5
+#define O2_S5 (O_S5 << O_SHIFT)
+#define O3_S5 (O_S5 << (2 * O_SHIFT))
+#define O4_S5 (O_S5 << (3 * O_SHIFT))
+
+#define O1_S8 O_S8
+#define O2_S8 (O_S8 << O_SHIFT)
+#define O3_S8 (O_S8 << (2 * O_SHIFT))
+#define O4_S8 (O_S8 << (3 * O_SHIFT))
+
+#define O1_S12 O_S12
+#define O2_S12 (O_S12 << O_SHIFT)
+#define O3_S12 (O_S12 << (2 * O_SHIFT))
+#define O4_S12 (O_S12 << (3 * O_SHIFT))
+
+#define O1_S16 O_S16
+#define O2_S16 (O_S16 << O_SHIFT)
+#define O3_S16 (O_S16 << (2 * O_SHIFT))
+#define O4_S16 (O_S16 << (3 * O_SHIFT))
+
+#define O1_S32 O_S32
+#define O2_S32 (O_S32 << O_SHIFT)
+#define O3_S32 (O_S32 << (2 * O_SHIFT))
+#define O4_S32 (O_S32 << (3 * O_SHIFT))
+
+#define O1_ELEM O_ELEM
+#define O2_ELEM (O_ELEM << O_SHIFT)
+#define O3_ELEM (O_ELEM << (2 * O_SHIFT))
+#define O4_ELEM (O_ELEM << (3 * O_SHIFT))
+
+#define O1_LIT O_LIT
+#define O2_LIT (O_LIT << O_SHIFT)
+#define O3_LIT (O_LIT << (2 * O_SHIFT))
+#define O4_LIT (O_LIT << (3 * O_SHIFT))
+
+
+/* Builtin flags. Flags applying to the whole builtin definition.  */
+
+#undef B_INT
+#undef B_HTM
+#undef B_VX
+
+#undef BFLAGS_MASK_INIT
+#define BFLAGS_MASK_INIT (B_INT)
+
+#define B_INT   (1 << 0)  /* Internal builtins.  This builtin cannot be used in user programs.  */
+#define B_HTM   (1 << 1)  /* Builtins requiring the transactional execution facility.  */
+#define B_VX    (1 << 2)  /* Builtins requiring the z13 vector extensions.  */
+
+
+/* B_DEF defines a standard (not overloaded) builtin
+   B_DEF (<builtin name>, <RTL expander name>, <function attributes>, <builtin flags>, <operand flags, see above>, <fntype>)
+
+   OB_DEF defines an overloaded builtin
+   OB_DEF (<builtin name>, <start variant>, <end variant>, <builtin flags>, <fntype>)
+
+   OB_DEF_VAR defines a variant of an overloaded builtin
+   OB_DEF_VAR (<variant name>, <standard builtin name>, <flags>, <fntype>) */
+
+
+B_DEF      (tbeginc,                    tbeginc,            0,                  B_HTM,              0,                  BT_FN_INT)
+B_DEF      (tbegin,                     tbegin,             returns_twice_attr, B_HTM,              0,                  BT_FN_INT_VOIDPTR)
+B_DEF      (tbegin_nofloat,             tbegin_nofloat,     returns_twice_attr, B_HTM,              0,                  BT_FN_INT_VOIDPTR)
+B_DEF      (tbegin_retry,               tbegin_retry,       returns_twice_attr, B_HTM,              0,                  BT_FN_INT_VOIDPTR_INT)
+B_DEF      (tbegin_retry_nofloat,       tbegin_retry_nofloat,returns_twice_attr,B_HTM,              0,                  BT_FN_INT_VOIDPTR_INT)
+B_DEF      (tend,                       tend,               0,                  B_HTM,              0,                  BT_FN_INT)
+B_DEF      (tabort,                     tabort,             noreturn_attr,      B_HTM,              0,                  BT_FN_VOID_INT)
+B_DEF      (tx_nesting_depth,           etnd,               0,                  B_HTM,              0,                  BT_FN_INT)
+B_DEF      (non_tx_store,               ntstg,              0,                  B_HTM,              0,                  BT_FN_VOID_UINT64PTR_UINT64)
+B_DEF      (tx_assist,                  tx_assist,          0,                  B_HTM,              0,                  BT_FN_VOID_INT)
+B_DEF      (s390_sfpc,                  sfpc,               0,                  0,                  0,                  BT_FN_VOID_UINT)
+B_DEF      (s390_efpc,                  efpc,               0,                  0,                  0,                  BT_FN_UINT)
+B_DEF      (s390_lcbb,                  lcbb,               0,                  B_VX,               O2_U4,              BT_FN_UINT_VOIDCONSTPTR_INT)
+
+OB_DEF     (s390_vec_step,              MAX,                MAX,                B_VX,               BT_FN_INT_INT)
+
+OB_DEF     (s390_vec_gather_element,    s390_vec_gather_element_s32,s390_vec_gather_element_dbl,B_VX,BT_FN_OV4SI_OV4SI_OUV4SI_INTCONSTPTR_UCHAR)
+OB_DEF_VAR (s390_vec_gather_element_s32,s390_vgef,          O4_U2,              BT_OV_V4SI_V4SI_UV4SI_INTCONSTPTR_UCHAR)
+OB_DEF_VAR (s390_vec_gather_element_b32,s390_vgef,          O4_U2,              BT_OV_BV4SI_BV4SI_UV4SI_UINTCONSTPTR_UCHAR)
+OB_DEF_VAR (s390_vec_gather_element_u32,s390_vgef,          O4_U2,              BT_OV_UV4SI_UV4SI_UV4SI_UINTCONSTPTR_UCHAR)
+OB_DEF_VAR (s390_vec_gather_element_s64,s390_vgeg,          O4_U1,              BT_OV_V2DI_V2DI_UV2DI_LONGLONGCONSTPTR_UCHAR)
+OB_DEF_VAR (s390_vec_gather_element_b64,s390_vgeg,          O4_U1,              BT_OV_BV2DI_BV2DI_UV2DI_ULONGLONGCONSTPTR_UCHAR)
+OB_DEF_VAR (s390_vec_gather_element_u64,s390_vgeg,          O4_U1,              BT_OV_UV2DI_UV2DI_UV2DI_ULONGLONGCONSTPTR_UCHAR)
+OB_DEF_VAR (s390_vec_gather_element_dbl,s390_vgeg,          O4_U1,              BT_OV_V2DF_V2DF_UV2DI_DBLCONSTPTR_UCHAR)
+
+B_DEF      (s390_vgef,                  vec_gather_elementv4si,0,               B_VX,               O4_U2,              BT_FN_UV4SI_UV4SI_UV4SI_UINTCONSTPTR_UCHAR)
+B_DEF      (s390_vgeg,                  vec_gather_elementv2di,0,               B_VX,               O4_U1,              BT_FN_UV2DI_UV2DI_UV2DI_ULONGLONGCONSTPTR_UCHAR)
+B_DEF      (s390_vgbm,                  vec_genbytemaskv16qi,0,                 B_VX,               O1_U16,             BT_FN_UV16QI_USHORT)
+B_DEF      (s390_vgmb,                  vec_genmaskv16qi,   0,                  B_VX,               O1_U8 | O2_U8,      BT_FN_UV16QI_UCHAR_UCHAR)
+B_DEF      (s390_vgmh,                  vec_genmaskv8hi,    0,                  B_VX,               O1_U8 | O2_U8,      BT_FN_UV8HI_UCHAR_UCHAR)
+B_DEF      (s390_vgmf,                  vec_genmaskv4si,    0,                  B_VX,               O1_U8 | O2_U8,      BT_FN_UV4SI_UCHAR_UCHAR)
+B_DEF      (s390_vgmg,                  vec_genmaskv2di,    0,                  B_VX,               O1_U8 | O2_U8,      BT_FN_UV2DI_UCHAR_UCHAR)
+
+OB_DEF     (s390_vec_xld2,              s390_vec_xld2_s8,   s390_vec_xld2_dbl,  B_VX,               BT_FN_V4SI_INT_VOIDPTR)
+OB_DEF_VAR (s390_vec_xld2_s8,           MAX,                O1_LIT,             BT_OV_V16QI_LONG_SCHARPTR)               /* vl */
+OB_DEF_VAR (s390_vec_xld2_u8,           MAX,                O1_LIT,             BT_OV_UV16QI_LONG_UCHARPTR)              /* vl */
+OB_DEF_VAR (s390_vec_xld2_s16,          MAX,                O1_LIT,             BT_OV_V8HI_LONG_SHORTPTR)                /* vl */
+OB_DEF_VAR (s390_vec_xld2_u16,          MAX,                O1_LIT,             BT_OV_UV8HI_LONG_USHORTPTR)              /* vl */
+OB_DEF_VAR (s390_vec_xld2_s32,          MAX,                O1_LIT,             BT_OV_V4SI_LONG_INTPTR)                  /* vl */
+OB_DEF_VAR (s390_vec_xld2_u32,          MAX,                O1_LIT,             BT_OV_UV4SI_LONG_UINTPTR)                /* vl */
+OB_DEF_VAR (s390_vec_xld2_s64,          MAX,                O1_LIT,             BT_OV_V2DI_LONG_LONGLONGPTR)             /* vl */
+OB_DEF_VAR (s390_vec_xld2_u64,          MAX,                O1_LIT,             BT_OV_UV2DI_LONG_ULONGLONGPTR)           /* vl */
+OB_DEF_VAR (s390_vec_xld2_dbl,          MAX,                O1_LIT,             BT_OV_V2DF_LONG_DBLPTR)                  /* vl */
+
+OB_DEF     (s390_vec_xlw4,              s390_vec_xlw4_s8,   s390_vec_xlw4_u32,  B_VX,               BT_FN_V4SI_INT_VOIDPTR)
+OB_DEF_VAR (s390_vec_xlw4_s8,           MAX,                O1_LIT,             BT_OV_V16QI_LONG_SCHARPTR)               /* vl */
+OB_DEF_VAR (s390_vec_xlw4_u8,           MAX,                O1_LIT,             BT_OV_UV16QI_LONG_UCHARPTR)              /* vl */
+OB_DEF_VAR (s390_vec_xlw4_s16,          MAX,                O1_LIT,             BT_OV_V8HI_LONG_SHORTPTR)                /* vl */
+OB_DEF_VAR (s390_vec_xlw4_u16,          MAX,                O1_LIT,             BT_OV_UV8HI_LONG_USHORTPTR)              /* vl */
+OB_DEF_VAR (s390_vec_xlw4_s32,          MAX,                O1_LIT,             BT_OV_V4SI_LONG_INTPTR)                  /* vl */
+OB_DEF_VAR (s390_vec_xlw4_u32,          MAX,                O1_LIT,             BT_OV_UV4SI_LONG_UINTPTR)                /* vl */
+
+OB_DEF     (s390_vec_splats,            s390_vec_splats_s8, s390_vec_splats_dbl,B_VX,               BT_FN_OV4SI_INT)
+OB_DEF_VAR (s390_vec_splats_s8,         s390_vlrepb,        0,                  BT_OV_V16QI_SCHAR)
+OB_DEF_VAR (s390_vec_splats_u8,         s390_vlrepb,        0,                  BT_OV_UV16QI_UCHAR)
+OB_DEF_VAR (s390_vec_splats_s16,        s390_vlreph,        0,                  BT_OV_V8HI_SHORT)
+OB_DEF_VAR (s390_vec_splats_u16,        s390_vlreph,        0,                  BT_OV_UV8HI_USHORT)
+OB_DEF_VAR (s390_vec_splats_s32,        s390_vlrepf,        0,                  BT_OV_V4SI_INT)
+OB_DEF_VAR (s390_vec_splats_u32,        s390_vlrepf,        0,                  BT_OV_UV4SI_UINT)
+OB_DEF_VAR (s390_vec_splats_s64,        s390_vlrepg,        0,                  BT_OV_V2DI_LONGLONG)
+OB_DEF_VAR (s390_vec_splats_u64,        s390_vlrepg,        0,                  BT_OV_UV2DI_ULONGLONG)
+OB_DEF_VAR (s390_vec_splats_dbl,        s390_vlrepg_dbl,    0,                  BT_OV_V2DF_DBL)                          /* vlrepg */
+
+B_DEF      (s390_vlrepb,                vec_splatsv16qi,    0,                  B_VX,               0,                  BT_FN_UV16QI_UCHAR)
+B_DEF      (s390_vlreph,                vec_splatsv8hi,     0,                  B_VX,               0,                  BT_FN_UV8HI_USHORT)
+B_DEF      (s390_vlrepf,                vec_splatsv4si,     0,                  B_VX,               0,                  BT_FN_UV4SI_UINT)
+B_DEF      (s390_vlrepg,                vec_splatsv2di,     0,                  B_VX,               0,                  BT_FN_UV2DI_ULONGLONG)
+B_DEF      (s390_vlrepg_dbl,            vec_splatsv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_V2DF_DBL)
+B_DEF      (s390_vrepib,                vec_splatsv16qi,    0,                  B_VX,               O1_U8,              BT_FN_V16QI_UCHAR)
+B_DEF      (s390_vrepih,                vec_splatsv8hi,     0,                  B_VX,               O1_S16,             BT_FN_V8HI_SHORT)
+B_DEF      (s390_vrepif,                vec_splatsv4si,     0,                  B_VX,               O1_S16,             BT_FN_V4SI_SHORT)
+B_DEF      (s390_vrepig,                vec_splatsv2di,     0,                  B_VX,               O1_S16,             BT_FN_V2DI_SHORT)
+
+B_DEF      (s390_vec_splat_u8,          vec_splatsv16qi,    0,                  B_VX,               O1_U8,              BT_FN_UV16QI_UCHAR)
+B_DEF      (s390_vec_splat_s8,          vec_splatsv16qi,    0,                  B_VX,               O1_S8,              BT_FN_V16QI_SCHAR)
+B_DEF      (s390_vec_splat_u16,         vec_splatsv8hi,     0,                  B_VX,               O1_U16,             BT_FN_UV8HI_USHORT)
+B_DEF      (s390_vec_splat_s16,         vec_splatsv8hi,     0,                  B_VX,               O1_S16,             BT_FN_V8HI_SHORT)
+B_DEF      (s390_vec_splat_u32,         vec_splatsv4si,     0,                  B_VX,               O1_U16,             BT_FN_UV4SI_USHORT)
+B_DEF      (s390_vec_splat_s32,         vec_splatsv4si,     0,                  B_VX,               O1_S16,             BT_FN_V4SI_SHORT)
+B_DEF      (s390_vec_splat_u64,         vec_splatsv2di,     0,                  B_VX,               O1_U16,             BT_FN_UV2DI_USHORT)
+B_DEF      (s390_vec_splat_s64,         vec_splatsv2di,     0,                  B_VX,               O1_S16,             BT_FN_V2DI_SHORT)
+
+OB_DEF     (s390_vec_insert,            s390_vec_insert_s8, s390_vec_insert_dbl,B_VX,               BT_FN_OV4SI_INT_OV4SI_INT)
+OB_DEF_VAR (s390_vec_insert_s8,         s390_vlvgb,         O3_ELEM,            BT_OV_V16QI_SCHAR_V16QI_INT)
+OB_DEF_VAR (s390_vec_insert_u8,         s390_vlvgb,         O3_ELEM,            BT_OV_UV16QI_UCHAR_UV16QI_INT)
+OB_DEF_VAR (s390_vec_insert_b8,         s390_vlvgb,         O3_ELEM,            BT_OV_UV16QI_UCHAR_BV16QI_INT)
+OB_DEF_VAR (s390_vec_insert_s16,        s390_vlvgh,         O3_ELEM,            BT_OV_V8HI_SHORT_V8HI_INT)
+OB_DEF_VAR (s390_vec_insert_u16,        s390_vlvgh,         O3_ELEM,            BT_OV_UV8HI_USHORT_UV8HI_INT)
+OB_DEF_VAR (s390_vec_insert_b16,        s390_vlvgh,         O3_ELEM,            BT_OV_UV8HI_USHORT_BV8HI_INT)
+OB_DEF_VAR (s390_vec_insert_s32,        s390_vlvgf,         O3_ELEM,            BT_OV_V4SI_INT_V4SI_INT)
+OB_DEF_VAR (s390_vec_insert_u32,        s390_vlvgf,         O3_ELEM,            BT_OV_UV4SI_UINT_UV4SI_INT)
+OB_DEF_VAR (s390_vec_insert_b32,        s390_vlvgf,         O3_ELEM,            BT_OV_UV4SI_UINT_BV4SI_INT)
+OB_DEF_VAR (s390_vec_insert_s64,        s390_vlvgg,         O3_ELEM,            BT_OV_V2DI_LONGLONG_V2DI_INT)
+OB_DEF_VAR (s390_vec_insert_u64,        s390_vlvgg,         O3_ELEM,            BT_OV_UV2DI_ULONGLONG_UV2DI_INT)
+OB_DEF_VAR (s390_vec_insert_b64,        s390_vlvgg,         O3_ELEM,            BT_OV_UV2DI_ULONGLONG_BV2DI_INT)
+OB_DEF_VAR (s390_vec_insert_dbl,        s390_vlvgg_dbl,     O3_ELEM,            BT_OV_V2DF_DBL_V2DF_INT)
+
+B_DEF      (s390_vlvgb,                 vec_insertv16qi,    0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UCHAR_INT)
+B_DEF      (s390_vlvgh,                 vec_insertv8hi,     0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_USHORT_INT)
+B_DEF      (s390_vlvgf,                 vec_insertv4si,     0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UINT_INT)
+B_DEF      (s390_vlvgg,                 vec_insertv2di,     0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_ULONGLONG_INT)
+B_DEF      (s390_vlvgg_dbl,             vec_insertv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_V2DF_V2DF_DBL_INT)
+
+OB_DEF     (s390_vec_promote,           s390_vec_promote_s8,s390_vec_promote_dbl,B_VX,              BT_FN_OV4SI_INT_INT)
+OB_DEF_VAR (s390_vec_promote_s8,        s390_vlvgb_noin,    O2_ELEM,            BT_OV_V16QI_SCHAR_INT)                   /* vlvgb */
+OB_DEF_VAR (s390_vec_promote_u8,        s390_vlvgb_noin,    O2_ELEM,            BT_OV_UV16QI_UCHAR_INT)                  /* vlvgb */
+OB_DEF_VAR (s390_vec_promote_s16,       s390_vlvgh_noin,    O2_ELEM,            BT_OV_V8HI_SHORT_INT)                    /* vlvgh */
+OB_DEF_VAR (s390_vec_promote_u16,       s390_vlvgh_noin,    O2_ELEM,            BT_OV_UV8HI_USHORT_INT)                  /* vlvgh */
+OB_DEF_VAR (s390_vec_promote_s32,       s390_vlvgf_noin,    O2_ELEM,            BT_OV_V4SI_INT_INT)                      /* vlvgf */
+OB_DEF_VAR (s390_vec_promote_u32,       s390_vlvgf_noin,    O2_ELEM,            BT_OV_UV4SI_UINT_INT)                    /* vlvgf */
+OB_DEF_VAR (s390_vec_promote_s64,       s390_vlvgg_noin,    O2_ELEM,            BT_OV_V2DI_LONGLONG_INT)                 /* vlvgg */
+OB_DEF_VAR (s390_vec_promote_u64,       s390_vlvgg_noin,    O2_ELEM,            BT_OV_UV2DI_ULONGLONG_INT)               /* vlvgg */
+OB_DEF_VAR (s390_vec_promote_dbl,       s390_vlvgg_dbl_noin,O2_ELEM,            BT_OV_V2DF_DBL_INT)                      /* vlvgg */
+
+B_DEF      (s390_vlvgb_noin,            vec_promotev16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_UV16QI_UCHAR_INT)
+B_DEF      (s390_vlvgh_noin,            vec_promotev8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_UV8HI_USHORT_INT)
+B_DEF      (s390_vlvgf_noin,            vec_promotev4si,    0,                  B_VX | B_INT,       0,                  BT_FN_UV4SI_UINT_INT)
+B_DEF      (s390_vlvgg_noin,            vec_promotev2di,    0,                  B_VX | B_INT,       0,                  BT_FN_UV2DI_ULONGLONG_INT)
+B_DEF      (s390_vlvgg_dbl_noin,        vec_promotev2df,    0,                  B_VX | B_INT,       0,                  BT_FN_V2DF_DBL_INT)
+
+OB_DEF     (s390_vec_extract,           s390_vec_extract_s8,s390_vec_extract_dbl,B_VX,              BT_FN_INT_OV4SI_INT)
+OB_DEF_VAR (s390_vec_extract_s8,        s390_vlgvb,         O2_ELEM,            BT_OV_SCHAR_V16QI_INT)
+OB_DEF_VAR (s390_vec_extract_u8,        s390_vlgvb,         O2_ELEM,            BT_OV_UCHAR_UV16QI_INT)
+OB_DEF_VAR (s390_vec_extract_b8,        s390_vlgvb,         O2_ELEM,            BT_OV_UCHAR_BV16QI_INT)
+OB_DEF_VAR (s390_vec_extract_s16,       s390_vlgvh,         O2_ELEM,            BT_OV_SHORT_V8HI_INT)
+OB_DEF_VAR (s390_vec_extract_u16,       s390_vlgvh,         O2_ELEM,            BT_OV_USHORT_UV8HI_INT)
+OB_DEF_VAR (s390_vec_extract_b16,       s390_vlgvh,         O2_ELEM,            BT_OV_USHORT_BV8HI_INT)
+OB_DEF_VAR (s390_vec_extract_s32,       s390_vlgvf,         O2_ELEM,            BT_OV_INT_V4SI_INT)
+OB_DEF_VAR (s390_vec_extract_u32,       s390_vlgvf,         O2_ELEM,            BT_OV_UINT_UV4SI_INT)
+OB_DEF_VAR (s390_vec_extract_b32,       s390_vlgvf,         O2_ELEM,            BT_OV_UINT_BV4SI_INT)
+OB_DEF_VAR (s390_vec_extract_s64,       s390_vlgvg,         O2_ELEM,            BT_OV_LONGLONG_V2DI_INT)
+OB_DEF_VAR (s390_vec_extract_u64,       s390_vlgvg,         O2_ELEM,            BT_OV_ULONGLONG_UV2DI_INT)
+OB_DEF_VAR (s390_vec_extract_b64,       s390_vlgvg,         O2_ELEM,            BT_OV_ULONGLONG_BV2DI_INT)
+OB_DEF_VAR (s390_vec_extract_dbl,       s390_vlgvg_dbl,     O2_ELEM,            BT_OV_DBL_V2DF_INT)                      /* vlgvg */
+
+B_DEF      (s390_vlgvb,                 vec_extractv16qi,   0,                  B_VX,               0,                  BT_FN_UCHAR_UV16QI_INT)
+B_DEF      (s390_vlgvh,                 vec_extractv8hi,    0,                  B_VX,               0,                  BT_FN_USHORT_UV8HI_INT)
+B_DEF      (s390_vlgvf,                 vec_extractv4si,    0,                  B_VX,               0,                  BT_FN_UINT_UV4SI_INT)
+B_DEF      (s390_vlgvg,                 vec_extractv2di,    0,                  B_VX,               0,                  BT_FN_ULONGLONG_UV2DI_INT)
+B_DEF      (s390_vlgvg_dbl,             vec_extractv2df,    0,                  B_VX | B_INT,       0,                  BT_FN_DBL_V2DF_INT)
+
+OB_DEF     (s390_vec_insert_and_zero,   s390_vec_insert_and_zero_s8,s390_vec_insert_and_zero_dbl,B_VX,BT_FN_OV4SI_INTCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_s8,s390_vllezb,        0,                  BT_OV_V16QI_SCHARCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_u8,s390_vllezb,        0,                  BT_OV_UV16QI_UCHARCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_s16,s390_vllezh,       0,                  BT_OV_V8HI_SHORTCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_u16,s390_vllezh,       0,                  BT_OV_UV8HI_USHORTCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_s32,s390_vllezf,       0,                  BT_OV_V4SI_INTCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_u32,s390_vllezf,       0,                  BT_OV_UV4SI_UINTCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_s64,s390_vllezg,       0,                  BT_OV_V2DI_LONGLONGCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_u64,s390_vllezg,       0,                  BT_OV_UV2DI_ULONGLONGCONSTPTR)
+OB_DEF_VAR (s390_vec_insert_and_zero_dbl,s390_vllezg,       0,                  BT_OV_V2DF_DBLCONSTPTR)
+
+B_DEF      (s390_vllezb,                vec_insert_and_zerov16qi,0,             B_VX,               0,                  BT_FN_UV16QI_UCHARCONSTPTR)
+B_DEF      (s390_vllezh,                vec_insert_and_zerov8hi,0,              B_VX,               0,                  BT_FN_UV8HI_USHORTCONSTPTR)
+B_DEF      (s390_vllezf,                vec_insert_and_zerov4si,0,              B_VX,               0,                  BT_FN_UV4SI_UINTCONSTPTR)
+B_DEF      (s390_vllezg,                vec_insert_and_zerov2di,0,              B_VX,               0,                  BT_FN_UV2DI_ULONGLONGCONSTPTR)
+
+OB_DEF     (s390_vec_load_bndry,        s390_vec_load_bndry_s8,s390_vec_load_bndry_dbl,B_VX,        BT_FN_OV4SI_INTCONSTPTR_INT)
+OB_DEF_VAR (s390_vec_load_bndry_s8,     s390_vlbb,          O2_U16,              BT_OV_V16QI_SCHARCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_u8,     s390_vlbb,          O2_U16,              BT_OV_UV16QI_UCHARCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_s16,    s390_vlbb,          O2_U16,              BT_OV_V8HI_SHORTCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_u16,    s390_vlbb,          O2_U16,              BT_OV_UV8HI_USHORTCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_s32,    s390_vlbb,          O2_U16,              BT_OV_V4SI_INTCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_u32,    s390_vlbb,          O2_U16,              BT_OV_UV4SI_UINTCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_s64,    s390_vlbb,          O2_U16,              BT_OV_V2DI_LONGLONGCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_u64,    s390_vlbb,          O2_U16,              BT_OV_UV2DI_ULONGLONGCONSTPTR_USHORT)
+OB_DEF_VAR (s390_vec_load_bndry_dbl,    s390_vlbb,          O2_U16,              BT_OV_V2DF_DBLCONSTPTR_USHORT)
+
+B_DEF      (s390_vlbb,                  vlbb,               0,                  B_VX,               O2_U3,              BT_FN_UV16QI_UCHARCONSTPTR_USHORT)
+
+OB_DEF     (s390_vec_load_pair,         s390_vec_load_pair_s64,s390_vec_load_pair_u64,B_VX,         BT_FN_OV2DI_LONGLONG_LONGLONG)
+OB_DEF_VAR (s390_vec_load_pair_s64,     MAX,                0,                  BT_OV_V2DI_LONGLONG_LONGLONG)            /* vlvgp */
+OB_DEF_VAR (s390_vec_load_pair_u64,     MAX,                0,                  BT_OV_UV2DI_ULONGLONG_ULONGLONG)         /* vlvgp */
+
+OB_DEF     (s390_vec_load_len,          s390_vec_load_len_s8,s390_vec_load_len_dbl,B_VX,            BT_FN_OV4SI_INTCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_s8,       s390_vll,           0,                  BT_OV_V16QI_SCHARCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_u8,       s390_vll,           0,                  BT_OV_UV16QI_UCHARCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_s16,      s390_vll,           0,                  BT_OV_V8HI_SHORTCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_u16,      s390_vll,           0,                  BT_OV_UV8HI_USHORTCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_s32,      s390_vll,           0,                  BT_OV_V4SI_INTCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_u32,      s390_vll,           0,                  BT_OV_UV4SI_UINTCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_s64,      s390_vll,           0,                  BT_OV_V2DI_LONGLONGCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_u64,      s390_vll,           0,                  BT_OV_UV2DI_ULONGLONGCONSTPTR_UINT)
+OB_DEF_VAR (s390_vec_load_len_dbl,      s390_vll,           0,                  BT_OV_V2DF_DBLCONSTPTR_UINT)
+
+B_DEF      (s390_vll,                   vllv16qi,           0,                  B_VX,               0,                  BT_FN_V16QI_UINT_VOIDCONSTPTR)
+
+OB_DEF     (s390_vec_mergeh,            s390_vec_mergeh_s8, s390_vec_mergeh_dbl,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_mergeh_s8,         s390_vmrhb,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mergeh_u8,         s390_vmrhb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mergeh_b8,         s390_vmrhb,         0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_mergeh_s16,        s390_vmrhh,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mergeh_u16,        s390_vmrhh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mergeh_b16,        s390_vmrhh,         0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_mergeh_s32,        s390_vmrhf,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_mergeh_u32,        s390_vmrhf,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mergeh_b32,        s390_vmrhf,         0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_mergeh_s64,        s390_vmrhg,         0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_mergeh_u64,        s390_vmrhg,         0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_mergeh_b64,        s390_vmrhg,         0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_mergeh_dbl,        s390_vmrhg,         0,                  BT_OV_V2DF_V2DF_V2DF)
+
+B_DEF      (s390_vmrhb,                 vec_mergehv16qi,    0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vmrhh,                 vec_mergehv8hi,     0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vmrhf,                 vec_mergehv4si,     0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vmrhg,                 vec_mergehv2di,     0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_mergel,            s390_vec_mergel_s8, s390_vec_mergel_dbl,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_mergel_s8,         s390_vmrlb,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mergel_u8,         s390_vmrlb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mergel_b8,         s390_vmrlb,         0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_mergel_s16,        s390_vmrlh,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mergel_u16,        s390_vmrlh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mergel_b16,        s390_vmrlh,         0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_mergel_s32,        s390_vmrlf,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_mergel_u32,        s390_vmrlf,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mergel_b32,        s390_vmrlf,         0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_mergel_s64,        s390_vmrlg,         0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_mergel_u64,        s390_vmrlg,         0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_mergel_b64,        s390_vmrlg,         0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_mergel_dbl,        s390_vmrlg,         0,                  BT_OV_V2DF_V2DF_V2DF)
+
+B_DEF      (s390_vmrlb,                 vec_mergelv16qi,    0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vmrlh,                 vec_mergelv8hi,     0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vmrlf,                 vec_mergelv4si,     0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vmrlg,                 vec_mergelv2di,     0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_pack,              s390_vec_pack_s16,  s390_vec_pack_b64,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_pack_s16,          s390_vpkh,          0,                  BT_OV_V16QI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_pack_u16,          s390_vpkh,          0,                  BT_OV_UV16QI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_pack_b16,          s390_vpkh,          0,                  BT_OV_BV16QI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_pack_s32,          s390_vpkf,          0,                  BT_OV_V8HI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_pack_u32,          s390_vpkf,          0,                  BT_OV_UV8HI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_pack_b32,          s390_vpkf,          0,                  BT_OV_BV8HI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_pack_s64,          s390_vpkg,          0,                  BT_OV_V4SI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_pack_u64,          s390_vpkg,          0,                  BT_OV_UV4SI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_pack_b64,          s390_vpkg,          0,                  BT_OV_BV4SI_BV2DI_BV2DI)
+
+B_DEF      (s390_vpkh,                  vec_packv8hi,       0,                  B_VX,               0,                  BT_FN_UV16QI_UV8HI_UV8HI)
+B_DEF      (s390_vpkf,                  vec_packv4si,       0,                  B_VX,               0,                  BT_FN_UV8HI_UV4SI_UV4SI)
+B_DEF      (s390_vpkg,                  vec_packv2di,       0,                  B_VX,               0,                  BT_FN_UV4SI_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_packs,             s390_vec_packs_s16, s390_vec_packs_u64, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_packs_s16,         s390_vpksh,         0,                  BT_OV_V16QI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_packs_u16,         s390_vpklsh,        0,                  BT_OV_UV16QI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_packs_s32,         s390_vpksf,         0,                  BT_OV_V8HI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_packs_u32,         s390_vpklsf,        0,                  BT_OV_UV8HI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_packs_s64,         s390_vpksg,         0,                  BT_OV_V4SI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_packs_u64,         s390_vpklsg,        0,                  BT_OV_UV4SI_UV2DI_UV2DI)
+
+B_DEF      (s390_vpksh,                 vec_packsv8hi,      0,                  B_VX,               0,                  BT_FN_V16QI_V8HI_V8HI)
+B_DEF      (s390_vpklsh,                vec_packsuv8hi,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV8HI_UV8HI)
+B_DEF      (s390_vpksf,                 vec_packsv4si,      0,                  B_VX,               0,                  BT_FN_V8HI_V4SI_V4SI)
+B_DEF      (s390_vpklsf,                vec_packsuv4si,     0,                  B_VX,               0,                  BT_FN_UV8HI_UV4SI_UV4SI)
+B_DEF      (s390_vpksg,                 vec_packsv2di,      0,                  B_VX,               0,                  BT_FN_V4SI_V2DI_V2DI)
+B_DEF      (s390_vpklsg,                vec_packsuv2di,     0,                  B_VX,               0,                  BT_FN_UV4SI_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_packs_cc,          s390_vec_packs_cc_s16,s390_vec_packs_cc_u64,B_VX,           BT_FN_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vec_packs_cc_s16,      s390_vpkshs,        0,                  BT_OV_V16QI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vec_packs_cc_u16,      s390_vpklshs,       0,                  BT_OV_UV16QI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vec_packs_cc_s32,      s390_vpksfs,        0,                  BT_OV_V8HI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vec_packs_cc_u32,      s390_vpklsfs,       0,                  BT_OV_UV8HI_UV4SI_UV4SI_INTPTR)
+OB_DEF_VAR (s390_vec_packs_cc_s64,      s390_vpksgs,        0,                  BT_OV_V4SI_V2DI_V2DI_INTPTR)
+OB_DEF_VAR (s390_vec_packs_cc_u64,      s390_vpklsgs,       0,                  BT_OV_UV4SI_UV2DI_UV2DI_INTPTR)
+
+B_DEF      (s390_vpkshs,                vec_packs_ccv8hi,   0,                  B_VX,               0,                  BT_FN_V16QI_V8HI_V8HI_INTPTR)
+B_DEF      (s390_vpklshs,               vec_packsu_ccv8hi,  0,                  B_VX,               0,                  BT_FN_UV16QI_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vpksfs,                vec_packs_ccv4si,   0,                  B_VX,               0,                  BT_FN_V8HI_V4SI_V4SI_INTPTR)
+B_DEF      (s390_vpklsfs,               vec_packsu_ccv4si,  0,                  B_VX,               0,                  BT_FN_UV8HI_UV4SI_UV4SI_INTPTR)
+B_DEF      (s390_vpksgs,                vec_packs_ccv2di,   0,                  B_VX,               0,                  BT_FN_V4SI_V2DI_V2DI_INTPTR)
+B_DEF      (s390_vpklsgs,               vec_packsu_ccv2di,  0,                  B_VX,               0,                  BT_FN_UV4SI_UV2DI_UV2DI_INTPTR)
+
+OB_DEF     (s390_vec_packsu,            s390_vec_packsu_s16,s390_vec_packsu_u64,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_packsu_s16,        s390_vec_packsu_u16,0,                  BT_OV_UV16QI_V8HI_V8HI)                  /* vpklsh */
+OB_DEF_VAR (s390_vec_packsu_u16,        s390_vpklsh,        0,                  BT_OV_UV16QI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_packsu_s32,        s390_vec_packsu_u32,0,                  BT_OV_UV8HI_V4SI_V4SI)                   /* vpklsf */
+OB_DEF_VAR (s390_vec_packsu_u32,        s390_vpklsf,        0,                  BT_OV_UV8HI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_packsu_s64,        s390_vec_packsu_u64,0,                  BT_OV_UV4SI_V2DI_V2DI)                   /* vpklsg */
+OB_DEF_VAR (s390_vec_packsu_u64,        s390_vpklsg,        0,                  BT_OV_UV4SI_UV2DI_UV2DI)
+
+B_DEF      (s390_vec_packsu_u16,        vec_packsu_uv8hi,   0,                  B_VX | B_INT,       0,                  BT_FN_UV16QI_UV8HI_UV8HI)                /* vpklsh */
+B_DEF      (s390_vec_packsu_u32,        vec_packsu_uv4si,   0,                  B_VX | B_INT,       0,                  BT_FN_UV8HI_UV4SI_UV4SI)                 /* vpklsf */
+B_DEF      (s390_vec_packsu_u64,        vec_packsu_uv2di,   0,                  B_VX | B_INT,       0,                  BT_FN_UV4SI_UV2DI_UV2DI)                 /* vpklsg */
+
+OB_DEF     (s390_vec_packsu_cc,         s390_vec_packsu_cc_u16,s390_vec_packsu_cc_u64,B_VX,         BT_FN_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vec_packsu_cc_u16,     s390_vpklshs,       0,                  BT_OV_UV16QI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vec_packsu_cc_u32,     s390_vpklsfs,       0,                  BT_OV_UV8HI_UV4SI_UV4SI_INTPTR)
+OB_DEF_VAR (s390_vec_packsu_cc_u64,     s390_vpklsgs,       0,                  BT_OV_UV4SI_UV2DI_UV2DI_INTPTR)
+
+OB_DEF     (s390_vec_perm,              s390_vec_perm_s8,   s390_vec_perm_dbl,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_perm_s8,           s390_vperm,         0,                  BT_OV_V16QI_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_b8,           s390_vperm,         0,                  BT_OV_BV16QI_BV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_u8,           s390_vperm,         0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_s16,          s390_vperm,         0,                  BT_OV_V8HI_V8HI_V8HI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_b16,          s390_vperm,         0,                  BT_OV_BV8HI_BV8HI_BV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_u16,          s390_vperm,         0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_s32,          s390_vperm,         0,                  BT_OV_V4SI_V4SI_V4SI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_b32,          s390_vperm,         0,                  BT_OV_BV4SI_BV4SI_BV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_u32,          s390_vperm,         0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_s64,          s390_vperm,         0,                  BT_OV_V2DI_V2DI_V2DI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_b64,          s390_vperm,         0,                  BT_OV_BV2DI_BV2DI_BV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_u64,          s390_vperm,         0,                  BT_OV_UV2DI_UV2DI_UV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_perm_dbl,          s390_vperm,         0,                  BT_OV_V2DF_V2DF_V2DF_UV16QI)
+
+B_DEF      (s390_vperm,                 vec_permv16qi,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_permi,             s390_vec_permi_s64, s390_vec_permi_dbl, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_INT)
+OB_DEF_VAR (s390_vec_permi_s64,         s390_vpdi,          O3_U2,              BT_OV_V2DI_V2DI_V2DI_INT)
+OB_DEF_VAR (s390_vec_permi_b64,         s390_vpdi,          O3_U2,              BT_OV_BV2DI_BV2DI_BV2DI_INT)
+OB_DEF_VAR (s390_vec_permi_u64,         s390_vpdi,          O3_U2,              BT_OV_UV2DI_UV2DI_UV2DI_INT)
+OB_DEF_VAR (s390_vec_permi_dbl,         s390_vpdi,          O3_U2,              BT_OV_V2DF_V2DF_V2DF_INT)
+
+B_DEF      (s390_vpdi,                  vec_permiv2di,      0,                  B_VX,               O3_U2,              BT_FN_UV2DI_UV2DI_UV2DI_INT)
+
+OB_DEF     (s390_vec_splat,             s390_vec_splat2_s8, s390_vec_splat2_dbl,B_VX,               BT_FN_OV4SI_OV4SI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_s8,         s390_vrepb,         O2_U4,              BT_OV_V16QI_V16QI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_b8,         s390_vrepb,         O2_U4,              BT_OV_BV16QI_BV16QI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_u8,         s390_vrepb,         O2_U4,              BT_OV_UV16QI_UV16QI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_s16,        s390_vreph,         O2_U3,              BT_OV_V8HI_V8HI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_b16,        s390_vreph,         O2_U3,              BT_OV_BV8HI_BV8HI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_u16,        s390_vreph,         O2_U3,              BT_OV_UV8HI_UV8HI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_s32,        s390_vrepf,         O2_U2,              BT_OV_V4SI_V4SI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_b32,        s390_vrepf,         O2_U2,              BT_OV_BV4SI_BV4SI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_u32,        s390_vrepf,         O2_U2,              BT_OV_UV4SI_UV4SI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_s64,        s390_vrepg,         O2_U1,              BT_OV_V2DI_V2DI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_b64,        s390_vrepg,         O2_U1,              BT_OV_BV2DI_BV2DI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_u64,        s390_vrepg,         O2_U1,              BT_OV_UV2DI_UV2DI_UCHAR)
+OB_DEF_VAR (s390_vec_splat2_dbl,        s390_vrepg,         O2_U1,              BT_OV_V2DF_V2DF_UCHAR)
+
+B_DEF      (s390_vrepb,                 vec_splatv16qi,     0,                  B_VX,               O2_U4,              BT_FN_UV16QI_UV16QI_UCHAR)
+B_DEF      (s390_vreph,                 vec_splatv8hi,      0,                  B_VX,               O2_U3,              BT_FN_UV8HI_UV8HI_UCHAR)
+B_DEF      (s390_vrepf,                 vec_splatv4si,      0,                  B_VX,               O2_U2,              BT_FN_UV4SI_UV4SI_UCHAR)
+B_DEF      (s390_vrepg,                 vec_splatv2di,      0,                  B_VX,               O2_U1,              BT_FN_UV2DI_UV2DI_UCHAR)
+
+OB_DEF     (s390_vec_scatter_element,   s390_vec_scatter_element_s32,s390_vec_scatter_element_dbl,B_VX,BT_FN_VOID_V4SI_V4SI_INTPTR_ULONGLONG)
+OB_DEF_VAR (s390_vec_scatter_element_s32,s390_vscef,        O4_U2,              BT_OV_VOID_V4SI_UV4SI_INTPTR_ULONGLONG)
+OB_DEF_VAR (s390_vec_scatter_element_b32,s390_vscef,        O4_U2,              BT_OV_VOID_BV4SI_UV4SI_UINTPTR_ULONGLONG)
+OB_DEF_VAR (s390_vec_scatter_element_u32,s390_vscef,        O4_U2,              BT_OV_VOID_UV4SI_UV4SI_UINTPTR_ULONGLONG)
+OB_DEF_VAR (s390_vec_scatter_element_s64,s390_vsceg,        O4_U1,              BT_OV_VOID_V2DI_UV2DI_LONGLONGPTR_ULONGLONG)
+OB_DEF_VAR (s390_vec_scatter_element_b64,s390_vsceg,        O4_U1,              BT_OV_VOID_BV2DI_UV2DI_ULONGLONGPTR_ULONGLONG)
+OB_DEF_VAR (s390_vec_scatter_element_u64,s390_vsceg,        O4_U1,              BT_OV_VOID_UV2DI_UV2DI_ULONGLONGPTR_ULONGLONG)
+OB_DEF_VAR (s390_vec_scatter_element_dbl,s390_vsceg,        O4_U1,              BT_OV_VOID_V2DF_UV2DI_DBLPTR_ULONGLONG)
+
+B_DEF      (s390_vscef,                 vec_scatter_elementv4si,0,              B_VX,               O4_U2,              BT_FN_VOID_UV4SI_UV4SI_UINTPTR_ULONGLONG)
+B_DEF      (s390_vsceg,                 vec_scatter_elementv2di,0,              B_VX,               O4_U1,              BT_FN_VOID_UV2DI_UV2DI_ULONGLONGPTR_ULONGLONG)
+
+OB_DEF     (s390_vec_sel,               s390_vec_sel_b8_a,  s390_vec_sel_dbl_b, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_sel_b8_a,          s390_vsel,          0,                  BT_OV_BV16QI_BV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sel_b8_b,          s390_vsel,          0,                  BT_OV_BV16QI_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_sel_s8_a,          s390_vsel,          0,                  BT_OV_V16QI_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sel_s8_b,          s390_vsel,          0,                  BT_OV_V16QI_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_sel_u8_a,          s390_vsel,          0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sel_u8_b,          s390_vsel,          0,                  BT_OV_UV16QI_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_sel_b16_a,         s390_vsel,          0,                  BT_OV_BV8HI_BV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sel_b16_b,         s390_vsel,          0,                  BT_OV_BV8HI_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_sel_s16_a,         s390_vsel,          0,                  BT_OV_V8HI_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sel_s16_b,         s390_vsel,          0,                  BT_OV_V8HI_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_sel_u16_a,         s390_vsel,          0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sel_u16_b,         s390_vsel,          0,                  BT_OV_UV8HI_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_sel_b32_a,         s390_vsel,          0,                  BT_OV_BV4SI_BV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sel_b32_b,         s390_vsel,          0,                  BT_OV_BV4SI_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_sel_s32_a,         s390_vsel,          0,                  BT_OV_V4SI_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sel_s32_b,         s390_vsel,          0,                  BT_OV_V4SI_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_sel_u32_a,         s390_vsel,          0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sel_u32_b,         s390_vsel,          0,                  BT_OV_UV4SI_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_sel_b64_a,         s390_vsel,          0,                  BT_OV_BV2DI_BV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_sel_b64_b,         s390_vsel,          0,                  BT_OV_BV2DI_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_sel_s64_a,         s390_vsel,          0,                  BT_OV_V2DI_V2DI_V2DI_UV2DI)
+OB_DEF_VAR (s390_vec_sel_s64_b,         s390_vsel,          0,                  BT_OV_V2DI_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_sel_u64_a,         s390_vsel,          0,                  BT_OV_UV2DI_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_sel_u64_b,         s390_vsel,          0,                  BT_OV_UV2DI_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_sel_dbl_a,         s390_vsel,          0,                  BT_OV_V2DF_V2DF_V2DF_UV2DI)
+OB_DEF_VAR (s390_vec_sel_dbl_b,         s390_vsel,          0,                  BT_OV_V2DF_V2DF_V2DF_BV2DI)
+
+B_DEF      (s390_vsel,                  vec_selv16qi,       0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_extend_s64,        s390_vec_extend_s64_s8,s390_vec_extend_s64_s32,B_VX,        BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_extend_s64_s8,     s390_vsegb,         0,                  BT_OV_V2DI_V16QI)
+OB_DEF_VAR (s390_vec_extend_s64_s16,    s390_vsegh,         0,                  BT_OV_V2DI_V8HI)
+OB_DEF_VAR (s390_vec_extend_s64_s32,    s390_vsegf,         0,                  BT_OV_V2DI_V4SI)
+
+B_DEF      (s390_vsegb,                 vec_extendv16qi,    0,                  B_VX,               0,                  BT_FN_V2DI_V16QI)
+B_DEF      (s390_vsegh,                 vec_extendv8hi,     0,                  B_VX,               0,                  BT_FN_V2DI_V8HI)
+B_DEF      (s390_vsegf,                 vec_extendv4si,     0,                  B_VX,               0,                  BT_FN_V2DI_V4SI)
+
+OB_DEF     (s390_vec_xstd2,             s390_vec_xstd2_s8,  s390_vec_xstd2_dbl, B_VX,               BT_FN_VOID_OV4SI_INT_VOIDPTR)
+OB_DEF_VAR (s390_vec_xstd2_s8,          MAX,                O2_LIT,             BT_OV_VOID_V16QI_LONG_SCHARPTR)          /* vst */
+OB_DEF_VAR (s390_vec_xstd2_u8,          MAX,                O2_LIT,             BT_OV_VOID_UV16QI_LONG_UCHARPTR)         /* vst */
+OB_DEF_VAR (s390_vec_xstd2_s16,         MAX,                O2_LIT,             BT_OV_VOID_V8HI_LONG_SHORTPTR)           /* vst */
+OB_DEF_VAR (s390_vec_xstd2_u16,         MAX,                O2_LIT,             BT_OV_VOID_UV8HI_LONG_USHORTPTR)         /* vst */
+OB_DEF_VAR (s390_vec_xstd2_s32,         MAX,                O2_LIT,             BT_OV_VOID_V4SI_LONG_INTPTR)             /* vst */
+OB_DEF_VAR (s390_vec_xstd2_u32,         MAX,                O2_LIT,             BT_OV_VOID_UV4SI_LONG_UINTPTR)           /* vst */
+OB_DEF_VAR (s390_vec_xstd2_s64,         MAX,                O2_LIT,             BT_OV_VOID_V2DI_LONG_LONGLONGPTR)        /* vst */
+OB_DEF_VAR (s390_vec_xstd2_u64,         MAX,                O2_LIT,             BT_OV_VOID_UV2DI_LONG_ULONGLONGPTR)      /* vst */
+OB_DEF_VAR (s390_vec_xstd2_dbl,         MAX,                O2_LIT,             BT_OV_VOID_V2DF_LONG_DBLPTR)             /* vst */
+
+OB_DEF     (s390_vec_xstw4,             s390_vec_xstw4_s8,  s390_vec_xstw4_u32, B_VX,               BT_FN_VOID_OV4SI_INT_VOIDPTR)
+OB_DEF_VAR (s390_vec_xstw4_s8,          MAX,                O2_LIT,             BT_OV_VOID_V16QI_LONG_SCHARPTR)          /* vst */
+OB_DEF_VAR (s390_vec_xstw4_u8,          MAX,                O2_LIT,             BT_OV_VOID_UV16QI_LONG_UCHARPTR)         /* vst */
+OB_DEF_VAR (s390_vec_xstw4_s16,         MAX,                O2_LIT,             BT_OV_VOID_V8HI_LONG_SHORTPTR)           /* vst */
+OB_DEF_VAR (s390_vec_xstw4_u16,         MAX,                O2_LIT,             BT_OV_VOID_UV8HI_LONG_USHORTPTR)         /* vst */
+OB_DEF_VAR (s390_vec_xstw4_s32,         MAX,                O2_LIT,             BT_OV_VOID_V4SI_LONG_INTPTR)             /* vst */
+OB_DEF_VAR (s390_vec_xstw4_u32,         MAX,                O2_LIT,             BT_OV_VOID_UV4SI_LONG_UINTPTR)           /* vst */
+
+OB_DEF     (s390_vec_store_len,         s390_vec_store_len_s8,s390_vec_store_len_dbl,B_VX,          BT_FN_VOID_OV4SI_VOIDPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_s8,      s390_vstl,          0,                  BT_OV_VOID_V16QI_SCHARPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_u8,      s390_vstl,          0,                  BT_OV_VOID_UV16QI_UCHARPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_s16,     s390_vstl,          0,                  BT_OV_VOID_V8HI_SHORTPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_u16,     s390_vstl,          0,                  BT_OV_VOID_UV8HI_USHORTPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_s32,     s390_vstl,          0,                  BT_OV_VOID_V4SI_INTPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_u32,     s390_vstl,          0,                  BT_OV_VOID_UV4SI_UINTPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_s64,     s390_vstl,          0,                  BT_OV_VOID_V2DI_LONGLONGPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_u64,     s390_vstl,          0,                  BT_OV_VOID_UV2DI_ULONGLONGPTR_UINT)
+OB_DEF_VAR (s390_vec_store_len_dbl,     s390_vstl,          0,                  BT_OV_VOID_V2DF_DBLPTR_UINT)
+
+B_DEF      (s390_vstl,                  vstlv16qi,          0,                  B_VX,               0,                  BT_FN_VOID_V16QI_UINT_VOIDPTR)
+
+OB_DEF     (s390_vec_unpackh,           s390_vec_unpackh_s8,s390_vec_unpackh_u32,B_VX,              BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_unpackh_s8,        s390_vuphb,         0,                  BT_OV_V8HI_V16QI)
+OB_DEF_VAR (s390_vec_unpackh_b8,        s390_vuphb,         0,                  BT_OV_BV8HI_BV16QI)
+OB_DEF_VAR (s390_vec_unpackh_u8,        s390_vuplhb,        0,                  BT_OV_UV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_unpackh_s16,       s390_vuphh,         0,                  BT_OV_V4SI_V8HI)
+OB_DEF_VAR (s390_vec_unpackh_b16,       s390_vuphh,         0,                  BT_OV_BV4SI_BV8HI)
+OB_DEF_VAR (s390_vec_unpackh_u16,       s390_vuplhh,        0,                  BT_OV_UV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_unpackh_s32,       s390_vuphf,         0,                  BT_OV_V2DI_V4SI)
+OB_DEF_VAR (s390_vec_unpackh_b32,       s390_vuphf,         0,                  BT_OV_BV2DI_BV4SI)
+OB_DEF_VAR (s390_vec_unpackh_u32,       s390_vuplhf,        0,                  BT_OV_UV2DI_UV4SI)
+
+B_DEF      (s390_vuphb,                 vec_unpackhv16qi,   0,                  B_VX,               0,                  BT_FN_V8HI_V16QI)
+B_DEF      (s390_vuplhb,                vec_unpackh_lv16qi, 0,                  B_VX,               0,                  BT_FN_UV8HI_UV16QI)
+B_DEF      (s390_vuphh,                 vec_unpackhv8hi,    0,                  B_VX,               0,                  BT_FN_V4SI_V8HI)
+B_DEF      (s390_vuplhh,                vec_unpackh_lv8hi,  0,                  B_VX,               0,                  BT_FN_UV4SI_UV8HI)
+B_DEF      (s390_vuphf,                 vec_unpackhv4si,    0,                  B_VX,               0,                  BT_FN_V2DI_V4SI)
+B_DEF      (s390_vuplhf,                vec_unpackh_lv4si,  0,                  B_VX,               0,                  BT_FN_UV2DI_UV4SI)
+
+OB_DEF     (s390_vec_unpackl,           s390_vec_unpackl_s8,s390_vec_unpackl_u32,B_VX,              BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_unpackl_s8,        s390_vuplb,         0,                  BT_OV_V8HI_V16QI)
+OB_DEF_VAR (s390_vec_unpackl_b8,        s390_vuplb,         0,                  BT_OV_BV8HI_BV16QI)
+OB_DEF_VAR (s390_vec_unpackl_u8,        s390_vupllb,        0,                  BT_OV_UV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_unpackl_s16,       s390_vuplhw,        0,                  BT_OV_V4SI_V8HI)
+OB_DEF_VAR (s390_vec_unpackl_b16,       s390_vupllh,        0,                  BT_OV_BV4SI_BV8HI)
+OB_DEF_VAR (s390_vec_unpackl_u16,       s390_vupllh,        0,                  BT_OV_UV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_unpackl_s32,       s390_vuplf,         0,                  BT_OV_V2DI_V4SI)
+OB_DEF_VAR (s390_vec_unpackl_b32,       s390_vuplf,         0,                  BT_OV_BV2DI_BV4SI)
+OB_DEF_VAR (s390_vec_unpackl_u32,       s390_vupllf,        0,                  BT_OV_UV2DI_UV4SI)
+
+B_DEF      (s390_vuplb,                 vec_unpacklv16qi,   0,                  B_VX,               0,                  BT_FN_V8HI_V16QI)
+B_DEF      (s390_vupllb,                vec_unpackl_lv16qi, 0,                  B_VX,               0,                  BT_FN_UV8HI_UV16QI)
+B_DEF      (s390_vuplhw,                vec_unpacklv8hi,    0,                  B_VX,               0,                  BT_FN_V4SI_V8HI)
+B_DEF      (s390_vupllh,                vec_unpackl_lv8hi,  0,                  B_VX,               0,                  BT_FN_UV4SI_UV8HI)
+B_DEF      (s390_vuplf,                 vec_unpacklv4si,    0,                  B_VX,               0,                  BT_FN_V2DI_V4SI)
+B_DEF      (s390_vupllf,                vec_unpackl_lv4si,  0,                  B_VX,               0,                  BT_FN_UV2DI_UV4SI)
+B_DEF      (s390_vaq,                   vec_add_u128,       0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_addc,              s390_vec_addc_u8,   s390_vec_addc_u64,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_addc_u8,           s390_vaccb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_addc_u16,          s390_vacch,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_addc_u32,          s390_vaccf,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_addc_u64,          s390_vaccg,         0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+
+B_DEF      (s390_vaccb,                 vec_addcv16qi,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vacch,                 vec_addcv8hi,       0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vaccf,                 vec_addcv4si,       0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vaccg,                 vec_addcv2di,       0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+B_DEF      (s390_vaccq,                 vec_addc_u128,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vacq,                  vec_adde_u128,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vacccq,                vec_addec_u128,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_and,               s390_vec_and_b8,    s390_vec_and_dbl_c, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_and_b8,            s390_vn,            0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_and_s8_a,          s390_vn,            0,                  BT_OV_V16QI_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_and_s8_b,          s390_vn,            0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_and_s8_c,          s390_vn,            0,                  BT_OV_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_and_u8_a,          s390_vn,            0,                  BT_OV_UV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_and_u8_b,          s390_vn,            0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_and_u8_c,          s390_vn,            0,                  BT_OV_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_and_b16,           s390_vn,            0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_and_s16_a,         s390_vn,            0,                  BT_OV_V8HI_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_and_s16_b,         s390_vn,            0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_and_s16_c,         s390_vn,            0,                  BT_OV_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_and_u16_a,         s390_vn,            0,                  BT_OV_UV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_and_u16_b,         s390_vn,            0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_and_u16_c,         s390_vn,            0,                  BT_OV_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_and_b32,           s390_vn,            0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_and_s32_a,         s390_vn,            0,                  BT_OV_V4SI_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_and_s32_b,         s390_vn,            0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_and_s32_c,         s390_vn,            0,                  BT_OV_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_and_u32_a,         s390_vn,            0,                  BT_OV_UV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_and_u32_b,         s390_vn,            0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_and_u32_c,         s390_vn,            0,                  BT_OV_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_and_b64,           s390_vn,            0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_and_s64_a,         s390_vn,            0,                  BT_OV_V2DI_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_and_s64_b,         s390_vn,            0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_and_s64_c,         s390_vn,            0,                  BT_OV_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_and_u64_a,         s390_vn,            0,                  BT_OV_UV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_and_u64_b,         s390_vn,            0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_and_u64_c,         s390_vn,            0,                  BT_OV_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_and_dbl_a,         s390_vn,            0,                  BT_OV_V2DF_BV2DI_V2DF)
+OB_DEF_VAR (s390_vec_and_dbl_b,         s390_vn,            0,                  BT_OV_V2DF_V2DF_V2DF)
+OB_DEF_VAR (s390_vec_and_dbl_c,         s390_vn,            0,                  BT_OV_V2DF_V2DF_BV2DI)
+
+B_DEF      (s390_vn,                    andv16qi3,          0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_andc,              s390_vec_andc_b8,   s390_vec_andc_dbl_c,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_andc_b8,           s390_vnc,           0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_andc_s8_a,         s390_vnc,           0,                  BT_OV_V16QI_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_andc_s8_b,         s390_vnc,           0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_andc_s8_c,         s390_vnc,           0,                  BT_OV_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_andc_u8_a,         s390_vnc,           0,                  BT_OV_UV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_andc_u8_b,         s390_vnc,           0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_andc_u8_c,         s390_vnc,           0,                  BT_OV_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_andc_b16,          s390_vnc,           0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_andc_s16_a,        s390_vnc,           0,                  BT_OV_V8HI_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_andc_s16_b,        s390_vnc,           0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_andc_s16_c,        s390_vnc,           0,                  BT_OV_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_andc_u16_a,        s390_vnc,           0,                  BT_OV_UV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_andc_u16_b,        s390_vnc,           0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_andc_u16_c,        s390_vnc,           0,                  BT_OV_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_andc_b32,          s390_vnc,           0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_andc_s32_a,        s390_vnc,           0,                  BT_OV_V4SI_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_andc_s32_b,        s390_vnc,           0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_andc_s32_c,        s390_vnc,           0,                  BT_OV_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_andc_u32_a,        s390_vnc,           0,                  BT_OV_UV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_andc_u32_b,        s390_vnc,           0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_andc_u32_c,        s390_vnc,           0,                  BT_OV_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_andc_b64,          s390_vnc,           0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_andc_s64_a,        s390_vnc,           0,                  BT_OV_V2DI_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_andc_s64_b,        s390_vnc,           0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_andc_s64_c,        s390_vnc,           0,                  BT_OV_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_andc_u64_a,        s390_vnc,           0,                  BT_OV_UV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_andc_u64_b,        s390_vnc,           0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_andc_u64_c,        s390_vnc,           0,                  BT_OV_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_andc_dbl_a,        s390_vnc,           0,                  BT_OV_V2DF_BV2DI_V2DF)
+OB_DEF_VAR (s390_vec_andc_dbl_b,        s390_vnc,           0,                  BT_OV_V2DF_V2DF_V2DF)
+OB_DEF_VAR (s390_vec_andc_dbl_c,        s390_vnc,           0,                  BT_OV_V2DF_V2DF_BV2DI)
+
+B_DEF      (s390_vnc,                   vec_andcv16qi3,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_avg,               s390_vec_avg_s8,    s390_vec_avg_u64,   B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_avg_s8,            s390_vavgb,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_avg_u8,            s390_vavglb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_avg_s16,           s390_vavgh,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_avg_u16,           s390_vavglh,        0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_avg_s32,           s390_vavgf,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_avg_u32,           s390_vavglf,        0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_avg_s64,           s390_vavgg,         0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_avg_u64,           s390_vavglg,        0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+
+B_DEF      (s390_vavgb,                 vec_avgv16qi,       0,                  B_VX,               0,                  BT_FN_V16QI_V16QI_V16QI)
+B_DEF      (s390_vavglb,                vec_avguv16qi,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vavgh,                 vec_avgv8hi,        0,                  B_VX,               0,                  BT_FN_V8HI_V8HI_V8HI)
+B_DEF      (s390_vavglh,                vec_avguv8hi,       0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vavgf,                 vec_avgv4si,        0,                  B_VX,               0,                  BT_FN_V4SI_V4SI_V4SI)
+B_DEF      (s390_vavglf,                vec_avguv4si,       0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vavgg,                 vec_avgv2di,        0,                  B_VX,               0,                  BT_FN_V2DI_V2DI_V2DI)
+B_DEF      (s390_vavglg,                vec_avguv2di,       0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+B_DEF      (s390_vcksm,                 vec_checksum,       0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vceqbs,                vec_cmpeqv16qi_cc,  0,                  B_VX,               0,                  BT_FN_V16QI_UV16QI_UV16QI_INTPTR)
+B_DEF      (s390_vceqhs,                vec_cmpeqv8hi_cc,   0,                  B_VX,               0,                  BT_FN_V8HI_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vceqfs,                vec_cmpeqv4si_cc,   0,                  B_VX,               0,                  BT_FN_V4SI_UV4SI_UV4SI_INTPTR)
+B_DEF      (s390_vceqgs,                vec_cmpeqv2di_cc,   0,                  B_VX,               0,                  BT_FN_V2DI_UV2DI_UV2DI_INTPTR)
+B_DEF      (s390_vfcedbs,               vec_cmpeqv2df_cc,   0,                  B_VX,               0,                  BT_FN_V2DI_V2DF_V2DF_INTPTR)
+B_DEF      (s390_vchbs,                 vec_cmphv16qi_cc,   0,                  B_VX,               0,                  BT_FN_V16QI_V16QI_V16QI_INTPTR)
+B_DEF      (s390_vchlbs,                vec_cmphlv16qi_cc,  0,                  B_VX,               0,                  BT_FN_V16QI_UV16QI_UV16QI_INTPTR)
+B_DEF      (s390_vchhs,                 vec_cmphv8hi_cc,    0,                  B_VX,               0,                  BT_FN_V8HI_V8HI_V8HI_INTPTR)
+B_DEF      (s390_vchlhs,                vec_cmphlv8hi_cc,   0,                  B_VX,               0,                  BT_FN_V8HI_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vchfs,                 vec_cmphv4si_cc,    0,                  B_VX,               0,                  BT_FN_V4SI_V4SI_V4SI_INTPTR)
+B_DEF      (s390_vchlfs,                vec_cmphlv4si_cc,   0,                  B_VX,               0,                  BT_FN_V4SI_UV4SI_UV4SI_INTPTR)
+B_DEF      (s390_vchgs,                 vec_cmphv2di_cc,    0,                  B_VX,               0,                  BT_FN_V2DI_V2DI_V2DI_INTPTR)
+B_DEF      (s390_vchlgs,                vec_cmphlv2di_cc,   0,                  B_VX,               0,                  BT_FN_V2DI_UV2DI_UV2DI_INTPTR)
+B_DEF      (s390_vfchdbs,               vec_cmphv2df_cc,    0,                  B_VX,               0,                  BT_FN_V2DI_V2DF_V2DF_INTPTR)
+B_DEF      (s390_vfchedbs,              vec_cmphev2df_cc,   0,                  B_VX,               0,                  BT_FN_V2DI_V2DF_V2DF_INTPTR)
+B_DEF      (vec_all_eqv16qi,            vec_all_eqv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_all_eqv8hi,             vec_all_eqv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_all_eqv4si,             vec_all_eqv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_all_eqv2di,             vec_all_eqv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_all_eqv2df,             vec_all_eqv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_all_nev16qi,            vec_all_nev16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_all_nev8hi,             vec_all_nev8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_all_nev4si,             vec_all_nev4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_all_nev2di,             vec_all_nev2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_all_nev2df,             vec_all_nev2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_all_gev16qi,            vec_all_gev16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_all_geuv16qi,           vec_all_geuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_all_gev8hi,             vec_all_gev8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_all_geuv8hi,            vec_all_geuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_all_gev4si,             vec_all_gev4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_all_geuv4si,            vec_all_geuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_all_gev2di,             vec_all_gev2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_all_geuv2di,            vec_all_geuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_all_gev2df,             vec_all_gev2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_all_gtv16qi,            vec_all_gtv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_all_gtuv16qi,           vec_all_gtuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_all_gtv8hi,             vec_all_gtv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_all_gtuv8hi,            vec_all_gtuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_all_gtv4si,             vec_all_gtv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_all_gtuv4si,            vec_all_gtuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_all_gtv2di,             vec_all_gtv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_all_gtuv2di,            vec_all_gtuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_all_gtv2df,             vec_all_gtv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_all_lev16qi,            vec_all_lev16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_all_leuv16qi,           vec_all_leuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_all_lev8hi,             vec_all_lev8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_all_leuv8hi,            vec_all_leuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_all_lev4si,             vec_all_lev4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_all_leuv4si,            vec_all_leuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_all_lev2di,             vec_all_lev2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_all_leuv2di,            vec_all_leuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_all_lev2df,             vec_all_lev2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_all_ltv16qi,            vec_all_ltv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_all_ltuv16qi,           vec_all_ltuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_all_ltv8hi,             vec_all_ltv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_all_ltuv8hi,            vec_all_ltuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_all_ltv4si,             vec_all_ltv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_all_ltuv4si,            vec_all_ltuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_all_ltv2di,             vec_all_ltv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_all_ltuv2di,            vec_all_ltuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_all_ltv2df,             vec_all_ltv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_all_eq,            s390_vec_all_eq_s8_a,s390_vec_all_eq_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_all_eq_s8_a,       vec_all_eqv16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_eq_s8_b,       vec_all_eqv16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_eq_b8_a,       vec_all_eqv16qi,    0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_eq_b8_b,       vec_all_eqv16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_eq_b8_c,       vec_all_eqv16qi,    0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_eq_u8_a,       vec_all_eqv16qi,    0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_eq_u8_b,       vec_all_eqv16qi,    0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_eq_s16_a,      vec_all_eqv8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_eq_s16_b,      vec_all_eqv8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_eq_b16_a,      vec_all_eqv8hi,     0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_eq_b16_b,      vec_all_eqv8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_eq_b16_c,      vec_all_eqv8hi,     0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_eq_u16_a,      vec_all_eqv8hi,     0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_eq_u16_b,      vec_all_eqv8hi,     0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_eq_s32_a,      vec_all_eqv4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_eq_s32_b,      vec_all_eqv4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_eq_b32_a,      vec_all_eqv4si,     0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_eq_b32_b,      vec_all_eqv4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_eq_b32_c,      vec_all_eqv4si,     0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_eq_u32_a,      vec_all_eqv4si,     0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_eq_u32_b,      vec_all_eqv4si,     0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_eq_s64_a,      vec_all_eqv2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_eq_s64_b,      vec_all_eqv2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_eq_b64_a,      vec_all_eqv2di,     0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_eq_b64_b,      vec_all_eqv2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_eq_b64_c,      vec_all_eqv2di,     0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_eq_u64_a,      vec_all_eqv2di,     0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_eq_u64_b,      vec_all_eqv2di,     0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_eq_dbl,        vec_all_eqv2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_all_ne,            s390_vec_all_ne_s8_a,s390_vec_all_ne_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_all_ne_s8_a,       vec_all_nev16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_ne_s8_b,       vec_all_nev16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_ne_b8_a,       vec_all_nev16qi,    0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_ne_b8_b,       vec_all_nev16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_ne_b8_c,       vec_all_nev16qi,    0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_ne_u8_a,       vec_all_nev16qi,    0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_ne_u8_b,       vec_all_nev16qi,    0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_ne_s16_a,      vec_all_nev8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_ne_s16_b,      vec_all_nev8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_ne_b16_a,      vec_all_nev8hi,     0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_ne_b16_b,      vec_all_nev8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_ne_b16_c,      vec_all_nev8hi,     0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_ne_u16_a,      vec_all_nev8hi,     0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_ne_u16_b,      vec_all_nev8hi,     0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_ne_s32_a,      vec_all_nev4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_ne_s32_b,      vec_all_nev4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_ne_b32_a,      vec_all_nev4si,     0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_ne_b32_b,      vec_all_nev4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_ne_b32_c,      vec_all_nev4si,     0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_ne_u32_a,      vec_all_nev4si,     0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_ne_u32_b,      vec_all_nev4si,     0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_ne_s64_a,      vec_all_nev2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_ne_s64_b,      vec_all_nev2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_ne_b64_a,      vec_all_nev2di,     0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_ne_b64_b,      vec_all_nev2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_ne_b64_c,      vec_all_nev2di,     0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_ne_u64_a,      vec_all_nev2di,     0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_ne_u64_b,      vec_all_nev2di,     0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_ne_dbl,        vec_all_nev2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_all_ge,            s390_vec_all_ge_s8_a,s390_vec_all_ge_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_all_ge_s8_a,       vec_all_gev16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_ge_s8_b,       vec_all_gev16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_ge_b8_a,       vec_all_geuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_ge_b8_b,       vec_all_gev16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_ge_b8_c,       vec_all_geuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_ge_u8_a,       vec_all_geuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_ge_u8_b,       vec_all_geuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_ge_s16_a,      vec_all_gev8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_ge_s16_b,      vec_all_gev8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_ge_b16_a,      vec_all_geuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_ge_b16_b,      vec_all_gev8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_ge_b16_c,      vec_all_geuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_ge_u16_a,      vec_all_geuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_ge_u16_b,      vec_all_geuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_ge_s32_a,      vec_all_gev4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_ge_s32_b,      vec_all_gev4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_ge_b32_a,      vec_all_geuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_ge_b32_b,      vec_all_gev4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_ge_b32_c,      vec_all_geuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_ge_u32_a,      vec_all_geuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_ge_u32_b,      vec_all_geuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_ge_s64_a,      vec_all_gev2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_ge_s64_b,      vec_all_gev2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_ge_b64_a,      vec_all_geuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_ge_b64_b,      vec_all_gev2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_ge_b64_c,      vec_all_geuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_ge_u64_a,      vec_all_geuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_ge_u64_b,      vec_all_geuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_ge_dbl,        vec_all_gev2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_all_gt,            s390_vec_all_gt_s8_a,s390_vec_all_gt_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_all_gt_s8_a,       vec_all_gtv16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_gt_s8_b,       vec_all_gtv16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_gt_b8_a,       vec_all_gtuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_gt_b8_b,       vec_all_gtv16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_gt_b8_c,       vec_all_gtuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_gt_u8_a,       vec_all_gtuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_gt_u8_b,       vec_all_gtuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_gt_s16_a,      vec_all_gtv8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_gt_s16_b,      vec_all_gtv8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_gt_b16_a,      vec_all_gtuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_gt_b16_b,      vec_all_gtv8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_gt_b16_c,      vec_all_gtuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_gt_u16_a,      vec_all_gtuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_gt_u16_b,      vec_all_gtuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_gt_s32_a,      vec_all_gtv4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_gt_s32_b,      vec_all_gtv4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_gt_b32_a,      vec_all_gtuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_gt_b32_b,      vec_all_gtv4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_gt_b32_c,      vec_all_gtuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_gt_u32_a,      vec_all_gtuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_gt_u32_b,      vec_all_gtuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_gt_s64_a,      vec_all_gtv2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_gt_s64_b,      vec_all_gtv2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_gt_b64_a,      vec_all_gtuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_gt_b64_b,      vec_all_gtv2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_gt_b64_c,      vec_all_gtuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_gt_u64_a,      vec_all_gtuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_gt_u64_b,      vec_all_gtuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_gt_dbl,        vec_all_gtv2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_all_le,            s390_vec_all_le_s8_a,s390_vec_all_le_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_all_le_s8_a,       vec_all_lev16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_le_s8_b,       vec_all_lev16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_le_b8_a,       vec_all_leuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_le_b8_b,       vec_all_lev16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_le_b8_c,       vec_all_leuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_le_u8_a,       vec_all_leuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_le_u8_b,       vec_all_leuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_le_s16_a,      vec_all_lev8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_le_s16_b,      vec_all_lev8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_le_b16_a,      vec_all_leuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_le_b16_b,      vec_all_lev8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_le_b16_c,      vec_all_leuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_le_u16_a,      vec_all_leuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_le_u16_b,      vec_all_leuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_le_s32_a,      vec_all_lev4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_le_s32_b,      vec_all_lev4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_le_b32_a,      vec_all_leuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_le_b32_b,      vec_all_lev4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_le_b32_c,      vec_all_leuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_le_u32_a,      vec_all_leuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_le_u32_b,      vec_all_leuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_le_s64_a,      vec_all_lev2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_le_s64_b,      vec_all_lev2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_le_b64_a,      vec_all_leuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_le_b64_b,      vec_all_lev2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_le_b64_c,      vec_all_leuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_le_u64_a,      vec_all_leuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_le_u64_b,      vec_all_leuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_le_dbl,        vec_all_lev2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_all_lt,            s390_vec_all_lt_s8_a,s390_vec_all_lt_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_all_lt_s8_a,       vec_all_ltv16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_lt_s8_b,       vec_all_ltv16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_lt_b8_a,       vec_all_ltuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_lt_b8_b,       vec_all_ltv16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_all_lt_b8_c,       vec_all_ltuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_lt_u8_a,       vec_all_ltuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_all_lt_u8_b,       vec_all_ltuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_all_lt_s16_a,      vec_all_ltv8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_lt_s16_b,      vec_all_ltv8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_lt_b16_a,      vec_all_ltuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_lt_b16_b,      vec_all_ltv8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_all_lt_b16_c,      vec_all_ltuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_lt_u16_a,      vec_all_ltuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_all_lt_u16_b,      vec_all_ltuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_all_lt_s32_a,      vec_all_ltv4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_lt_s32_b,      vec_all_ltv4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_lt_b32_a,      vec_all_ltuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_lt_b32_b,      vec_all_ltv4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_all_lt_b32_c,      vec_all_ltuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_lt_u32_a,      vec_all_ltuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_all_lt_u32_b,      vec_all_ltuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_all_lt_s64_a,      vec_all_ltv2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_lt_s64_b,      vec_all_ltv2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_lt_b64_a,      vec_all_ltuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_lt_b64_b,      vec_all_ltv2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_all_lt_b64_c,      vec_all_ltuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_lt_u64_a,      vec_all_ltuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_all_lt_u64_b,      vec_all_ltuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_all_lt_dbl,        vec_all_ltv2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+B_DEF      (vec_any_eqv16qi,            vec_any_eqv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_any_eqv8hi,             vec_any_eqv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_any_eqv4si,             vec_any_eqv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_any_eqv2di,             vec_any_eqv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_any_eqv2df,             vec_any_eqv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_any_nev16qi,            vec_any_nev16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_any_nev8hi,             vec_any_nev8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_any_nev4si,             vec_any_nev4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_any_nev2di,             vec_any_nev2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_any_nev2df,             vec_any_nev2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_any_gev16qi,            vec_any_gev16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_any_geuv16qi,           vec_any_geuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_any_gev8hi,             vec_any_gev8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_any_geuv8hi,            vec_any_geuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_any_gev4si,             vec_any_gev4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_any_geuv4si,            vec_any_geuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_any_gev2di,             vec_any_gev2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_any_geuv2di,            vec_any_geuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_any_gev2df,             vec_any_gev2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_any_gtv16qi,            vec_any_gtv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_any_gtuv16qi,           vec_any_gtuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_any_gtv8hi,             vec_any_gtv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_any_gtuv8hi,            vec_any_gtuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_any_gtv4si,             vec_any_gtv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_any_gtuv4si,            vec_any_gtuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_any_gtv2di,             vec_any_gtv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_any_gtuv2di,            vec_any_gtuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_any_gtv2df,             vec_any_gtv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_any_lev16qi,            vec_any_lev16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_any_leuv16qi,           vec_any_leuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_any_lev8hi,             vec_any_lev8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_any_leuv8hi,            vec_any_leuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_any_lev4si,             vec_any_lev4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_any_leuv4si,            vec_any_leuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_any_lev2di,             vec_any_lev2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_any_leuv2di,            vec_any_leuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_any_lev2df,             vec_any_lev2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (vec_any_ltv16qi,            vec_any_ltv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_V16QI_V16QI)
+B_DEF      (vec_any_ltuv16qi,           vec_any_ltuv16qi,   0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (vec_any_ltv8hi,             vec_any_ltv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V8HI_V8HI)
+B_DEF      (vec_any_ltuv8hi,            vec_any_ltuv8hi,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV8HI_UV8HI)
+B_DEF      (vec_any_ltv4si,             vec_any_ltv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V4SI_V4SI)
+B_DEF      (vec_any_ltuv4si,            vec_any_ltuv4si,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV4SI_UV4SI)
+B_DEF      (vec_any_ltv2di,             vec_any_ltv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DI_V2DI)
+B_DEF      (vec_any_ltuv2di,            vec_any_ltuv2di,    0,                  B_VX | B_INT,       0,                  BT_FN_INT_UV2DI_UV2DI)
+B_DEF      (vec_any_ltv2df,             vec_any_ltv2df,     0,                  B_VX | B_INT,       0,                  BT_FN_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_any_eq,            s390_vec_any_eq_s8_a,s390_vec_any_eq_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_any_eq_s8_a,       vec_any_eqv16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_eq_s8_b,       vec_any_eqv16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_eq_b8_a,       vec_any_eqv16qi,    0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_eq_b8_b,       vec_any_eqv16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_eq_b8_c,       vec_any_eqv16qi,    0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_eq_u8_a,       vec_any_eqv16qi,    0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_eq_u8_b,       vec_any_eqv16qi,    0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_eq_s16_a,      vec_any_eqv8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_eq_s16_b,      vec_any_eqv8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_eq_b16_a,      vec_any_eqv8hi,     0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_eq_b16_b,      vec_any_eqv8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_eq_b16_c,      vec_any_eqv8hi,     0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_eq_u16_a,      vec_any_eqv8hi,     0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_eq_u16_b,      vec_any_eqv8hi,     0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_eq_s32_a,      vec_any_eqv4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_eq_s32_b,      vec_any_eqv4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_eq_b32_a,      vec_any_eqv4si,     0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_eq_b32_b,      vec_any_eqv4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_eq_b32_c,      vec_any_eqv4si,     0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_eq_u32_a,      vec_any_eqv4si,     0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_eq_u32_b,      vec_any_eqv4si,     0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_eq_s64_a,      vec_any_eqv2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_eq_s64_b,      vec_any_eqv2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_eq_b64_a,      vec_any_eqv2di,     0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_eq_b64_b,      vec_any_eqv2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_eq_b64_c,      vec_any_eqv2di,     0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_eq_u64_a,      vec_any_eqv2di,     0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_eq_u64_b,      vec_any_eqv2di,     0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_eq_dbl,        vec_any_eqv2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_any_ne,            s390_vec_any_ne_s8_a,s390_vec_any_ne_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_any_ne_s8_a,       vec_any_nev16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_ne_s8_b,       vec_any_nev16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_ne_b8_a,       vec_any_nev16qi,    0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_ne_b8_b,       vec_any_nev16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_ne_b8_c,       vec_any_nev16qi,    0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_ne_u8_a,       vec_any_nev16qi,    0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_ne_u8_b,       vec_any_nev16qi,    0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_ne_s16_a,      vec_any_nev8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_ne_s16_b,      vec_any_nev8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_ne_b16_a,      vec_any_nev8hi,     0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_ne_b16_b,      vec_any_nev8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_ne_b16_c,      vec_any_nev8hi,     0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_ne_u16_a,      vec_any_nev8hi,     0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_ne_u16_b,      vec_any_nev8hi,     0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_ne_s32_a,      vec_any_nev4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_ne_s32_b,      vec_any_nev4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_ne_b32_a,      vec_any_nev4si,     0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_ne_b32_b,      vec_any_nev4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_ne_b32_c,      vec_any_nev4si,     0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_ne_u32_a,      vec_any_nev4si,     0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_ne_u32_b,      vec_any_nev4si,     0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_ne_s64_a,      vec_any_nev2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_ne_s64_b,      vec_any_nev2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_ne_b64_a,      vec_any_nev2di,     0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_ne_b64_b,      vec_any_nev2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_ne_b64_c,      vec_any_nev2di,     0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_ne_u64_a,      vec_any_nev2di,     0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_ne_u64_b,      vec_any_nev2di,     0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_ne_dbl,        vec_any_nev2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_any_ge,            s390_vec_any_ge_s8_a,s390_vec_any_ge_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_any_ge_s8_a,       vec_any_gev16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_ge_s8_b,       vec_any_gev16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_ge_b8_a,       vec_any_geuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_ge_b8_b,       vec_any_gev16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_ge_b8_c,       vec_any_geuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_ge_u8_a,       vec_any_geuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_ge_u8_b,       vec_any_geuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_ge_s16_a,      vec_any_gev8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_ge_s16_b,      vec_any_gev8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_ge_b16_a,      vec_any_geuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_ge_b16_b,      vec_any_gev8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_ge_b16_c,      vec_any_geuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_ge_u16_a,      vec_any_geuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_ge_u16_b,      vec_any_geuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_ge_s32_a,      vec_any_gev4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_ge_s32_b,      vec_any_gev4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_ge_b32_a,      vec_any_geuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_ge_b32_b,      vec_any_gev4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_ge_b32_c,      vec_any_geuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_ge_u32_a,      vec_any_geuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_ge_u32_b,      vec_any_geuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_ge_s64_a,      vec_any_gev2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_ge_s64_b,      vec_any_gev2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_ge_b64_a,      vec_any_geuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_ge_b64_b,      vec_any_gev2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_ge_b64_c,      vec_any_geuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_ge_u64_a,      vec_any_geuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_ge_u64_b,      vec_any_geuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_ge_dbl,        vec_any_gev2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_any_gt,            s390_vec_any_gt_s8_a,s390_vec_any_gt_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_any_gt_s8_a,       vec_any_gtv16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_gt_s8_b,       vec_any_gtv16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_gt_b8_a,       vec_any_gtuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_gt_b8_b,       vec_any_gtv16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_gt_b8_c,       vec_any_gtuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_gt_u8_a,       vec_any_gtuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_gt_u8_b,       vec_any_gtuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_gt_s16_a,      vec_any_gtv8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_gt_s16_b,      vec_any_gtv8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_gt_b16_a,      vec_any_gtuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_gt_b16_b,      vec_any_gtv8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_gt_b16_c,      vec_any_gtuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_gt_u16_a,      vec_any_gtuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_gt_u16_b,      vec_any_gtuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_gt_s32_a,      vec_any_gtv4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_gt_s32_b,      vec_any_gtv4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_gt_b32_a,      vec_any_gtuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_gt_b32_b,      vec_any_gtv4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_gt_b32_c,      vec_any_gtuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_gt_u32_a,      vec_any_gtuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_gt_u32_b,      vec_any_gtuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_gt_s64_a,      vec_any_gtv2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_gt_s64_b,      vec_any_gtv2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_gt_b64_a,      vec_any_gtuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_gt_b64_b,      vec_any_gtv2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_gt_b64_c,      vec_any_gtuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_gt_u64_a,      vec_any_gtuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_gt_u64_b,      vec_any_gtuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_gt_dbl,        vec_any_gtv2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_any_le,            s390_vec_any_le_s8_a,s390_vec_any_le_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_any_le_s8_a,       vec_any_lev16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_le_s8_b,       vec_any_lev16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_le_b8_a,       vec_any_leuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_le_b8_b,       vec_any_lev16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_le_b8_c,       vec_any_leuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_le_u8_a,       vec_any_leuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_le_u8_b,       vec_any_leuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_le_s16_a,      vec_any_lev8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_le_s16_b,      vec_any_lev8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_le_b16_a,      vec_any_leuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_le_b16_b,      vec_any_lev8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_le_b16_c,      vec_any_leuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_le_u16_a,      vec_any_leuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_le_u16_b,      vec_any_leuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_le_s32_a,      vec_any_lev4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_le_s32_b,      vec_any_lev4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_le_b32_a,      vec_any_leuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_le_b32_b,      vec_any_lev4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_le_b32_c,      vec_any_leuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_le_u32_a,      vec_any_leuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_le_u32_b,      vec_any_leuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_le_s64_a,      vec_any_lev2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_le_s64_b,      vec_any_lev2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_le_b64_a,      vec_any_leuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_le_b64_b,      vec_any_lev2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_le_b64_c,      vec_any_leuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_le_u64_a,      vec_any_leuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_le_u64_b,      vec_any_leuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_le_dbl,        vec_any_lev2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_any_lt,            s390_vec_any_lt_s8_a,s390_vec_any_lt_dbl,B_VX,              BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_any_lt_s8_a,       vec_any_ltv16qi,    0,                  BT_OV_INT_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_lt_s8_b,       vec_any_ltv16qi,    0,                  BT_OV_INT_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_lt_b8_a,       vec_any_ltuv16qi,   0,                  BT_OV_INT_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_lt_b8_b,       vec_any_ltv16qi,    0,                  BT_OV_INT_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_any_lt_b8_c,       vec_any_ltuv16qi,   0,                  BT_OV_INT_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_lt_u8_a,       vec_any_ltuv16qi,   0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_any_lt_u8_b,       vec_any_ltuv16qi,   0,                  BT_OV_INT_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_any_lt_s16_a,      vec_any_ltv8hi,     0,                  BT_OV_INT_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_lt_s16_b,      vec_any_ltv8hi,     0,                  BT_OV_INT_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_lt_b16_a,      vec_any_ltuv8hi,    0,                  BT_OV_INT_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_lt_b16_b,      vec_any_ltv8hi,     0,                  BT_OV_INT_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_any_lt_b16_c,      vec_any_ltuv8hi,    0,                  BT_OV_INT_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_lt_u16_a,      vec_any_ltuv8hi,    0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_any_lt_u16_b,      vec_any_ltuv8hi,    0,                  BT_OV_INT_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_any_lt_s32_a,      vec_any_ltv4si,     0,                  BT_OV_INT_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_lt_s32_b,      vec_any_ltv4si,     0,                  BT_OV_INT_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_lt_b32_a,      vec_any_ltuv4si,    0,                  BT_OV_INT_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_lt_b32_b,      vec_any_ltv4si,     0,                  BT_OV_INT_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_any_lt_b32_c,      vec_any_ltuv4si,    0,                  BT_OV_INT_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_lt_u32_a,      vec_any_ltuv4si,    0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_any_lt_u32_b,      vec_any_ltuv4si,    0,                  BT_OV_INT_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_any_lt_s64_a,      vec_any_ltv2di,     0,                  BT_OV_INT_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_lt_s64_b,      vec_any_ltv2di,     0,                  BT_OV_INT_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_lt_b64_a,      vec_any_ltuv2di,    0,                  BT_OV_INT_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_lt_b64_b,      vec_any_ltv2di,     0,                  BT_OV_INT_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_any_lt_b64_c,      vec_any_ltuv2di,    0,                  BT_OV_INT_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_lt_u64_a,      vec_any_ltuv2di,    0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_any_lt_u64_b,      vec_any_ltuv2di,    0,                  BT_OV_INT_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_any_lt_dbl,        vec_any_ltv2df,     0,                  BT_OV_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_cmpeq,             s390_vec_cmpeq_s8,  s390_vec_cmpeq_dbl, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_cmpeq_s8,          s390_vceqb,         0,                  BT_OV_BV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_cmpeq_u8,          s390_vceqb,         0,                  BT_OV_BV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_cmpeq_b8,          s390_vceqb,         0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_cmpeq_s16,         s390_vceqh,         0,                  BT_OV_BV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_cmpeq_u16,         s390_vceqh,         0,                  BT_OV_BV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_cmpeq_b16,         s390_vceqh,         0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_cmpeq_s32,         s390_vceqf,         0,                  BT_OV_BV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_cmpeq_u32,         s390_vceqf,         0,                  BT_OV_BV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_cmpeq_b32,         s390_vceqf,         0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_cmpeq_s64,         s390_vceqg,         0,                  BT_OV_BV2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_cmpeq_u64,         s390_vceqg,         0,                  BT_OV_BV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_cmpeq_b64,         s390_vceqg,         0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_cmpeq_dbl,         s390_vfcedb,        0,                  BT_OV_BV2DI_V2DF_V2DF)
+
+B_DEF      (s390_vceqb,                 vec_cmpeqv16qi,     0,                  B_VX,               0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (s390_vceqh,                 vec_cmpeqv8hi,      0,                  B_VX,               0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (s390_vceqf,                 vec_cmpeqv4si,      0,                  B_VX,               0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (s390_vceqg,                 vec_cmpeqv2di,      0,                  B_VX,               0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (s390_vfcedb,                vec_cmpeqv2df,      0,                  B_VX,               0,                  BT_FN_V2DI_V2DF_V2DF)
+
+OB_DEF     (s390_vec_cmpge,             s390_vec_cmpge_s8,  s390_vec_cmpge_dbl, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_cmpge_s8,          vec_cmpgev16qi,     0,                  BT_OV_BV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_cmpge_u8,          vec_cmpgeuv16qi,    0,                  BT_OV_BV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_cmpge_s16,         vec_cmpgev8hi,      0,                  BT_OV_BV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_cmpge_u16,         vec_cmpgeuv8hi,     0,                  BT_OV_BV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_cmpge_s32,         vec_cmpgev4si,      0,                  BT_OV_BV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_cmpge_u32,         vec_cmpgeuv4si,     0,                  BT_OV_BV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_cmpge_s64,         vec_cmpgev2di,      0,                  BT_OV_BV2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_cmpge_u64,         vec_cmpgeuv2di,     0,                  BT_OV_BV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_cmpge_dbl,         s390_vfchedb,       0,                  BT_OV_BV2DI_V2DF_V2DF)
+
+B_DEF      (vec_cmpgev16qi,             vec_cmpgev16qi,     0,                  B_VX | B_INT,       0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (vec_cmpgeuv16qi,            vec_cmpgeuv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (vec_cmpgev8hi,              vec_cmpgev8hi,      0,                  B_VX | B_INT,       0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (vec_cmpgeuv8hi,             vec_cmpgeuv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (vec_cmpgev4si,              vec_cmpgev4si,      0,                  B_VX | B_INT,       0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (vec_cmpgeuv4si,             vec_cmpgeuv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (vec_cmpgev2di,              vec_cmpgev2di,      0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (vec_cmpgeuv2di,             vec_cmpgeuv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (s390_vfchedb,               vec_cmpgev2df,      0,                  B_VX,               0,                  BT_FN_V2DI_V2DF_V2DF)
+
+OB_DEF     (s390_vec_cmpgt,             s390_vec_cmpgt_s8,  s390_vec_cmpgt_dbl, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_cmpgt_s8,          s390_vchb,          0,                  BT_OV_BV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_cmpgt_u8,          s390_vchlb,         0,                  BT_OV_BV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_cmpgt_s16,         s390_vchh,          0,                  BT_OV_BV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_cmpgt_u16,         s390_vchlh,         0,                  BT_OV_BV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_cmpgt_s32,         s390_vchf,          0,                  BT_OV_BV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_cmpgt_u32,         s390_vchlf,         0,                  BT_OV_BV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_cmpgt_s64,         s390_vchg,          0,                  BT_OV_BV2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_cmpgt_u64,         s390_vchlg,         0,                  BT_OV_BV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_cmpgt_dbl,         s390_vfchdb,        0,                  BT_OV_BV2DI_V2DF_V2DF)
+
+B_DEF      (s390_vchb,                  vec_cmpgtv16qi,     0,                  B_VX,               0,                  BT_FN_V16QI_V16QI_V16QI)
+B_DEF      (s390_vchlb,                 vec_cmpgtuv16qi,    0,                  B_VX,               0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (s390_vchh,                  vec_cmpgtv8hi,      0,                  B_VX,               0,                  BT_FN_V8HI_V8HI_V8HI)
+B_DEF      (s390_vchlh,                 vec_cmpgtuv8hi,     0,                  B_VX,               0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (s390_vchf,                  vec_cmpgtv4si,      0,                  B_VX,               0,                  BT_FN_V4SI_V4SI_V4SI)
+B_DEF      (s390_vchlf,                 vec_cmpgtuv4si,     0,                  B_VX,               0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (s390_vchg,                  vec_cmpgtv2di,      0,                  B_VX,               0,                  BT_FN_V2DI_V2DI_V2DI)
+B_DEF      (s390_vchlg,                 vec_cmpgtuv2di,     0,                  B_VX,               0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (s390_vfchdb,                vec_cmpgtv2df,      0,                  B_VX,               0,                  BT_FN_V2DI_V2DF_V2DF)
+
+OB_DEF     (s390_vec_cmple,             s390_vec_cmple_s8,  s390_vec_cmple_dbl, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_cmple_s8,          vec_cmplev16qi,     0,                  BT_OV_BV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_cmple_u8,          vec_cmpleuv16qi,    0,                  BT_OV_BV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_cmple_s16,         vec_cmplev8hi,      0,                  BT_OV_BV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_cmple_u16,         vec_cmpleuv8hi,     0,                  BT_OV_BV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_cmple_s32,         vec_cmplev4si,      0,                  BT_OV_BV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_cmple_u32,         vec_cmpleuv4si,     0,                  BT_OV_BV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_cmple_s64,         vec_cmplev2di,      0,                  BT_OV_BV2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_cmple_u64,         vec_cmpleuv2di,     0,                  BT_OV_BV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_cmple_dbl,         vec_cmplev2df,      0,                  BT_OV_BV2DI_V2DF_V2DF)
+
+B_DEF      (vec_cmplev16qi,             vec_cmplev16qi,     0,                  B_VX | B_INT,       0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (vec_cmpleuv16qi,            vec_cmpleuv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (vec_cmplev8hi,              vec_cmplev8hi,      0,                  B_VX | B_INT,       0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (vec_cmpleuv8hi,             vec_cmpleuv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (vec_cmplev4si,              vec_cmplev4si,      0,                  B_VX | B_INT,       0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (vec_cmpleuv4si,             vec_cmpleuv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (vec_cmplev2di,              vec_cmplev2di,      0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (vec_cmpleuv2di,             vec_cmpleuv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (vec_cmplev2df,              vec_cmplev2df,      0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_V2DF_V2DF)
+
+OB_DEF     (s390_vec_cmplt,             s390_vec_cmplt_s8,  s390_vec_cmplt_dbl, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_cmplt_s8,          vec_cmpltv16qi,     0,                  BT_OV_BV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_cmplt_u8,          vec_cmpltuv16qi,    0,                  BT_OV_BV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_cmplt_s16,         vec_cmpltv8hi,      0,                  BT_OV_BV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_cmplt_u16,         vec_cmpltuv8hi,     0,                  BT_OV_BV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_cmplt_s32,         vec_cmpltv4si,      0,                  BT_OV_BV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_cmplt_u32,         vec_cmpltuv4si,     0,                  BT_OV_BV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_cmplt_s64,         vec_cmpltv2di,      0,                  BT_OV_BV2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_cmplt_u64,         vec_cmpltuv2di,     0,                  BT_OV_BV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_cmplt_dbl,         vec_cmpltv2df,      0,                  BT_OV_BV2DI_V2DF_V2DF)
+
+B_DEF      (vec_cmpltv16qi,             vec_cmpltv16qi,     0,                  B_VX | B_INT,       0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (vec_cmpltuv16qi,            vec_cmpltuv16qi,    0,                  B_VX | B_INT,       0,                  BT_FN_V16QI_UV16QI_UV16QI)
+B_DEF      (vec_cmpltv8hi,              vec_cmpltv8hi,      0,                  B_VX | B_INT,       0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (vec_cmpltuv8hi,             vec_cmpltuv8hi,     0,                  B_VX | B_INT,       0,                  BT_FN_V8HI_UV8HI_UV8HI)
+B_DEF      (vec_cmpltv4si,              vec_cmpltv4si,      0,                  B_VX | B_INT,       0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (vec_cmpltuv4si,             vec_cmpltuv4si,     0,                  B_VX | B_INT,       0,                  BT_FN_V4SI_UV4SI_UV4SI)
+B_DEF      (vec_cmpltv2di,              vec_cmpltv2di,      0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (vec_cmpltuv2di,             vec_cmpltuv2di,     0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_UV2DI_UV2DI)
+B_DEF      (vec_cmpltv2df,              vec_cmpltv2df,      0,                  B_VX | B_INT,       0,                  BT_FN_V2DI_V2DF_V2DF)
+
+OB_DEF     (s390_vec_cntlz,             s390_vec_cntlz_s8,  s390_vec_cntlz_u64, B_VX,               BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_cntlz_s8,          s390_vclzb,         0,                  BT_OV_UV16QI_V16QI)
+OB_DEF_VAR (s390_vec_cntlz_u8,          s390_vclzb,         0,                  BT_OV_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_cntlz_s16,         s390_vclzh,         0,                  BT_OV_UV8HI_V8HI)
+OB_DEF_VAR (s390_vec_cntlz_u16,         s390_vclzh,         0,                  BT_OV_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_cntlz_s32,         s390_vclzf,         0,                  BT_OV_UV4SI_V4SI)
+OB_DEF_VAR (s390_vec_cntlz_u32,         s390_vclzf,         0,                  BT_OV_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_cntlz_s64,         s390_vclzg,         0,                  BT_OV_UV2DI_V2DI)
+OB_DEF_VAR (s390_vec_cntlz_u64,         s390_vclzg,         0,                  BT_OV_UV2DI_UV2DI)
+
+B_DEF      (s390_vclzb,                 clzv16qi2,          0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI)
+B_DEF      (s390_vclzh,                 clzv8hi2,           0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI)
+B_DEF      (s390_vclzf,                 clzv4si2,           0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI)
+B_DEF      (s390_vclzg,                 clzv2di2,           0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_cnttz,             s390_vec_cnttz_s8,  s390_vec_cnttz_u64, B_VX,               BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_cnttz_s8,          s390_vctzb,         0,                  BT_OV_UV16QI_V16QI)
+OB_DEF_VAR (s390_vec_cnttz_u8,          s390_vctzb,         0,                  BT_OV_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_cnttz_s16,         s390_vctzh,         0,                  BT_OV_UV8HI_V8HI)
+OB_DEF_VAR (s390_vec_cnttz_u16,         s390_vctzh,         0,                  BT_OV_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_cnttz_s32,         s390_vctzf,         0,                  BT_OV_UV4SI_V4SI)
+OB_DEF_VAR (s390_vec_cnttz_u32,         s390_vctzf,         0,                  BT_OV_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_cnttz_s64,         s390_vctzg,         0,                  BT_OV_UV2DI_V2DI)
+OB_DEF_VAR (s390_vec_cnttz_u64,         s390_vctzg,         0,                  BT_OV_UV2DI_UV2DI)
+
+B_DEF      (s390_vctzb,                 ctzv16qi2,          0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI)
+B_DEF      (s390_vctzh,                 ctzv8hi2,           0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI)
+B_DEF      (s390_vctzf,                 ctzv4si2,           0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI)
+B_DEF      (s390_vctzg,                 ctzv2di2,           0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_xor,               s390_vec_xor_b8,    s390_vec_xor_dbl_c, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_xor_b8,            s390_vx,            0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_xor_s8_a,          s390_vx,            0,                  BT_OV_V16QI_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_xor_s8_b,          s390_vx,            0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_xor_s8_c,          s390_vx,            0,                  BT_OV_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_xor_u8_a,          s390_vx,            0,                  BT_OV_UV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_xor_u8_b,          s390_vx,            0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_xor_u8_c,          s390_vx,            0,                  BT_OV_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_xor_b16,           s390_vx,            0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_xor_s16_a,         s390_vx,            0,                  BT_OV_V8HI_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_xor_s16_b,         s390_vx,            0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_xor_s16_c,         s390_vx,            0,                  BT_OV_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_xor_u16_a,         s390_vx,            0,                  BT_OV_UV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_xor_u16_b,         s390_vx,            0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_xor_u16_c,         s390_vx,            0,                  BT_OV_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_xor_b32,           s390_vx,            0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_xor_s32_a,         s390_vx,            0,                  BT_OV_V4SI_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_xor_s32_b,         s390_vx,            0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_xor_s32_c,         s390_vx,            0,                  BT_OV_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_xor_u32_a,         s390_vx,            0,                  BT_OV_UV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_xor_u32_b,         s390_vx,            0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_xor_u32_c,         s390_vx,            0,                  BT_OV_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_xor_b64,           s390_vx,            0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_xor_s64_a,         s390_vx,            0,                  BT_OV_V2DI_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_xor_s64_b,         s390_vx,            0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_xor_s64_c,         s390_vx,            0,                  BT_OV_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_xor_u64_a,         s390_vx,            0,                  BT_OV_UV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_xor_u64_b,         s390_vx,            0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_xor_u64_c,         s390_vx,            0,                  BT_OV_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_xor_dbl_a,         s390_vx,            0,                  BT_OV_V2DF_BV2DI_V2DF)
+OB_DEF_VAR (s390_vec_xor_dbl_b,         s390_vx,            0,                  BT_OV_V2DF_V2DF_V2DF)
+OB_DEF_VAR (s390_vec_xor_dbl_c,         s390_vx,            0,                  BT_OV_V2DF_V2DF_BV2DI)
+
+B_DEF      (s390_vx,                    xorv16qi3,          0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_gfmsum,            s390_vec_gfmsum_u8, s390_vec_gfmsum_u32,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_gfmsum_u8,         s390_vgfmb,         0,                  BT_OV_UV8HI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_gfmsum_u16,        s390_vgfmh,         0,                  BT_OV_UV4SI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_gfmsum_u32,        s390_vgfmf,         0,                  BT_OV_UV2DI_UV4SI_UV4SI)
+
+B_DEF      (s390_vgfmb,                 vec_gfmsumv16qi,    0,                  B_VX,               0,                  BT_FN_UV8HI_UV16QI_UV16QI)
+B_DEF      (s390_vgfmh,                 vec_gfmsumv8hi,     0,                  B_VX,               0,                  BT_FN_UV4SI_UV8HI_UV8HI)
+B_DEF      (s390_vgfmf,                 vec_gfmsumv4si,     0,                  B_VX,               0,                  BT_FN_UV2DI_UV4SI_UV4SI)
+B_DEF      (s390_vgfmg,                 vec_gfmsum_128,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_gfmsum_accum,      s390_vec_gfmsum_accum_u8,s390_vec_gfmsum_accum_u32,B_VX,    BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_gfmsum_accum_u8,   s390_vgfmab,        0,                  BT_OV_UV8HI_UV16QI_UV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_gfmsum_accum_u16,  s390_vgfmah,        0,                  BT_OV_UV4SI_UV8HI_UV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_gfmsum_accum_u32,  s390_vgfmaf,        0,                  BT_OV_UV2DI_UV4SI_UV4SI_UV2DI)
+
+B_DEF      (s390_vgfmab,                vec_gfmsum_accumv16qi,0,                B_VX,               0,                  BT_FN_UV8HI_UV16QI_UV16QI_UV8HI)
+B_DEF      (s390_vgfmah,                vec_gfmsum_accumv8hi,0,                 B_VX,               0,                  BT_FN_UV4SI_UV8HI_UV8HI_UV4SI)
+B_DEF      (s390_vgfmaf,                vec_gfmsum_accumv4si,0,                 B_VX,               0,                  BT_FN_UV2DI_UV4SI_UV4SI_UV2DI)
+B_DEF      (s390_vgfmag,                vec_gfmsum_accum_128,0,                 B_VX,               0,                  BT_FN_UV16QI_UV2DI_UV2DI_UV16QI)
+
+OB_DEF     (s390_vec_abs,               s390_vec_abs_s8,    s390_vec_abs_dbl,   B_VX,               BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_abs_s8,            s390_vlpb,          0,                  BT_OV_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_abs_s16,           s390_vlph,          0,                  BT_OV_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_abs_s32,           s390_vlpf,          0,                  BT_OV_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_abs_s64,           s390_vlpg,          0,                  BT_OV_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_abs_dbl,           s390_vflpdb,        0,                  BT_OV_V2DF_V2DF)
+
+B_DEF      (s390_vlpb,                  absv16qi2,          0,                  B_VX,               0,                  BT_FN_V16QI_V16QI)
+B_DEF      (s390_vlph,                  absv8hi2,           0,                  B_VX,               0,                  BT_FN_V8HI_V8HI)
+B_DEF      (s390_vlpf,                  absv4si2,           0,                  B_VX,               0,                  BT_FN_V4SI_V4SI)
+B_DEF      (s390_vlpg,                  absv2di2,           0,                  B_VX,               0,                  BT_FN_V2DI_V2DI)
+B_DEF      (s390_vflpdb,                absv2df2,           0,                  B_VX,               0,                  BT_FN_V2DF_V2DF)
+
+OB_DEF     (s390_vec_max,               s390_vec_max_s8_a,  s390_vec_max_dbl,   B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_max_s8_a,          s390_vmxb,          0,                  BT_OV_V16QI_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_max_s8_b,          s390_vmxb,          0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_max_s8_c,          s390_vmxb,          0,                  BT_OV_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_max_u8_a,          s390_vmxlb,         0,                  BT_OV_UV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_max_u8_b,          s390_vmxlb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_max_u8_c,          s390_vmxlb,         0,                  BT_OV_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_max_s16_a,         s390_vmxh,          0,                  BT_OV_V8HI_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_max_s16_b,         s390_vmxh,          0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_max_s16_c,         s390_vmxh,          0,                  BT_OV_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_max_u16_a,         s390_vmxlh,         0,                  BT_OV_UV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_max_u16_b,         s390_vmxlh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_max_u16_c,         s390_vmxlh,         0,                  BT_OV_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_max_s32_a,         s390_vmxf,          0,                  BT_OV_V4SI_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_max_s32_b,         s390_vmxf,          0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_max_s32_c,         s390_vmxf,          0,                  BT_OV_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_max_u32_a,         s390_vmxlf,         0,                  BT_OV_UV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_max_u32_b,         s390_vmxlf,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_max_u32_c,         s390_vmxlf,         0,                  BT_OV_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_max_s64_a,         s390_vmxg,          0,                  BT_OV_V2DI_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_max_s64_b,         s390_vmxg,          0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_max_s64_c,         s390_vmxg,          0,                  BT_OV_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_max_u64_a,         s390_vmxlg,         0,                  BT_OV_UV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_max_u64_b,         s390_vmxlg,         0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_max_u64_c,         s390_vmxlg,         0,                  BT_OV_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_max_dbl,           s390_vec_max_dbl,   0,                  BT_OV_V2DF_V2DF_V2DF)
+
+B_DEF      (s390_vmxb,                  smaxv16qi3,         0,                  B_VX,               0,                  BT_FN_V16QI_BV16QI_V16QI)
+B_DEF      (s390_vmxlb,                 umaxv16qi3,         0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vmxh,                  smaxv8hi3,          0,                  B_VX,               0,                  BT_FN_V8HI_BV8HI_V8HI)
+B_DEF      (s390_vmxlh,                 umaxv8hi3,          0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vmxf,                  smaxv4si3,          0,                  B_VX,               0,                  BT_FN_V4SI_BV4SI_V4SI)
+B_DEF      (s390_vmxlf,                 umaxv4si3,          0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vmxg,                  smaxv2di3,          0,                  B_VX,               0,                  BT_FN_V2DI_BV2DI_V2DI)
+B_DEF      (s390_vmxlg,                 umaxv2di3,          0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+B_DEF      (s390_vec_max_dbl,           smaxv2df3,          0,                  B_VX | B_INT,       0,                  BT_FN_V2DF_V2DF_V2DF)
+
+OB_DEF     (s390_vec_min,               s390_vec_min_s8_a,  s390_vec_min_dbl,   B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_min_s8_a,          s390_vmnb,          0,                  BT_OV_V16QI_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_min_s8_b,          s390_vmnb,          0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_min_s8_c,          s390_vmnb,          0,                  BT_OV_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_min_u8_a,          s390_vmnlb,         0,                  BT_OV_UV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_min_u8_b,          s390_vmnlb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_min_u8_c,          s390_vmnlb,         0,                  BT_OV_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_min_s16_a,         s390_vmnh,          0,                  BT_OV_V8HI_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_min_s16_b,         s390_vmnh,          0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_min_s16_c,         s390_vmnh,          0,                  BT_OV_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_min_u16_a,         s390_vmnlh,         0,                  BT_OV_UV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_min_u16_b,         s390_vmnlh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_min_u16_c,         s390_vmnlh,         0,                  BT_OV_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_min_s32_a,         s390_vmnf,          0,                  BT_OV_V4SI_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_min_s32_b,         s390_vmnf,          0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_min_s32_c,         s390_vmnf,          0,                  BT_OV_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_min_u32_a,         s390_vmnlf,         0,                  BT_OV_UV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_min_u32_b,         s390_vmnlf,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_min_u32_c,         s390_vmnlf,         0,                  BT_OV_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_min_s64_a,         s390_vmng,          0,                  BT_OV_V2DI_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_min_s64_b,         s390_vmng,          0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_min_s64_c,         s390_vmng,          0,                  BT_OV_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_min_u64_a,         s390_vmnlg,         0,                  BT_OV_UV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_min_u64_b,         s390_vmnlg,         0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_min_u64_c,         s390_vmnlg,         0,                  BT_OV_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_min_dbl,           s390_vec_min_dbl,   0,                  BT_OV_V2DF_V2DF_V2DF)
+
+B_DEF      (s390_vmnb,                  sminv16qi3,         0,                  B_VX,               0,                  BT_FN_V16QI_BV16QI_V16QI)
+B_DEF      (s390_vmnlb,                 uminv16qi3,         0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vmnh,                  sminv8hi3,          0,                  B_VX,               0,                  BT_FN_V8HI_BV8HI_V8HI)
+B_DEF      (s390_vmnlh,                 uminv8hi3,          0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vmnf,                  sminv4si3,          0,                  B_VX,               0,                  BT_FN_V4SI_BV4SI_V4SI)
+B_DEF      (s390_vmnlf,                 uminv4si3,          0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vmng,                  sminv2di3,          0,                  B_VX,               0,                  BT_FN_V2DI_BV2DI_V2DI)
+B_DEF      (s390_vmnlg,                 uminv2di3,          0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+B_DEF      (s390_vec_min_dbl,           sminv2df3,          0,                  B_VX | B_INT,       0,                  BT_FN_V2DF_V2DF_V2DF)
+
+OB_DEF     (s390_vec_mladd,             s390_vec_mladd_u8,  s390_vec_mladd_s32_c,B_VX,              BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_mladd_u8,          s390_vmalb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mladd_s8_a,        s390_vmalb,         0,                  BT_OV_V16QI_UV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mladd_s8_b,        s390_vmalb,         0,                  BT_OV_V16QI_V16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mladd_s8_c,        s390_vmalb,         0,                  BT_OV_V16QI_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mladd_u16,         s390_vmalhw,        0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mladd_s16_a,       s390_vmalhw,        0,                  BT_OV_V8HI_UV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mladd_s16_b,       s390_vmalhw,        0,                  BT_OV_V8HI_V8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mladd_s16_c,       s390_vmalhw,        0,                  BT_OV_V8HI_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mladd_u32,         s390_vmalf,         0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mladd_s32_a,       s390_vmalf,         0,                  BT_OV_V4SI_UV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_mladd_s32_b,       s390_vmalf,         0,                  BT_OV_V4SI_V4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mladd_s32_c,       s390_vmalf,         0,                  BT_OV_V4SI_V4SI_V4SI_V4SI)
+
+B_DEF      (s390_vmalb,                 vec_vmalv16qi,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vmalhw,                vec_vmalv8hi,       0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vmalf,                 vec_vmalv4si,       0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_mhadd,             s390_vec_mhadd_u8,  s390_vec_mhadd_s32, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_mhadd_u8,          s390_vmalhb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mhadd_s8,          s390_vmahb,         0,                  BT_OV_V16QI_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mhadd_u16,         s390_vmalhh,        0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mhadd_s16,         s390_vmahh,         0,                  BT_OV_V8HI_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mhadd_u32,         s390_vmalhf,        0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mhadd_s32,         s390_vmahf,         0,                  BT_OV_V4SI_V4SI_V4SI_V4SI)
+
+B_DEF      (s390_vmalhb,                vec_vmalhv16qi,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vmahb,                 vec_vmahv16qi,      0,                  B_VX,               0,                  BT_FN_V16QI_V16QI_V16QI_V16QI)
+B_DEF      (s390_vmalhh,                vec_vmalhv8hi,      0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vmahh,                 vec_vmahv8hi,       0,                  B_VX,               0,                  BT_FN_V8HI_V8HI_V8HI_V8HI)
+B_DEF      (s390_vmalhf,                vec_vmalhv4si,      0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vmahf,                 vec_vmahv4si,       0,                  B_VX,               0,                  BT_FN_V4SI_V4SI_V4SI_V4SI)
+
+OB_DEF     (s390_vec_meadd,             s390_vec_meadd_u8,  s390_vec_meadd_s32, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_meadd_u8,          s390_vmaleb,        0,                  BT_OV_UV8HI_UV16QI_UV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_meadd_s8,          s390_vmaeb,         0,                  BT_OV_V8HI_V16QI_V16QI_V8HI)
+OB_DEF_VAR (s390_vec_meadd_u16,         s390_vmaleh,        0,                  BT_OV_UV4SI_UV8HI_UV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_meadd_s16,         s390_vmaeh,         0,                  BT_OV_V4SI_V8HI_V8HI_V4SI)
+OB_DEF_VAR (s390_vec_meadd_u32,         s390_vmalef,        0,                  BT_OV_UV2DI_UV4SI_UV4SI_UV2DI)
+OB_DEF_VAR (s390_vec_meadd_s32,         s390_vmaef,         0,                  BT_OV_V2DI_V4SI_V4SI_V2DI)
+
+B_DEF      (s390_vmaleb,                vec_vmalev16qi,     0,                  B_VX,               0,                  BT_FN_UV8HI_UV16QI_UV16QI_UV8HI)
+B_DEF      (s390_vmaeb,                 vec_vmaev16qi,      0,                  B_VX,               0,                  BT_FN_V8HI_V16QI_V16QI_V8HI)
+B_DEF      (s390_vmaleh,                vec_vmalev8hi,      0,                  B_VX,               0,                  BT_FN_UV4SI_UV8HI_UV8HI_UV4SI)
+B_DEF      (s390_vmaeh,                 vec_vmaev8hi,       0,                  B_VX,               0,                  BT_FN_V4SI_V8HI_V8HI_V4SI)
+B_DEF      (s390_vmalef,                vec_vmalev4si,      0,                  B_VX,               0,                  BT_FN_UV2DI_UV4SI_UV4SI_UV2DI)
+B_DEF      (s390_vmaef,                 vec_vmaev4si,       0,                  B_VX,               0,                  BT_FN_V2DI_V4SI_V4SI_V2DI)
+
+OB_DEF     (s390_vec_moadd,             s390_vec_moadd_u8,  s390_vec_moadd_s32, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_moadd_u8,          s390_vmalob,        0,                  BT_OV_UV8HI_UV16QI_UV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_moadd_s8,          s390_vmaob,         0,                  BT_OV_V8HI_V16QI_V16QI_V8HI)
+OB_DEF_VAR (s390_vec_moadd_u16,         s390_vmaloh,        0,                  BT_OV_UV4SI_UV8HI_UV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_moadd_s16,         s390_vmaoh,         0,                  BT_OV_V4SI_V8HI_V8HI_V4SI)
+OB_DEF_VAR (s390_vec_moadd_u32,         s390_vmalof,        0,                  BT_OV_UV2DI_UV4SI_UV4SI_UV2DI)
+OB_DEF_VAR (s390_vec_moadd_s32,         s390_vmaof,         0,                  BT_OV_V2DI_V4SI_V4SI_V2DI)
+
+B_DEF      (s390_vmalob,                vec_vmalov16qi,     0,                  B_VX,               0,                  BT_FN_UV8HI_UV16QI_UV16QI_UV8HI)
+B_DEF      (s390_vmaob,                 vec_vmaov16qi,      0,                  B_VX,               0,                  BT_FN_V8HI_V16QI_V16QI_V8HI)
+B_DEF      (s390_vmaloh,                vec_vmalov8hi,      0,                  B_VX,               0,                  BT_FN_UV4SI_UV8HI_UV8HI_UV4SI)
+B_DEF      (s390_vmaoh,                 vec_vmaov8hi,       0,                  B_VX,               0,                  BT_FN_V4SI_V8HI_V8HI_V4SI)
+B_DEF      (s390_vmalof,                vec_vmalov4si,      0,                  B_VX,               0,                  BT_FN_UV2DI_UV4SI_UV4SI_UV2DI)
+B_DEF      (s390_vmaof,                 vec_vmaov4si,       0,                  B_VX,               0,                  BT_FN_V2DI_V4SI_V4SI_V2DI)
+
+OB_DEF     (s390_vec_mulh,              s390_vec_mulh_u8,   s390_vec_mulh_s32,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_mulh_u8,           s390_vmlhb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mulh_s8,           s390_vmhb,          0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mulh_u16,          s390_vmlhh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mulh_s16,          s390_vmhh,          0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mulh_u32,          s390_vmlhf,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mulh_s32,          s390_vmhf,          0,                  BT_OV_V4SI_V4SI_V4SI)
+
+B_DEF      (s390_vmlhb,                 vec_umulhv16qi,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vmhb,                  vec_smulhv16qi,     0,                  B_VX,               0,                  BT_FN_V16QI_V16QI_V16QI)
+B_DEF      (s390_vmlhh,                 vec_umulhv8hi,      0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vmhh,                  vec_smulhv8hi,      0,                  B_VX,               0,                  BT_FN_V8HI_V8HI_V8HI)
+B_DEF      (s390_vmlhf,                 vec_umulhv4si,      0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vmhf,                  vec_smulhv4si,      0,                  B_VX,               0,                  BT_FN_V4SI_V4SI_V4SI)
+
+OB_DEF     (s390_vec_mule,              s390_vec_mule_u8,   s390_vec_mule_s32,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_mule_u8,           s390_vmleb,         0,                  BT_OV_UV8HI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mule_s8,           s390_vmeb,          0,                  BT_OV_V8HI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mule_u16,          s390_vmleh,         0,                  BT_OV_UV4SI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mule_s15,          s390_vmeh,          0,                  BT_OV_V4SI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mule_u32,          s390_vmlef,         0,                  BT_OV_UV2DI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mule_s32,          s390_vmef,          0,                  BT_OV_V2DI_V4SI_V4SI)
+
+B_DEF      (s390_vmleb,                 vec_widen_umult_even_v16qi,0,           B_VX,               0,                  BT_FN_UV8HI_UV16QI_UV16QI)
+B_DEF      (s390_vmeb,                  vec_widen_smult_even_v16qi,0,           B_VX,               0,                  BT_FN_V8HI_V16QI_V16QI)
+B_DEF      (s390_vmleh,                 vec_widen_umult_even_v8hi,0,            B_VX,               0,                  BT_FN_UV4SI_UV8HI_UV8HI)
+B_DEF      (s390_vmeh,                  vec_widen_smult_even_v8hi,0,            B_VX,               0,                  BT_FN_V4SI_V8HI_V8HI)
+B_DEF      (s390_vmlef,                 vec_widen_umult_even_v4si,0,            B_VX,               0,                  BT_FN_UV2DI_UV4SI_UV4SI)
+B_DEF      (s390_vmef,                  vec_widen_smult_even_v4si,0,            B_VX,               0,                  BT_FN_V2DI_V4SI_V4SI)
+
+OB_DEF     (s390_vec_mulo,              s390_vec_mulo_u8,   s390_vec_mulo_s32,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_mulo_u8,           s390_vmlob,         0,                  BT_OV_UV8HI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_mulo_s8,           s390_vmob,          0,                  BT_OV_V8HI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_mulo_u16,          s390_vmloh,         0,                  BT_OV_UV4SI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_mulo_s16,          s390_vmoh,          0,                  BT_OV_V4SI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_mulo_u32,          s390_vmlof,         0,                  BT_OV_UV2DI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_mulo_s32,          s390_vmof,          0,                  BT_OV_V2DI_V4SI_V4SI)
+
+B_DEF      (s390_vmlob,                 vec_widen_umult_odd_v16qi,0,            B_VX,               0,                  BT_FN_UV8HI_UV16QI_UV16QI)
+B_DEF      (s390_vmob,                  vec_widen_smult_odd_v16qi,0,            B_VX,               0,                  BT_FN_V8HI_V16QI_V16QI)
+B_DEF      (s390_vmloh,                 vec_widen_umult_odd_v8hi,0,             B_VX,               0,                  BT_FN_UV4SI_UV8HI_UV8HI)
+B_DEF      (s390_vmoh,                  vec_widen_smult_odd_v8hi,0,             B_VX,               0,                  BT_FN_V4SI_V8HI_V8HI)
+B_DEF      (s390_vmlof,                 vec_widen_umult_odd_v4si,0,             B_VX,               0,                  BT_FN_UV2DI_UV4SI_UV4SI)
+B_DEF      (s390_vmof,                  vec_widen_smult_odd_v4si,0,             B_VX,               0,                  BT_FN_V2DI_V4SI_V4SI)
+
+OB_DEF     (s390_vec_nor,               s390_vec_nor_b8,    s390_vec_nor_dbl_c, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_nor_b8,            s390_vno,           0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_nor_s8_a,          s390_vno,           0,                  BT_OV_V16QI_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_nor_s8_b,          s390_vno,           0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_nor_s8_c,          s390_vno,           0,                  BT_OV_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_nor_u8_a,          s390_vno,           0,                  BT_OV_UV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_nor_u8_b,          s390_vno,           0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_nor_u8_c,          s390_vno,           0,                  BT_OV_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_nor_b16,           s390_vno,           0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_nor_s16_a,         s390_vno,           0,                  BT_OV_V8HI_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_nor_s16_b,         s390_vno,           0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_nor_s16_c,         s390_vno,           0,                  BT_OV_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_nor_u16_a,         s390_vno,           0,                  BT_OV_UV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_nor_u16_b,         s390_vno,           0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_nor_u16_c,         s390_vno,           0,                  BT_OV_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_nor_b32,           s390_vno,           0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_nor_s32_a,         s390_vno,           0,                  BT_OV_V4SI_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_nor_s32_b,         s390_vno,           0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_nor_s32_c,         s390_vno,           0,                  BT_OV_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_nor_u32_a,         s390_vno,           0,                  BT_OV_UV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_nor_u32_b,         s390_vno,           0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_nor_u32_c,         s390_vno,           0,                  BT_OV_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_nor_b64,           s390_vno,           0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_nor_s64_a,         s390_vno,           0,                  BT_OV_V2DI_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_nor_s64_b,         s390_vno,           0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_nor_s64_c,         s390_vno,           0,                  BT_OV_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_nor_u64_a,         s390_vno,           0,                  BT_OV_UV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_nor_u64_b,         s390_vno,           0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_nor_u64_c,         s390_vno,           0,                  BT_OV_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_nor_dbl_a,         s390_vno,           0,                  BT_OV_V2DF_BV2DI_V2DF)
+OB_DEF_VAR (s390_vec_nor_dbl_b,         s390_vno,           0,                  BT_OV_V2DF_V2DF_V2DF)
+OB_DEF_VAR (s390_vec_nor_dbl_c,         s390_vno,           0,                  BT_OV_V2DF_V2DF_BV2DI)
+
+B_DEF      (s390_vno,                   vec_norv16qi3,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_or,                s390_vec_or_b8,     s390_vec_or_dbl_c,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_or_b8,             s390_vo,            0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_or_s8_a,           s390_vo,            0,                  BT_OV_V16QI_BV16QI_V16QI)
+OB_DEF_VAR (s390_vec_or_s8_b,           s390_vo,            0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_or_s8_c,           s390_vo,            0,                  BT_OV_V16QI_V16QI_BV16QI)
+OB_DEF_VAR (s390_vec_or_u8_a,           s390_vo,            0,                  BT_OV_UV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_or_u8_b,           s390_vo,            0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_or_u8_c,           s390_vo,            0,                  BT_OV_UV16QI_UV16QI_BV16QI)
+OB_DEF_VAR (s390_vec_or_b16,            s390_vo,            0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_or_s16_a,          s390_vo,            0,                  BT_OV_V8HI_BV8HI_V8HI)
+OB_DEF_VAR (s390_vec_or_s16_b,          s390_vo,            0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_or_s16_c,          s390_vo,            0,                  BT_OV_V8HI_V8HI_BV8HI)
+OB_DEF_VAR (s390_vec_or_u16_a,          s390_vo,            0,                  BT_OV_UV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_or_u16_b,          s390_vo,            0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_or_u16_c,          s390_vo,            0,                  BT_OV_UV8HI_UV8HI_BV8HI)
+OB_DEF_VAR (s390_vec_or_b32,            s390_vo,            0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_or_s32_a,          s390_vo,            0,                  BT_OV_V4SI_BV4SI_V4SI)
+OB_DEF_VAR (s390_vec_or_s32_b,          s390_vo,            0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_or_s32_c,          s390_vo,            0,                  BT_OV_V4SI_V4SI_BV4SI)
+OB_DEF_VAR (s390_vec_or_u32_a,          s390_vo,            0,                  BT_OV_UV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_or_u32_b,          s390_vo,            0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_or_u32_c,          s390_vo,            0,                  BT_OV_UV4SI_UV4SI_BV4SI)
+OB_DEF_VAR (s390_vec_or_b64,            s390_vo,            0,                  BT_OV_BV2DI_BV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_or_s64_a,          s390_vo,            0,                  BT_OV_V2DI_BV2DI_V2DI)
+OB_DEF_VAR (s390_vec_or_s64_b,          s390_vo,            0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_or_s64_c,          s390_vo,            0,                  BT_OV_V2DI_V2DI_BV2DI)
+OB_DEF_VAR (s390_vec_or_u64_a,          s390_vo,            0,                  BT_OV_UV2DI_BV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_or_u64_b,          s390_vo,            0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_or_u64_c,          s390_vo,            0,                  BT_OV_UV2DI_UV2DI_BV2DI)
+OB_DEF_VAR (s390_vec_or_dbl_a,          s390_vo,            0,                  BT_OV_V2DF_BV2DI_V2DF)
+OB_DEF_VAR (s390_vec_or_dbl_b,          s390_vo,            0,                  BT_OV_V2DF_V2DF_V2DF)
+OB_DEF_VAR (s390_vec_or_dbl_c,          s390_vo,            0,                  BT_OV_V2DF_V2DF_BV2DI)
+
+B_DEF      (s390_vo,                    iorv16qi3,          0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_popcnt,            s390_vec_popcnt_s8, s390_vec_popcnt_u64,B_VX,               BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_popcnt_s8,         s390_vpopctb,       0,                  BT_OV_UV16QI_V16QI)  /* vpopct */
+OB_DEF_VAR (s390_vec_popcnt_u8,         s390_vpopctb,       0,                  BT_OV_UV16QI_UV16QI) /* vpopct */
+OB_DEF_VAR (s390_vec_popcnt_s16,        s390_vpopcth,       0,                  BT_OV_UV8HI_V8HI)    /* vpopct */
+OB_DEF_VAR (s390_vec_popcnt_u16,        s390_vpopcth,       0,                  BT_OV_UV8HI_UV8HI)   /* vpopct */
+OB_DEF_VAR (s390_vec_popcnt_s32,        s390_vpopctf,       0,                  BT_OV_UV4SI_V4SI)    /* vpopct vsumb */
+OB_DEF_VAR (s390_vec_popcnt_u32,        s390_vpopctf,       0,                  BT_OV_UV4SI_UV4SI)   /* vpopct vsumb */
+OB_DEF_VAR (s390_vec_popcnt_s64,        s390_vpopctg,       0,                  BT_OV_UV2DI_V2DI)    /* vpopct vsumb vsumgf */
+OB_DEF_VAR (s390_vec_popcnt_u64,        s390_vpopctg,       0,                  BT_OV_UV2DI_UV2DI)   /* vpopct vsumb vsumgf */
+
+B_DEF      (s390_vpopctb,               popcountv16qi2,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI) /* vpopct */
+B_DEF      (s390_vpopcth,               popcountv8hi2,      0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI)   /* vpopct */
+B_DEF      (s390_vpopctf,               popcountv4si2,      0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI)   /* vpopct vsumb */
+B_DEF      (s390_vpopctg,               popcountv2di2,      0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI)   /* vpopct vsumb vsumgf */
+
+OB_DEF     (s390_vec_rl,                s390_vec_rl_u8,     s390_vec_rl_s64,    B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_rl_u8,             s390_verllvb,       0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_rl_s8,             s390_verllvb,       0,                  BT_OV_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_rl_u16,            s390_verllvh,       0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_rl_s16,            s390_verllvh,       0,                  BT_OV_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_rl_u32,            s390_verllvf,       0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_rl_s32,            s390_verllvf,       0,                  BT_OV_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_rl_u64,            s390_verllvg,       0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_rl_s64,            s390_verllvg,       0,                  BT_OV_V2DI_V2DI_UV2DI)
+
+B_DEF      (s390_verllvb,               vrotlv16qi3,        0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_verllvh,               vrotlv8hi3,         0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_verllvf,               vrotlv4si3,         0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_verllvg,               vrotlv2di3,         0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_rli,               s390_vec_rli_u8,    s390_vec_rli_s64,   B_VX,               BT_FN_OV4SI_OV4SI_ULONG)
+OB_DEF_VAR (s390_vec_rli_u8,            s390_verllb,        0,                  BT_OV_UV16QI_UV16QI_ULONG)
+OB_DEF_VAR (s390_vec_rli_s8,            s390_verllb,        0,                  BT_OV_V16QI_V16QI_ULONG)
+OB_DEF_VAR (s390_vec_rli_u16,           s390_verllh,        0,                  BT_OV_UV8HI_UV8HI_ULONG)
+OB_DEF_VAR (s390_vec_rli_s16,           s390_verllh,        0,                  BT_OV_V8HI_V8HI_ULONG)
+OB_DEF_VAR (s390_vec_rli_u32,           s390_verllf,        0,                  BT_OV_UV4SI_UV4SI_ULONG)
+OB_DEF_VAR (s390_vec_rli_s32,           s390_verllf,        0,                  BT_OV_V4SI_V4SI_ULONG)
+OB_DEF_VAR (s390_vec_rli_u64,           s390_verllg,        0,                  BT_OV_UV2DI_UV2DI_ULONG)
+OB_DEF_VAR (s390_vec_rli_s64,           s390_verllg,        0,                  BT_OV_V2DI_V2DI_ULONG)
+
+B_DEF      (s390_verllb,                rotlv16qi3,         0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UINT)
+B_DEF      (s390_verllh,                rotlv8hi3,          0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UINT)
+B_DEF      (s390_verllf,                rotlv4si3,          0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UINT)
+B_DEF      (s390_verllg,                rotlv2di3,          0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UINT)
+
+OB_DEF     (s390_vec_rl_mask,           s390_vec_rl_mask_s8,s390_vec_rl_mask_u64,B_VX,              BT_FN_OV4SI_OV4SI_OV4SI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_s8,        s390_verimb,        O3_U8,              BT_OV_V16QI_V16QI_UV16QI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_u8,        s390_verimb,        O3_U8,              BT_OV_UV16QI_UV16QI_UV16QI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_s16,       s390_verimh,        O3_U8,              BT_OV_V8HI_V8HI_UV8HI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_u16,       s390_verimh,        O3_U8,              BT_OV_UV8HI_UV8HI_UV8HI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_s32,       s390_verimf,        O3_U8,              BT_OV_V4SI_V4SI_UV4SI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_u32,       s390_verimf,        O3_U8,              BT_OV_UV4SI_UV4SI_UV4SI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_s64,       s390_verimg,        O3_U8,              BT_OV_V2DI_V2DI_UV2DI_UCHAR)
+OB_DEF_VAR (s390_vec_rl_mask_u64,       s390_verimg,        O3_U8,              BT_OV_UV2DI_UV2DI_UV2DI_UCHAR)
+
+B_DEF      (s390_verimb,                verimv16qi,         0,                  B_VX,               O4_U8,              BT_FN_UV16QI_UV16QI_UV16QI_UV16QI_INT)
+B_DEF      (s390_verimh,                verimv8hi,          0,                  B_VX,               O4_U8,              BT_FN_UV8HI_UV8HI_UV8HI_UV8HI_INT)
+B_DEF      (s390_verimf,                verimv4si,          0,                  B_VX,               O4_U8,              BT_FN_UV4SI_UV4SI_UV4SI_UV4SI_INT)
+B_DEF      (s390_verimg,                verimv2di,          0,                  B_VX,               O4_U8,              BT_FN_UV2DI_UV2DI_UV2DI_UV2DI_INT)
+
+OB_DEF     (s390_vec_sll,               s390_vec_sll_u8q,   s390_vec_sll_b64s,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_sll_u8q,           s390_vsl,           0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_u8h,           s390_vsl,           0,                  BT_OV_UV16QI_UV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_u8s,           s390_vsl,           0,                  BT_OV_UV16QI_UV16QI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_s8q,           s390_vsl,           0,                  BT_OV_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_s8h,           s390_vsl,           0,                  BT_OV_V16QI_V16QI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_s8s,           s390_vsl,           0,                  BT_OV_V16QI_V16QI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_b8q,           s390_vsl,           0,                  BT_OV_BV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_b8h,           s390_vsl,           0,                  BT_OV_BV16QI_BV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_b8s,           s390_vsl,           0,                  BT_OV_BV16QI_BV16QI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_u16q,          s390_vsl,           0,                  BT_OV_UV8HI_UV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_u16h,          s390_vsl,           0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_u16s,          s390_vsl,           0,                  BT_OV_UV8HI_UV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_s16q,          s390_vsl,           0,                  BT_OV_V8HI_V8HI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_s16h,          s390_vsl,           0,                  BT_OV_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_s16s,          s390_vsl,           0,                  BT_OV_V8HI_V8HI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_b16q,          s390_vsl,           0,                  BT_OV_BV8HI_BV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_b16h,          s390_vsl,           0,                  BT_OV_BV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_b16s,          s390_vsl,           0,                  BT_OV_BV8HI_BV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_u32q,          s390_vsl,           0,                  BT_OV_UV4SI_UV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_u32h,          s390_vsl,           0,                  BT_OV_UV4SI_UV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_u32s,          s390_vsl,           0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_s32q,          s390_vsl,           0,                  BT_OV_V4SI_V4SI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_s32h,          s390_vsl,           0,                  BT_OV_V4SI_V4SI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_s32s,          s390_vsl,           0,                  BT_OV_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_b32q,          s390_vsl,           0,                  BT_OV_BV4SI_BV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_b32h,          s390_vsl,           0,                  BT_OV_BV4SI_BV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_b32s,          s390_vsl,           0,                  BT_OV_BV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_u64q,          s390_vsl,           0,                  BT_OV_UV2DI_UV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_u64h,          s390_vsl,           0,                  BT_OV_UV2DI_UV2DI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_u64s,          s390_vsl,           0,                  BT_OV_UV2DI_UV2DI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_s64q,          s390_vsl,           0,                  BT_OV_V2DI_V2DI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_s64h,          s390_vsl,           0,                  BT_OV_V2DI_V2DI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_s64s,          s390_vsl,           0,                  BT_OV_V2DI_V2DI_UV4SI)
+OB_DEF_VAR (s390_vec_sll_b64q,          s390_vsl,           0,                  BT_OV_BV2DI_BV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_sll_b64h,          s390_vsl,           0,                  BT_OV_BV2DI_BV2DI_UV8HI)
+OB_DEF_VAR (s390_vec_sll_b64s,          s390_vsl,           0,                  BT_OV_BV2DI_BV2DI_UV4SI)
+
+B_DEF      (s390_vsl,                   vec_sllv16qiv16qi,  0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_slb,               s390_vec_slb_u8_u8, s390_vec_slb_dbl_s64,B_VX,              BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_slb_u8_u8,         s390_vslb,          0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_slb_u8_s8,         s390_vslb,          0,                  BT_OV_UV16QI_UV16QI_V16QI)
+OB_DEF_VAR (s390_vec_slb_s8_u8,         s390_vslb,          0,                  BT_OV_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_slb_s8_s8,         s390_vslb,          0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_slb_u16_u16,       s390_vslb,          0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_slb_u16_s16,       s390_vslb,          0,                  BT_OV_UV8HI_UV8HI_V8HI)
+OB_DEF_VAR (s390_vec_slb_s16_u16,       s390_vslb,          0,                  BT_OV_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_slb_s16_s16,       s390_vslb,          0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_slb_u32_u32,       s390_vslb,          0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_slb_u32_s32,       s390_vslb,          0,                  BT_OV_UV4SI_UV4SI_V4SI)
+OB_DEF_VAR (s390_vec_slb_s32_u32,       s390_vslb,          0,                  BT_OV_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_slb_s32_s32,       s390_vslb,          0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_slb_u64_u64,       s390_vslb,          0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_slb_u64_s64,       s390_vslb,          0,                  BT_OV_UV2DI_UV2DI_V2DI)
+OB_DEF_VAR (s390_vec_slb_s64_u64,       s390_vslb,          0,                  BT_OV_V2DI_V2DI_UV2DI)
+OB_DEF_VAR (s390_vec_slb_s64_s64,       s390_vslb,          0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_slb_dbl_u64,       s390_vslb,          0,                  BT_OV_V2DF_V2DF_UV2DI)
+OB_DEF_VAR (s390_vec_slb_dbl_s64,       s390_vslb,          0,                  BT_OV_V2DF_V2DF_V2DI)
+
+B_DEF      (s390_vslb,                  vec_slbv16qi,       0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_sld,               s390_vec_sld_s8,    s390_vec_sld_dbl,   B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_s8,            s390_vsldb,         O3_U4,              BT_OV_V16QI_V16QI_V16QI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_u8,            s390_vsldb,         O3_U4,              BT_OV_UV16QI_UV16QI_UV16QI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_s16,           s390_vsldb,         O3_U4,              BT_OV_V8HI_V8HI_V8HI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_u16,           s390_vsldb,         O3_U4,              BT_OV_UV8HI_UV8HI_UV8HI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_s32,           s390_vsldb,         O3_U4,              BT_OV_V4SI_V4SI_V4SI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_u32,           s390_vsldb,         O3_U4,              BT_OV_UV4SI_UV4SI_UV4SI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_s64,           s390_vsldb,         O3_U4,              BT_OV_V2DI_V2DI_V2DI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_u64,           s390_vsldb,         O3_U4,              BT_OV_UV2DI_UV2DI_UV2DI_ULONGLONG)
+OB_DEF_VAR (s390_vec_sld_dbl,           s390_vsldb,         O3_U4,              BT_OV_V2DF_V2DF_V2DF_ULONGLONG)
+
+B_DEF      (s390_vsldb,                 vec_sldv16qi,       0,                  B_VX,               O3_U4,              BT_FN_UV16QI_UV16QI_UV16QI_INT)
+
+OB_DEF     (s390_vec_sldw,              s390_vec_sldw_s8,   s390_vec_sldw_dbl,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_INT)
+OB_DEF_VAR (s390_vec_sldw_s8,           s390_vsldb,         O3_U4,              BT_OV_V16QI_V16QI_V16QI_INT)
+OB_DEF_VAR (s390_vec_sldw_u8,           s390_vsldb,         O3_U4,              BT_OV_UV16QI_UV16QI_UV16QI_INT)
+OB_DEF_VAR (s390_vec_sldw_s16,          s390_vsldb,         O3_U4,              BT_OV_V8HI_V8HI_V8HI_INT)
+OB_DEF_VAR (s390_vec_sldw_u16,          s390_vsldb,         O3_U4,              BT_OV_UV8HI_UV8HI_UV8HI_INT)
+OB_DEF_VAR (s390_vec_sldw_s32,          s390_vsldb,         O3_U4,              BT_OV_V4SI_V4SI_V4SI_INT)
+OB_DEF_VAR (s390_vec_sldw_u32,          s390_vsldb,         O3_U4,              BT_OV_UV4SI_UV4SI_UV4SI_INT)
+OB_DEF_VAR (s390_vec_sldw_s64,          s390_vsldb,         O3_U4,              BT_OV_V2DI_V2DI_V2DI_INT)
+OB_DEF_VAR (s390_vec_sldw_u64,          s390_vsldb,         O3_U4,              BT_OV_UV2DI_UV2DI_UV2DI_INT)
+OB_DEF_VAR (s390_vec_sldw_dbl,          s390_vsldb,         O3_U4,              BT_OV_V2DF_V2DF_V2DF_INT)
+
+OB_DEF     (s390_vec_sral,              s390_vec_sral_u8q,  s390_vec_sral_b64s, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_sral_u8q,          s390_vsra,          0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_u8h,          s390_vsra,          0,                  BT_OV_UV16QI_UV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_u8s,          s390_vsra,          0,                  BT_OV_UV16QI_UV16QI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_s8q,          s390_vsra,          0,                  BT_OV_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_s8h,          s390_vsra,          0,                  BT_OV_V16QI_V16QI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_s8s,          s390_vsra,          0,                  BT_OV_V16QI_V16QI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_b8q,          s390_vsra,          0,                  BT_OV_BV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_b8h,          s390_vsra,          0,                  BT_OV_BV16QI_BV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_b8s,          s390_vsra,          0,                  BT_OV_BV16QI_BV16QI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_u16q,         s390_vsra,          0,                  BT_OV_UV8HI_UV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_u16h,         s390_vsra,          0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_u16s,         s390_vsra,          0,                  BT_OV_UV8HI_UV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_s16q,         s390_vsra,          0,                  BT_OV_V8HI_V8HI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_s16h,         s390_vsra,          0,                  BT_OV_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_s16s,         s390_vsra,          0,                  BT_OV_V8HI_V8HI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_b16q,         s390_vsra,          0,                  BT_OV_BV8HI_BV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_b16h,         s390_vsra,          0,                  BT_OV_BV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_b16s,         s390_vsra,          0,                  BT_OV_BV8HI_BV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_u32q,         s390_vsra,          0,                  BT_OV_UV4SI_UV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_u32h,         s390_vsra,          0,                  BT_OV_UV4SI_UV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_u32s,         s390_vsra,          0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_s32q,         s390_vsra,          0,                  BT_OV_V4SI_V4SI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_s32h,         s390_vsra,          0,                  BT_OV_V4SI_V4SI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_s32s,         s390_vsra,          0,                  BT_OV_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_b32q,         s390_vsra,          0,                  BT_OV_BV4SI_BV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_b32h,         s390_vsra,          0,                  BT_OV_BV4SI_BV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_b32s,         s390_vsra,          0,                  BT_OV_BV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_u64q,         s390_vsra,          0,                  BT_OV_UV2DI_UV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_u64h,         s390_vsra,          0,                  BT_OV_UV2DI_UV2DI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_u64s,         s390_vsra,          0,                  BT_OV_UV2DI_UV2DI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_s64q,         s390_vsra,          0,                  BT_OV_V2DI_V2DI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_s64h,         s390_vsra,          0,                  BT_OV_V2DI_V2DI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_s64s,         s390_vsra,          0,                  BT_OV_V2DI_V2DI_UV4SI)
+OB_DEF_VAR (s390_vec_sral_b64q,         s390_vsra,          0,                  BT_OV_BV2DI_BV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_sral_b64h,         s390_vsra,          0,                  BT_OV_BV2DI_BV2DI_UV8HI)
+OB_DEF_VAR (s390_vec_sral_b64s,         s390_vsra,          0,                  BT_OV_BV2DI_BV2DI_UV4SI)
+
+B_DEF      (s390_vsra,                  vec_sralv16qiv16qi, 0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_srab,              s390_vec_srab_u8_u8,s390_vec_srab_dbl_s64,B_VX,             BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_srab_u8_u8,        s390_vsrab,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_srab_u8_s8,        s390_vsrab,         0,                  BT_OV_UV16QI_UV16QI_V16QI)
+OB_DEF_VAR (s390_vec_srab_s8_u8,        s390_vsrab,         0,                  BT_OV_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_srab_s8_s8,        s390_vsrab,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_srab_u16_u16,      s390_vsrab,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_srab_u16_s16,      s390_vsrab,         0,                  BT_OV_UV8HI_UV8HI_V8HI)
+OB_DEF_VAR (s390_vec_srab_s16_u16,      s390_vsrab,         0,                  BT_OV_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_srab_s16_s16,      s390_vsrab,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_srab_u32_u32,      s390_vsrab,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_srab_u32_s32,      s390_vsrab,         0,                  BT_OV_UV4SI_UV4SI_V4SI)
+OB_DEF_VAR (s390_vec_srab_s32_u32,      s390_vsrab,         0,                  BT_OV_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_srab_s32_s32,      s390_vsrab,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_srab_u64_u64,      s390_vsrab,         0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_srab_u64_s64,      s390_vsrab,         0,                  BT_OV_UV2DI_UV2DI_V2DI)
+OB_DEF_VAR (s390_vec_srab_s64_u64,      s390_vsrab,         0,                  BT_OV_V2DI_V2DI_UV2DI)
+OB_DEF_VAR (s390_vec_srab_s64_s64,      s390_vsrab,         0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_srab_dbl_u64,      s390_vsrab,         0,                  BT_OV_V2DF_V2DF_UV2DI)
+OB_DEF_VAR (s390_vec_srab_dbl_s64,      s390_vsrab,         0,                  BT_OV_V2DF_V2DF_V2DI)
+
+B_DEF      (s390_vsrab,                 vec_srabv16qi,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_srl,               s390_vec_srl_u8q,   s390_vec_srl_b64s,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_srl_u8q,           s390_vsrl,          0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_u8h,           s390_vsrl,          0,                  BT_OV_UV16QI_UV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_u8s,           s390_vsrl,          0,                  BT_OV_UV16QI_UV16QI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_s8q,           s390_vsrl,          0,                  BT_OV_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_s8h,           s390_vsrl,          0,                  BT_OV_V16QI_V16QI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_s8s,           s390_vsrl,          0,                  BT_OV_V16QI_V16QI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_b8q,           s390_vsrl,          0,                  BT_OV_BV16QI_BV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_b8h,           s390_vsrl,          0,                  BT_OV_BV16QI_BV16QI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_b8s,           s390_vsrl,          0,                  BT_OV_BV16QI_BV16QI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_u16q,          s390_vsrl,          0,                  BT_OV_UV8HI_UV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_u16h,          s390_vsrl,          0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_u16s,          s390_vsrl,          0,                  BT_OV_UV8HI_UV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_s16q,          s390_vsrl,          0,                  BT_OV_V8HI_V8HI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_s16h,          s390_vsrl,          0,                  BT_OV_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_s16s,          s390_vsrl,          0,                  BT_OV_V8HI_V8HI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_b16q,          s390_vsrl,          0,                  BT_OV_BV8HI_BV8HI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_b16h,          s390_vsrl,          0,                  BT_OV_BV8HI_BV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_b16s,          s390_vsrl,          0,                  BT_OV_BV8HI_BV8HI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_u32q,          s390_vsrl,          0,                  BT_OV_UV4SI_UV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_u32h,          s390_vsrl,          0,                  BT_OV_UV4SI_UV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_u32s,          s390_vsrl,          0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_s32q,          s390_vsrl,          0,                  BT_OV_V4SI_V4SI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_s32h,          s390_vsrl,          0,                  BT_OV_V4SI_V4SI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_s32s,          s390_vsrl,          0,                  BT_OV_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_b32q,          s390_vsrl,          0,                  BT_OV_BV4SI_BV4SI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_b32h,          s390_vsrl,          0,                  BT_OV_BV4SI_BV4SI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_b32s,          s390_vsrl,          0,                  BT_OV_BV4SI_BV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_u64q,          s390_vsrl,          0,                  BT_OV_UV2DI_UV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_u64h,          s390_vsrl,          0,                  BT_OV_UV2DI_UV2DI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_u64s,          s390_vsrl,          0,                  BT_OV_UV2DI_UV2DI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_s64q,          s390_vsrl,          0,                  BT_OV_V2DI_V2DI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_s64h,          s390_vsrl,          0,                  BT_OV_V2DI_V2DI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_s64s,          s390_vsrl,          0,                  BT_OV_V2DI_V2DI_UV4SI)
+OB_DEF_VAR (s390_vec_srl_b64q,          s390_vsrl,          0,                  BT_OV_BV2DI_BV2DI_UV16QI)
+OB_DEF_VAR (s390_vec_srl_b64h,          s390_vsrl,          0,                  BT_OV_BV2DI_BV2DI_UV8HI)
+OB_DEF_VAR (s390_vec_srl_b64s,          s390_vsrl,          0,                  BT_OV_BV2DI_BV2DI_UV4SI)
+
+B_DEF      (s390_vsrl,                  vec_srlv16qiv16qi,  0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_srb,               s390_vec_srb_u8_u8, s390_vec_srb_dbl_s64,B_VX,              BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_srb_u8_u8,         s390_vsrlb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_srb_u8_s8,         s390_vsrlb,         0,                  BT_OV_UV16QI_UV16QI_V16QI)
+OB_DEF_VAR (s390_vec_srb_s8_u8,         s390_vsrlb,         0,                  BT_OV_V16QI_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_srb_s8_s8,         s390_vsrlb,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vec_srb_u16_u16,       s390_vsrlb,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_srb_u16_s16,       s390_vsrlb,         0,                  BT_OV_UV8HI_UV8HI_V8HI)
+OB_DEF_VAR (s390_vec_srb_s16_u16,       s390_vsrlb,         0,                  BT_OV_V8HI_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_srb_s16_s16,       s390_vsrlb,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vec_srb_u32_u32,       s390_vsrlb,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_srb_u32_s32,       s390_vsrlb,         0,                  BT_OV_UV4SI_UV4SI_V4SI)
+OB_DEF_VAR (s390_vec_srb_s32_u32,       s390_vsrlb,         0,                  BT_OV_V4SI_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_srb_s32_s32,       s390_vsrlb,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vec_srb_u64_u64,       s390_vsrlb,         0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_srb_u64_s64,       s390_vsrlb,         0,                  BT_OV_UV2DI_UV2DI_V2DI)
+OB_DEF_VAR (s390_vec_srb_s64_u64,       s390_vsrlb,         0,                  BT_OV_V2DI_V2DI_UV2DI)
+OB_DEF_VAR (s390_vec_srb_s64_s64,       s390_vsrlb,         0,                  BT_OV_V2DI_V2DI_V2DI)
+OB_DEF_VAR (s390_vec_srb_dbl_u64,       s390_vsrlb,         0,                  BT_OV_V2DF_V2DF_UV2DI)
+OB_DEF_VAR (s390_vec_srb_dbl_s64,       s390_vsrlb,         0,                  BT_OV_V2DF_V2DF_V2DI)
+
+B_DEF      (s390_vsrlb,                 vec_srbv16qi,       0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vsq,                   vec_sub_u128,       0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_subc,              s390_vec_subc_u8,   s390_vec_subc_u64,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_subc_u8,           s390_vscbib,        0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_subc_u16,          s390_vscbih,        0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_subc_u32,          s390_vscbif,        0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_subc_u64,          s390_vscbig,        0,                  BT_OV_UV2DI_UV2DI_UV2DI)
+
+B_DEF      (s390_vscbib,                vec_subcv16qi,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vscbih,                vec_subcv8hi,       0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vscbif,                vec_subcv4si,       0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vscbig,                vec_subcv2di,       0,                  B_VX,               0,                  BT_FN_UV2DI_UV2DI_UV2DI)
+B_DEF      (s390_vscbiq,                vec_subc_u128,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vsbiq,                 vec_sube_u128,      0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vsbcbiq,               vec_subec_u128,     0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_UV16QI)
+
+OB_DEF     (s390_vec_sum2,              s390_vec_sum2_u16,  s390_vec_sum2_u32,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_sum2_u16,          s390_vsumgh,        0,                  BT_OV_UV2DI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_sum2_u32,          s390_vsumgf,        0,                  BT_OV_UV2DI_UV4SI_UV4SI)
+
+B_DEF      (s390_vsumgh,                vec_sum2v8hi,       0,                  B_VX,               0,                  BT_FN_UV2DI_UV8HI_UV8HI)
+B_DEF      (s390_vsumgf,                vec_sum2v4si,       0,                  B_VX,               0,                  BT_FN_UV2DI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_sum_u128,          s390_vec_sum_u128_u32,s390_vec_sum_u128_u64,B_VX,           BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_sum_u128_u32,      s390_vsumqf,        0,                  BT_OV_UV16QI_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_sum_u128_u64,      s390_vsumqg,        0,                  BT_OV_UV16QI_UV2DI_UV2DI)
+
+B_DEF      (s390_vsumqf,                vec_sum_u128v4si,   0,                  B_VX,               0,                  BT_FN_UV16QI_UV4SI_UV4SI)
+B_DEF      (s390_vsumqg,                vec_sum_u128v2di,   0,                  B_VX,               0,                  BT_FN_UV16QI_UV2DI_UV2DI)
+
+OB_DEF     (s390_vec_sum4,              s390_vec_sum4_u8,   s390_vec_sum4_u16,  B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_sum4_u8,           s390_vsumb,         0,                  BT_OV_UV4SI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_sum4_u16,          s390_vsumh,         0,                  BT_OV_UV4SI_UV8HI_UV8HI)
+
+B_DEF      (s390_vsumb,                 vec_sum4v16qi,      0,                  B_VX,               0,                  BT_FN_UV4SI_UV16QI_UV16QI)
+B_DEF      (s390_vsumh,                 vec_sum4v8hi,       0,                  B_VX,               0,                  BT_FN_UV4SI_UV8HI_UV8HI)
+
+OB_DEF     (s390_vec_test_mask,         s390_vec_test_mask_s8,s390_vec_test_mask_dbl,B_VX,          BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vec_test_mask_s8,      s390_vtm,           0,                  BT_OV_INT_V16QI_UV16QI)
+OB_DEF_VAR (s390_vec_test_mask_u8,      s390_vtm,           0,                  BT_OV_INT_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vec_test_mask_s16,     s390_vtm,           0,                  BT_OV_INT_V8HI_UV8HI)
+OB_DEF_VAR (s390_vec_test_mask_u16,     s390_vtm,           0,                  BT_OV_INT_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vec_test_mask_s32,     s390_vtm,           0,                  BT_OV_INT_V4SI_UV4SI)
+OB_DEF_VAR (s390_vec_test_mask_u32,     s390_vtm,           0,                  BT_OV_INT_UV4SI_UV4SI)
+OB_DEF_VAR (s390_vec_test_mask_s64,     s390_vtm,           0,                  BT_OV_INT_V2DI_UV2DI)
+OB_DEF_VAR (s390_vec_test_mask_u64,     s390_vtm,           0,                  BT_OV_INT_UV2DI_UV2DI)
+OB_DEF_VAR (s390_vec_test_mask_dbl,     s390_vtm,           0,                  BT_OV_INT_V2DF_UV2DI)
+
+B_DEF      (s390_vtm,                   vec_test_mask_intv16qi,0,               B_VX,               0,                  BT_FN_INT_UV16QI_UV16QI)
+B_DEF      (s390_vfaeb,                 vfaev16qi,          0,                  B_VX,               O3_U4,              BT_FN_UV16QI_UV16QI_UV16QI_INT)
+B_DEF      (s390_vfaeh,                 vfaev8hi,           0,                  B_VX,               O3_U4,              BT_FN_UV8HI_UV8HI_UV8HI_INT)
+B_DEF      (s390_vfaef,                 vfaev4si,           0,                  B_VX,               O3_U4,              BT_FN_UV4SI_UV4SI_UV4SI_INT)
+B_DEF      (s390_vfaezb,                vfaezv16qi,         0,                  B_VX,               O3_U4,              BT_FN_UV16QI_UV16QI_UV16QI_INT)
+B_DEF      (s390_vfaezh,                vfaezv8hi,          0,                  B_VX,               O3_U4,              BT_FN_UV8HI_UV8HI_UV8HI_INT)
+B_DEF      (s390_vfaezf,                vfaezv4si,          0,                  B_VX,               O3_U4,              BT_FN_UV4SI_UV4SI_UV4SI_INT)
+B_DEF      (s390_vfaebs,                vfaesv16qi,         0,                  B_VX,               O3_U4,              BT_FN_UV16QI_UV16QI_UV16QI_INT_INTPTR)
+B_DEF      (s390_vfaehs,                vfaesv8hi,          0,                  B_VX,               O3_U4,              BT_FN_UV8HI_UV8HI_UV8HI_INT_INTPTR)
+B_DEF      (s390_vfaefs,                vfaesv4si,          0,                  B_VX,               O3_U4,              BT_FN_UV4SI_UV4SI_UV4SI_INT_INTPTR)
+B_DEF      (s390_vfaezbs,               vfaezsv16qi,        0,                  B_VX,               O3_U4,              BT_FN_UV16QI_UV16QI_UV16QI_INT_INTPTR)
+B_DEF      (s390_vfaezhs,               vfaezsv8hi,         0,                  B_VX,               O3_U4,              BT_FN_UV8HI_UV8HI_UV8HI_INT_INTPTR)
+B_DEF      (s390_vfaezfs,               vfaezsv4si,         0,                  B_VX,               O3_U4,              BT_FN_UV4SI_UV4SI_UV4SI_INT_INTPTR)
+
+OB_DEF     (s390_vec_find_any_eq_idx,   s390_vfaeb_idx_s8,  s390_vfaef_idx_u32b,B_VX,               BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfaeb_idx_s8,          s390_vfaeb,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfaeb_idx_u8a,         s390_vfaeb,         0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfaeb_idx_u8b,         s390_vfaeb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfaeh_idx_s16,         s390_vfaeh,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfaeh_idx_u16a,        s390_vfaeh,         0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfaeh_idx_u16b,        s390_vfaeh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfaef_idx_s32,         s390_vfaef,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfaef_idx_u32a,        s390_vfaef,         0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfaef_idx_u32b,        s390_vfaef,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_find_any_ne_idx,   s390_vfaeb_inv_idx_s8,s390_vfaef_inv_idx_u32b,B_VX,         BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfaeb_inv_idx_s8,      s390_vfaeb,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfaeb_inv_idx_u8a,     s390_vfaeb,         0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfaeb_inv_idx_u8b,     s390_vfaeb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfaeh_inv_idx_s16,     s390_vfaeh,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfaeh_inv_idx_u16a,    s390_vfaeh,         0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfaeh_inv_idx_u16b,    s390_vfaeh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfaef_inv_idx_s32,     s390_vfaef,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfaef_inv_idx_u32a,    s390_vfaef,         0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfaef_inv_idx_u32b,    s390_vfaef,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_find_any_eq_or_0_idx,s390_vfaezb_idx_s8,s390_vfaezf_idx_u32b,B_VX,             BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfaezb_idx_s8,         s390_vfaezb,        0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfaezb_idx_u8a,        s390_vfaezb,        0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfaezb_idx_u8b,        s390_vfaezb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfaezh_idx_s16,        s390_vfaezh,        0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfaezh_idx_u16a,       s390_vfaezh,        0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfaezh_idx_u16b,       s390_vfaezh,        0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfaezf_idx_s32,        s390_vfaezf,        0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfaezf_idx_u32a,       s390_vfaezf,        0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfaezf_idx_u32b,       s390_vfaezf,        0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_find_any_ne_or_0_idx,s390_vfaezb_inv_idx_s8,s390_vfaezf_inv_idx_u32b,B_VX,     BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfaezb_inv_idx_s8,     s390_vfaezb,        0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfaezb_inv_idx_u8a,    s390_vfaezb,        0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfaezb_inv_idx_u8b,    s390_vfaezb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfaezh_inv_idx_s16,    s390_vfaezh,        0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfaezh_inv_idx_u16a,   s390_vfaezh,        0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfaezh_inv_idx_u16b,   s390_vfaezh,        0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfaezf_inv_idx_s32,    s390_vfaezf,        0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfaezf_inv_idx_u32a,   s390_vfaezf,        0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfaezf_inv_idx_u32b,   s390_vfaezf,        0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_find_any_eq,       s390_vfaeb_s8,      s390_vfaef_b32,     B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfaeb_s8,              s390_vfaeb,         0,                  BT_OV_BV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfaeb_u8,              s390_vfaeb,         0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfaeb_b8,              s390_vfaeb,         0,                  BT_OV_BV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfaeh_s16,             s390_vfaeh,         0,                  BT_OV_BV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfaeh_u16,             s390_vfaeh,         0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfaeh_b16,             s390_vfaeh,         0,                  BT_OV_BV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfaef_s32,             s390_vfaef,         0,                  BT_OV_BV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfaef_u32,             s390_vfaef,         0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfaef_b32,             s390_vfaef,         0,                  BT_OV_BV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_find_any_ne,       s390_vfaeb_inv_s8,  s390_vfaef_inv_b32, B_VX,               BT_FN_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfaeb_inv_s8,          s390_vfaeb,         0,                  BT_OV_BV16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfaeb_inv_u8,          s390_vfaeb,         0,                  BT_OV_BV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfaeb_inv_b8,          s390_vfaeb,         0,                  BT_OV_BV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfaeh_inv_s16,         s390_vfaeh,         0,                  BT_OV_BV8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfaeh_inv_u16,         s390_vfaeh,         0,                  BT_OV_BV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfaeh_inv_b16,         s390_vfaeh,         0,                  BT_OV_BV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfaef_inv_s32,         s390_vfaef,         0,                  BT_OV_BV4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfaef_inv_u32,         s390_vfaef,         0,                  BT_OV_BV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfaef_inv_b32,         s390_vfaef,         0,                  BT_OV_BV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_find_any_eq_idx_cc,s390_vfaebs_idx_s8, s390_vfaefs_idx_u32b,B_VX,              BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_idx_s8,         s390_vfaebs,        0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_idx_u8a,        s390_vfaebs,        0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_idx_u8b,        s390_vfaebs,        0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_idx_s16,        s390_vfaehs,        0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_idx_u16a,       s390_vfaehs,        0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_idx_u16b,       s390_vfaehs,        0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_idx_s32,        s390_vfaefs,        0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_idx_u32a,       s390_vfaefs,        0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_idx_u32b,       s390_vfaefs,        0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_find_any_ne_idx_cc,s390_vfaebs_inv_idx_s8,s390_vfaefs_inv_idx_u32b,B_VX,       BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_inv_idx_s8,     s390_vfaebs,        0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_inv_idx_u8a,    s390_vfaebs,        0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_inv_idx_u8b,    s390_vfaebs,        0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_inv_idx_s16,    s390_vfaehs,        0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_inv_idx_u16a,   s390_vfaehs,        0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_inv_idx_u16b,   s390_vfaehs,        0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_inv_idx_s32,    s390_vfaefs,        0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_inv_idx_u32a,   s390_vfaefs,        0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_inv_idx_u32b,   s390_vfaefs,        0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_find_any_eq_or_0_idx_cc,s390_vfaezbs_idx_s8,s390_vfaezfs_idx_u32b,B_VX,        BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaezbs_idx_s8,        s390_vfaezbs,       0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfaezbs_idx_u8a,       s390_vfaezbs,       0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaezbs_idx_u8b,       s390_vfaezbs,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaezhs_idx_s16,       s390_vfaezhs,       0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfaezhs_idx_u16a,      s390_vfaezhs,       0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaezhs_idx_u16b,      s390_vfaezhs,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaezfs_idx_s32,       s390_vfaezfs,       0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfaezfs_idx_u32a,      s390_vfaezfs,       0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaezfs_idx_u32b,      s390_vfaezfs,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_find_any_ne_or_0_idx_cc,s390_vfaezbs_inv_idx_s8,s390_vfaezfs_inv_idx_u32b,B_VX,BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaezbs_inv_idx_s8,    s390_vfaezbs,       0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfaezbs_inv_idx_u8a,   s390_vfaezbs,       0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaezbs_inv_idx_u8b,   s390_vfaezbs,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaezhs_inv_idx_s16,   s390_vfaezhs,       0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfaezhs_inv_idx_u16a,  s390_vfaezhs,       0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaezhs_inv_idx_u16b,  s390_vfaezhs,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaezfs_inv_idx_s32,   s390_vfaezfs,       0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfaezfs_inv_idx_u32a,  s390_vfaezfs,       0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaezfs_inv_idx_u32b,  s390_vfaezfs,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_find_any_eq_cc,    s390_vfaebs_s8,     s390_vfaefs_b32,    B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_s8,             s390_vfaebs,        0,                  BT_OV_BV16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_u8,             s390_vfaebs,        0,                  BT_OV_BV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_b8,             s390_vfaebs,        0,                  BT_OV_BV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_s16,            s390_vfaehs,        0,                  BT_OV_BV8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_u16,            s390_vfaehs,        0,                  BT_OV_BV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_b16,            s390_vfaehs,        0,                  BT_OV_BV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_s32,            s390_vfaefs,        0,                  BT_OV_BV4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_u32,            s390_vfaefs,        0,                  BT_OV_BV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_b32,            s390_vfaefs,        0,                  BT_OV_BV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_find_any_ne_cc,    s390_vfaebs_inv_s8, s390_vfaefs_inv_b32,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_inv_s8,         s390_vfaebs,        0,                  BT_OV_BV16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_inv_u8,         s390_vfaebs,        0,                  BT_OV_BV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaebs_inv_b8,         s390_vfaebs,        0,                  BT_OV_BV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_inv_s16,        s390_vfaehs,        0,                  BT_OV_BV8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_inv_u16,        s390_vfaehs,        0,                  BT_OV_BV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaehs_inv_b16,        s390_vfaehs,        0,                  BT_OV_BV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_inv_s32,        s390_vfaefs,        0,                  BT_OV_BV4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_inv_u32,        s390_vfaefs,        0,                  BT_OV_BV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfaefs_inv_b32,        s390_vfaefs,        0,                  BT_OV_BV4SI_UV4SI_UV4SI_INTPTR)
+
+B_DEF      (s390_vfeeb,                 vfeev16qi,          0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vfeeh,                 vfeev8hi,           0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vfeef,                 vfeev4si,           0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vfeezb,                vfeezv16qi,         0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vfeezh,                vfeezv8hi,          0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vfeezf,                vfeezv4si,          0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vfeebs,                vfeesv16qi,         0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_INTPTR)
+B_DEF      (s390_vfeehs,                vfeesv8hi,          0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vfeefs,                vfeesv4si,          0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI_INTPTR)
+B_DEF      (s390_vfeezbs,               vfeezsv16qi,        0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_INTPTR)
+B_DEF      (s390_vfeezhs,               vfeezsv8hi,         0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vfeezfs,               vfeezsv4si,         0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmpeq_idx,         s390_vfeeb_s8,      s390_vfeef_u32b,    B_VX,               BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfeeb_s8,              s390_vfeeb,         0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfeeb_u8a,             s390_vfeeb,         0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfeeb_u8b,             s390_vfeeb,         0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfeeh_s16,             s390_vfeeh,         0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfeeh_u16a,            s390_vfeeh,         0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfeeh_u16b,            s390_vfeeh,         0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfeef_s32,             s390_vfeef,         0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfeef_u32a,            s390_vfeef,         0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfeef_u32b,            s390_vfeef,         0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmpeq_or_0_idx,    s390_vfeezb_s8,     s390_vfeezf_u32b,   B_VX,               BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfeezb_s8,             s390_vfeezb,        0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfeezb_u8a,            s390_vfeezb,        0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfeezb_u8b,            s390_vfeezb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfeezh_s16,            s390_vfeezh,        0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfeezh_u16a,           s390_vfeezh,        0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfeezh_u16b,           s390_vfeezh,        0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfeezf_s32,            s390_vfeezf,        0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfeezf_u32a,           s390_vfeezf,        0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfeezf_u32b,           s390_vfeezf,        0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmpeq_idx_cc,      s390_vfeebs_s8,     s390_vfeefs_u32b,   B_VX,               BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfeebs_s8,             s390_vfeebs,        0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfeebs_u8a,            s390_vfeebs,        0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfeebs_u8b,            s390_vfeebs,        0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfeehs_s16,            s390_vfeehs,        0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfeehs_u16a,           s390_vfeehs,        0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfeehs_u16b,           s390_vfeehs,        0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfeefs_s32,            s390_vfeefs,        0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfeefs_u32a,           s390_vfeefs,        0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfeefs_u32b,           s390_vfeefs,        0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmpeq_or_0_idx_cc, s390_vfeezbs_s8,    s390_vfeezfs_u32b,  B_VX,               BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfeezbs_s8,            s390_vfeezbs,       0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfeezbs_u8a,           s390_vfeezbs,       0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfeezbs_u8b,           s390_vfeezbs,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfeezhs_s16,           s390_vfeezhs,       0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfeezhs_u16a,          s390_vfeezhs,       0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfeezhs_u16b,          s390_vfeezhs,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfeezfs_s32,           s390_vfeezfs,       0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfeezfs_u32a,          s390_vfeezfs,       0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfeezfs_u32b,          s390_vfeezfs,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+B_DEF      (s390_vfeneb,                vfenev16qi,         0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vfeneh,                vfenev8hi,          0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vfenef,                vfenev4si,          0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vfenezb,               vfenezv16qi,        0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI)
+B_DEF      (s390_vfenezh,               vfenezv8hi,         0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI)
+B_DEF      (s390_vfenezf,               vfenezv4si,         0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI)
+B_DEF      (s390_vfenebs,               vfenesv16qi,        0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_INTPTR)
+B_DEF      (s390_vfenehs,               vfenesv8hi,         0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vfenefs,               vfenesv4si,         0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI_INTPTR)
+B_DEF      (s390_vfenezbs,              vfenezsv16qi,       0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_UV16QI_INTPTR)
+B_DEF      (s390_vfenezhs,              vfenezsv8hi,        0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vfenezfs,              vfenezsv4si,        0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmpne_idx,         s390_vfeneb_s8,     s390_vfenef_u32b,   B_VX,               BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfeneb_s8,             s390_vfeneb,        0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfeneb_u8a,            s390_vfeneb,        0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfeneb_u8b,            s390_vfeneb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfeneh_s16,            s390_vfeneh,        0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfeneh_u16a,           s390_vfeneh,        0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfeneh_u16b,           s390_vfeneh,        0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfenef_s32,            s390_vfenef,        0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfenef_u32a,           s390_vfenef,        0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfenef_u32b,           s390_vfenef,        0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmpne_or_0_idx,    s390_vfenezb_s8,    s390_vfenezf_u32b,  B_VX,               BT_FN_INT_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vfenezb_s8,            s390_vfenezb,       0,                  BT_OV_V16QI_V16QI_V16QI)
+OB_DEF_VAR (s390_vfenezb_u8a,           s390_vfenezb,       0,                  BT_OV_UV16QI_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vfenezb_u8b,           s390_vfenezb,       0,                  BT_OV_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vfenezh_s16,           s390_vfenezh,       0,                  BT_OV_V8HI_V8HI_V8HI)
+OB_DEF_VAR (s390_vfenezh_u16a,          s390_vfenezh,       0,                  BT_OV_UV8HI_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vfenezh_u16b,          s390_vfenezh,       0,                  BT_OV_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vfenezf_s32,           s390_vfenezf,       0,                  BT_OV_V4SI_V4SI_V4SI)
+OB_DEF_VAR (s390_vfenezf_u32a,          s390_vfenezf,       0,                  BT_OV_UV4SI_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vfenezf_u32b,          s390_vfenezf,       0,                  BT_OV_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmpne_idx_cc,      s390_vfenebs_s8,    s390_vfenefs_u32b,  B_VX,               BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfenebs_s8,            s390_vfenebs,       0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfenebs_u8a,           s390_vfenebs,       0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfenebs_u8b,           s390_vfenebs,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfenehs_s16,           s390_vfenehs,       0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfenehs_u16a,          s390_vfenehs,       0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfenehs_u16b,          s390_vfenehs,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfenefs_s32,           s390_vfenefs,       0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfenefs_u32a,          s390_vfenefs,       0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfenefs_u32b,          s390_vfenefs,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmpne_or_0_idx_cc, s390_vfenezbs_s8,   s390_vfenezfs_u32b, B_VX,               BT_FN_INT_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vfenezbs_s8,           s390_vfenezbs,      0,                  BT_OV_V16QI_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vfenezbs_u8a,          s390_vfenezbs,      0,                  BT_OV_UV16QI_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vfenezbs_u8b,          s390_vfenezbs,      0,                  BT_OV_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vfenezhs_s16,          s390_vfenezhs,      0,                  BT_OV_V8HI_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vfenezhs_u16a,         s390_vfenezhs,      0,                  BT_OV_UV8HI_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vfenezhs_u16b,         s390_vfenezhs,      0,                  BT_OV_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vfenezfs_s32,          s390_vfenezfs,      0,                  BT_OV_V4SI_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vfenezfs_u32a,         s390_vfenezfs,      0,                  BT_OV_UV4SI_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vfenezfs_u32b,         s390_vfenezfs,      0,                  BT_OV_UV4SI_UV4SI_UV4SI_INTPTR)
+
+B_DEF      (s390_vistrb,                vistrv16qi,         0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI)
+B_DEF      (s390_vistrh,                vistrv8hi,          0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI)
+B_DEF      (s390_vistrf,                vistrv4si,          0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI)
+B_DEF      (s390_vistrbs,               vistrsv16qi,        0,                  B_VX,               0,                  BT_FN_UV16QI_UV16QI_INTPTR)
+B_DEF      (s390_vistrhs,               vistrsv8hi,         0,                  B_VX,               0,                  BT_FN_UV8HI_UV8HI_INTPTR)
+B_DEF      (s390_vistrfs,               vistrsv4si,         0,                  B_VX,               0,                  BT_FN_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cp_until_zero,     s390_vistrb_s8,     s390_vistrf_u32,    B_VX,               BT_FN_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vistrb_s8,             s390_vistrb,        0,                  BT_OV_V16QI_V16QI)
+OB_DEF_VAR (s390_vistrb_b8,             s390_vistrb,        0,                  BT_OV_BV16QI_BV16QI)
+OB_DEF_VAR (s390_vistrb_u8,             s390_vistrb,        0,                  BT_OV_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vistrh_s16,            s390_vistrh,        0,                  BT_OV_V8HI_V8HI)
+OB_DEF_VAR (s390_vistrh_b16,            s390_vistrh,        0,                  BT_OV_BV8HI_BV8HI)
+OB_DEF_VAR (s390_vistrh_u16,            s390_vistrh,        0,                  BT_OV_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vistrf_s32,            s390_vistrf,        0,                  BT_OV_V4SI_V4SI)
+OB_DEF_VAR (s390_vistrf_b32,            s390_vistrf,        0,                  BT_OV_BV4SI_BV4SI)
+OB_DEF_VAR (s390_vistrf_u32,            s390_vistrf,        0,                  BT_OV_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cp_until_zero_cc,  s390_vistrbs_s8,    s390_vistrfs_u32,   B_VX,               BT_FN_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vistrbs_s8,            s390_vistrbs,       0,                  BT_OV_V16QI_V16QI_INTPTR)
+OB_DEF_VAR (s390_vistrbs_b8,            s390_vistrbs,       0,                  BT_OV_BV16QI_BV16QI_INTPTR)
+OB_DEF_VAR (s390_vistrbs_u8,            s390_vistrbs,       0,                  BT_OV_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vistrhs_s16,           s390_vistrhs,       0,                  BT_OV_V8HI_V8HI_INTPTR)
+OB_DEF_VAR (s390_vistrhs_b16,           s390_vistrhs,       0,                  BT_OV_BV8HI_BV8HI_INTPTR)
+OB_DEF_VAR (s390_vistrhs_u16,           s390_vistrhs,       0,                  BT_OV_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vistrfs_s32,           s390_vistrfs,       0,                  BT_OV_V4SI_V4SI_INTPTR)
+OB_DEF_VAR (s390_vistrfs_b32,           s390_vistrfs,       0,                  BT_OV_BV4SI_BV4SI_INTPTR)
+OB_DEF_VAR (s390_vistrfs_u32,           s390_vistrfs,       0,                  BT_OV_UV4SI_UV4SI_INTPTR)
+
+B_DEF      (s390_vstrcb,                vstrcv16qi,         0,                  B_VX,               O4_U4,              BT_FN_UV16QI_UV16QI_UV16QI_UV16QI_INT)
+B_DEF      (s390_vstrch,                vstrcv8hi,          0,                  B_VX,               O4_U4,              BT_FN_UV8HI_UV8HI_UV8HI_UV8HI_INT)
+B_DEF      (s390_vstrcf,                vstrcv4si,          0,                  B_VX,               O4_U4,              BT_FN_UV4SI_UV4SI_UV4SI_UV4SI_INT)
+B_DEF      (s390_vstrczb,               vstrczv16qi,        0,                  B_VX,               O4_U4,              BT_FN_UV16QI_UV16QI_UV16QI_UV16QI_INT)
+B_DEF      (s390_vstrczh,               vstrczv8hi,         0,                  B_VX,               O4_U4,              BT_FN_UV8HI_UV8HI_UV8HI_UV8HI_INT)
+B_DEF      (s390_vstrczf,               vstrczv4si,         0,                  B_VX,               O4_U4,              BT_FN_UV4SI_UV4SI_UV4SI_UV4SI_INT)
+B_DEF      (s390_vstrcbs,               vstrcsv16qi,        0,                  B_VX,               O4_U4,              BT_FN_UV16QI_UV16QI_UV16QI_UV16QI_INT_INTPTR)
+B_DEF      (s390_vstrchs,               vstrcsv8hi,         0,                  B_VX,               O4_U4,              BT_FN_UV8HI_UV8HI_UV8HI_UV8HI_INT_INTPTR)
+B_DEF      (s390_vstrcfs,               vstrcsv4si,         0,                  B_VX,               O4_U4,              BT_FN_UV4SI_UV4SI_UV4SI_UV4SI_INT_INTPTR)
+B_DEF      (s390_vstrczbs,              vstrczsv16qi,       0,                  B_VX,               O4_U4,              BT_FN_UV16QI_UV16QI_UV16QI_UV16QI_INT_INTPTR)
+B_DEF      (s390_vstrczhs,              vstrczsv8hi,        0,                  B_VX,               O4_U4,              BT_FN_UV8HI_UV8HI_UV8HI_UV8HI_INT_INTPTR)
+B_DEF      (s390_vstrczfs,              vstrczsv4si,        0,                  B_VX,               O4_U4,              BT_FN_UV4SI_UV4SI_UV4SI_UV4SI_INT_INTPTR)
+
+OB_DEF     (s390_vec_cmprg_idx,         s390_vstrcb_idx_u8, s390_vstrcf_idx_u32,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vstrcb_idx_u8,         s390_vstrcb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vstrch_idx_u16,        s390_vstrch,        0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vstrcf_idx_u32,        s390_vstrcf,        0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmpnrg_idx,        s390_vstrcb_inv_idx_u8,s390_vstrcf_inv_idx_u32,B_VX,        BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vstrcb_inv_idx_u8,     s390_vstrcb,        0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vstrch_inv_idx_u16,    s390_vstrch,        0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vstrcf_inv_idx_u32,    s390_vstrcf,        0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmprg_or_0_idx,    s390_vstrczb_idx_u8,s390_vstrczf_idx_u32,B_VX,              BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vstrczb_idx_u8,        s390_vstrczb,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vstrczh_idx_u16,       s390_vstrczh,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vstrczf_idx_u32,       s390_vstrczf,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmpnrg_or_0_idx,   s390_vstrczb_inv_idx_u8,s390_vstrczf_inv_idx_u32,B_VX,      BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vstrczb_inv_idx_u8,    s390_vstrczb,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vstrczh_inv_idx_u16,   s390_vstrczh,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vstrczf_inv_idx_u32,   s390_vstrczf,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmprg,             s390_vstrcb_u8,     s390_vstrcf_u32,    B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vstrcb_u8,             s390_vstrcb,        0,                  BT_OV_BV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vstrch_u16,            s390_vstrch,        0,                  BT_OV_BV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vstrcf_u32,            s390_vstrcf,        0,                  BT_OV_BV4SI_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmpnrg,            s390_vstrcb_inv_u8, s390_vstrcf_inv_u32,B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI)
+OB_DEF_VAR (s390_vstrcb_inv_u8,         s390_vstrcb,        0,                  BT_OV_BV16QI_UV16QI_UV16QI_UV16QI)
+OB_DEF_VAR (s390_vstrch_inv_u16,        s390_vstrch,        0,                  BT_OV_BV8HI_UV8HI_UV8HI_UV8HI)
+OB_DEF_VAR (s390_vstrcf_inv_u32,        s390_vstrcf,        0,                  BT_OV_BV4SI_UV4SI_UV4SI_UV4SI)
+
+OB_DEF     (s390_vec_cmprg_idx_cc,      s390_vstrcbs_idx_u8,s390_vstrcfs_idx_u32,B_VX,              BT_FN_OV4SI_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vstrcbs_idx_u8,        s390_vstrcbs,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vstrchs_idx_u16,       s390_vstrchs,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vstrcfs_idx_u32,       s390_vstrcfs,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmpnrg_idx_cc,     s390_vstrcbs_inv_idx_u8,s390_vstrcfs_inv_idx_u32,B_VX,      BT_FN_OV4SI_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vstrcbs_inv_idx_u8,    s390_vstrcbs,       0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI_INTPTR) /* vstrcb */
+OB_DEF_VAR (s390_vstrchs_inv_idx_u16,   s390_vstrchs,       0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI_INTPTR)    /* vstrch */
+OB_DEF_VAR (s390_vstrcfs_inv_idx_u32,   s390_vstrcfs,       0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI_INTPTR)    /* vstrcf */
+
+OB_DEF     (s390_vec_cmprg_or_0_idx_cc, s390_vstrczbs_idx_u8,s390_vstrczfs_idx_u32,B_VX,            BT_FN_OV4SI_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vstrczbs_idx_u8,       s390_vstrczbs,      0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vstrczhs_idx_u16,      s390_vstrczhs,      0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vstrczfs_idx_u32,      s390_vstrczfs,      0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmpnrg_or_0_idx_cc,s390_vstrczbs_inv_idx_u8,s390_vstrczfs_inv_idx_u32,B_VX,    BT_FN_OV4SI_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vstrczbs_inv_idx_u8,   s390_vstrczbs,      0,                  BT_OV_UV16QI_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vstrczhs_inv_idx_u16,  s390_vstrczhs,      0,                  BT_OV_UV8HI_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vstrczfs_inv_idx_u32,  s390_vstrczfs,      0,                  BT_OV_UV4SI_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmprg_cc,          s390_vstrcbs_u8,    s390_vstrcfs_u32,   B_VX,               BT_FN_OV4SI_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vstrcbs_u8,            s390_vstrcbs,       0,                  BT_OV_BV16QI_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vstrchs_u16,           s390_vstrchs,       0,                  BT_OV_BV8HI_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vstrcfs_u32,           s390_vstrcfs,       0,                  BT_OV_BV4SI_UV4SI_UV4SI_UV4SI_INTPTR)
+
+OB_DEF     (s390_vec_cmpnrg_cc,         s390_vstrcbs_inv_u8,s390_vstrcfs_inv_u32,B_VX,              BT_FN_OV4SI_OV4SI_OV4SI_OV4SI_INTPTR)
+OB_DEF_VAR (s390_vstrcbs_inv_u8,        s390_vstrcbs,       0,                  BT_OV_BV16QI_UV16QI_UV16QI_UV16QI_INTPTR)
+OB_DEF_VAR (s390_vstrchs_inv_u16,       s390_vstrchs,       0,                  BT_OV_BV8HI_UV8HI_UV8HI_UV8HI_INTPTR)
+OB_DEF_VAR (s390_vstrcfs_inv_u32,       s390_vstrcfs,       0,                  BT_OV_BV4SI_UV4SI_UV4SI_UV4SI_INTPTR)
+
+B_DEF      (s390_vec_all_nge,           vec_all_unltv2df,   0,                  B_VX,               0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (s390_vec_all_ngt,           vec_all_unlev2df,   0,                  B_VX,               0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (s390_vec_any_nge,           vec_any_unltv2df,   0,                  B_VX,               0,                  BT_FN_INT_V2DF_V2DF)
+B_DEF      (s390_vec_any_ngt,           vec_any_unlev2df,   0,                  B_VX,               0,                  BT_FN_INT_V2DF_V2DF)
+
+OB_DEF     (s390_vec_ctd,               s390_vec_ctd_s64,   s390_vec_ctd_u64,   B_VX,               BT_FN_V2DF_UV4SI_INT)
+OB_DEF_VAR (s390_vec_ctd_s64,           s390_vec_ctd_s64,   O2_U5,              BT_OV_V2DF_V2DI_INT)                     /* vcdgb */
+OB_DEF_VAR (s390_vec_ctd_u64,           s390_vec_ctd_u64,   O2_U5,              BT_OV_V2DF_UV2DI_INT)                    /* vcdlgb */
+
+B_DEF      (s390_vec_ctd_s64,           vec_ctd_s64,        0,                  B_VX,               O2_U3,              BT_FN_V2DF_V2DI_INT)                     /* vcdgb */
+B_DEF      (s390_vec_ctd_u64,           vec_ctd_u64,        0,                  B_VX,               O2_U3,              BT_FN_V2DF_UV2DI_INT)                    /* vcdlgb */
+B_DEF      (s390_vcdgb,                 vec_di_to_df_s64,   0,                  B_VX,               O2_U3,              BT_FN_V2DF_V2DI_INT)                     /* vcdgb */
+B_DEF      (s390_vcdlgb,                vec_di_to_df_u64,   0,                  B_VX,               O2_U3,              BT_FN_V2DF_UV2DI_INT)                    /* vcdlgb */
+B_DEF      (s390_vec_ctsl,              vec_ctsl,           0,                  B_VX,               O2_U3,              BT_FN_V2DI_V2DF_INT)                     /* vcgdb */
+B_DEF      (s390_vec_ctul,              vec_ctul,           0,                  B_VX,               O2_U3,              BT_FN_UV2DI_V2DF_INT)                    /* vclgdb */
+B_DEF      (s390_vcgdb,                 vec_df_to_di_s64,   0,                  B_VX,               O2_U3,              BT_FN_V2DI_V2DF_INT)                     /* vcgdb */
+B_DEF      (s390_vclgdb,                vec_df_to_di_u64,   0,                  B_VX,               O2_U3,              BT_FN_UV2DI_V2DF_INT)                    /* vclgdb */
+B_DEF      (s390_vfidb,                 vfidb,              0,                  B_VX,               O2_U4 | O3_U3,      BT_FN_V2DF_V2DF_UCHAR_UCHAR)
+B_DEF      (s390_vec_ld2f,              vec_ld2f,           0,                  B_VX,               0,                  BT_FN_V2DF_FLTCONSTPTR)                  /* vldeb */
+B_DEF      (s390_vec_st2f,              vec_st2f,           0,                  B_VX,               0,                  BT_FN_VOID_V2DF_FLTPTR)                  /* vledb */
+B_DEF      (s390_vfmadb,                fmav2df4,           0,                  B_VX,               0,                  BT_FN_V2DF_V2DF_V2DF_V2DF)
+B_DEF      (s390_vfmsdb,                fmsv2df4,           0,                  B_VX,               0,                  BT_FN_V2DF_V2DF_V2DF_V2DF)
+B_DEF      (s390_vflndb,                vec_nabs,           0,                  B_VX,               0,                  BT_FN_V2DF_V2DF)
+B_DEF      (s390_vfsqdb,                sqrtv2df2,          0,                  B_VX,               0,                  BT_FN_V2DF_V2DF)
+B_DEF      (s390_vftcidb,               vftcidb,            0,                  B_VX,               O2_U12,             BT_FN_V2DI_V2DF_INT_INTPTR)
--- gcc/config/s390/s390-builtins.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/s390-builtins.h	2016-05-11 17:33:27.000000000 +0200
@@ -0,0 +1,175 @@
+/* Common data structures used for builtin handling on S/390.
+   Copyright (C) 2015 Free Software Foundation, Inc.
+
+   Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com).
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+/* This files contains data structure definitions which can be used by
+   s390-builtins.c as well as s390-c.c.  Since the latter is
+   considered to be part of the front-end we have to be careful not
+   to use any of tree and rtx like data structures.  */
+
+/* Builtin types, data and prototypes. */
+
+enum s390_builtin_type_index
+{
+#undef DEF_TYPE
+#undef DEF_POINTER_TYPE
+#undef DEF_DISTINCT_TYPE
+#undef DEF_VECTOR_TYPE
+#undef DEF_OPAQUE_VECTOR_TYPE
+#undef DEF_FN_TYPE
+#undef DEF_OV_TYPE
+#define DEF_TYPE(INDEX, ...) INDEX,
+#define DEF_POINTER_TYPE(INDEX, ...) INDEX,
+#define DEF_DISTINCT_TYPE(INDEX, ...) INDEX,
+#define DEF_VECTOR_TYPE(INDEX, ...) INDEX,
+#define DEF_OPAQUE_VECTOR_TYPE(INDEX, ...) INDEX,
+#define DEF_FN_TYPE(...)
+#define DEF_OV_TYPE(...)
+#include "s390-builtin-types.def"
+  BT_MAX
+};
+
+enum s390_builtin_fn_type_index
+{
+#undef DEF_TYPE
+#undef DEF_POINTER_TYPE
+#undef DEF_DISTINCT_TYPE
+#undef DEF_VECTOR_TYPE
+#undef DEF_OPAQUE_VECTOR_TYPE
+#undef DEF_FN_TYPE
+#undef DEF_OV_TYPE
+#define DEF_TYPE(...)
+#define DEF_POINTER_TYPE(...)
+#define DEF_DISTINCT_TYPE(...)
+#define DEF_VECTOR_TYPE(...)
+#define DEF_OPAQUE_VECTOR_TYPE(...)
+#define DEF_FN_TYPE(INDEX, ...) INDEX,
+#define DEF_OV_TYPE(...)
+#include "s390-builtin-types.def"
+  BT_FN_MAX
+};
+
+enum s390_builtin_ov_type_index
+{
+#undef DEF_TYPE
+#undef DEF_POINTER_TYPE
+#undef DEF_DISTINCT_TYPE
+#undef DEF_VECTOR_TYPE
+#undef DEF_OPAQUE_VECTOR_TYPE
+#undef DEF_FN_TYPE
+#undef DEF_OV_TYPE
+#define DEF_TYPE(...)
+#define DEF_POINTER_TYPE(...)
+#define DEF_DISTINCT_TYPE(...)
+#define DEF_VECTOR_TYPE(...)
+#define DEF_OPAQUE_VECTOR_TYPE(...)
+#define DEF_FN_TYPE(...)
+#define DEF_OV_TYPE(INDEX, ...) INDEX,
+#include "s390-builtin-types.def"
+  BT_OV_MAX
+};
+
+#define MAX_OV_OPERANDS 6
+
+extern tree s390_builtin_types[BT_MAX];
+extern tree s390_builtin_fn_types[BT_FN_MAX];
+
+  /* Builtins.  */
+
+enum s390_builtins {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(NAME, ...) S390_BUILTIN_##NAME,
+#define OB_DEF(...)
+#define OB_DEF_VAR(...)
+
+#include "s390-builtins.def"
+  S390_BUILTIN_MAX
+};
+
+
+/* Generate an enumeration of all overloaded builtins defined with
+   OB_DEF in s390-builtins.def.  */
+enum s390_overloaded_builtins {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(NAME, ...) S390_OVERLOADED_BUILTIN_##NAME,
+#define OB_DEF_VAR(...)
+#include "s390-builtins.def"
+S390_OVERLOADED_BUILTIN_MAX
+};
+
+/* Generate an enumeration of all variants of overloaded builtins
+   defined with OB_DEF_VAR in s390-builtins.def.  */
+enum s390_overloaded_builtin_vars {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(...)
+#define OB_DEF_VAR(NAME, ...) S390_OVERLOADED_BUILTIN_VAR_##NAME,
+#include "s390-builtins.def"
+S390_OVERLOADED_BUILTIN_VAR_MAX
+};
+
+#define S390_OVERLOADED_BUILTIN_OFFSET S390_BUILTIN_MAX
+#define S390_OVERLOADED_BUILTIN_VAR_OFFSET \
+  (S390_BUILTIN_MAX + S390_OVERLOADED_BUILTIN_MAX)
+#define S390_ALL_BUILTIN_MAX				\
+  (S390_BUILTIN_MAX + S390_OVERLOADED_BUILTIN_MAX +	\
+   S390_OVERLOADED_BUILTIN_VAR_MAX)
+
+extern const unsigned int bflags_builtin[S390_BUILTIN_MAX + 1];
+extern const unsigned int opflags_builtin[S390_BUILTIN_MAX + 1];
+
+extern const unsigned int
+  bflags_overloaded_builtin[S390_OVERLOADED_BUILTIN_MAX + 1];
+extern const unsigned int
+  opflags_overloaded_builtin_var[S390_OVERLOADED_BUILTIN_VAR_MAX + 1];
+
+static inline unsigned int
+bflags_for_builtin (int fcode)
+{
+  if (fcode >= S390_OVERLOADED_BUILTIN_VAR_OFFSET)
+    gcc_unreachable ();
+  else if (fcode >= S390_OVERLOADED_BUILTIN_OFFSET)
+    return bflags_overloaded_builtin[fcode - S390_BUILTIN_MAX];
+  else
+    return bflags_builtin[fcode];
+}
+
+static inline unsigned int
+opflags_for_builtin (int fcode)
+{
+  if (fcode >= S390_OVERLOADED_BUILTIN_VAR_OFFSET)
+    return opflags_overloaded_builtin_var[fcode -
+					  S390_OVERLOADED_BUILTIN_VAR_OFFSET];
+  else if (fcode >= S390_OVERLOADED_BUILTIN_OFFSET)
+    gcc_unreachable ();
+  else
+    return opflags_builtin[fcode];
+}
+
+extern tree s390_builtin_decls[S390_BUILTIN_MAX +
+			       S390_OVERLOADED_BUILTIN_MAX +
+			       S390_OVERLOADED_BUILTIN_VAR_MAX];
--- gcc/config/s390/s390-builtin-types.def	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/s390-builtin-types.def	2016-05-11 17:53:39.000000000 +0200
@@ -0,0 +1,755 @@
+/* Builtin type definitions for IBM S/390 and zSeries
+   Copyright (C) 2015 Free Software Foundation, Inc.
+
+   Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com).
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.  */
+
+#define DEF_FN_TYPE_1(FN_TYPE, FLAGS, T1)	\
+  DEF_FN_TYPE (FN_TYPE,				\
+	       FLAGS,				\
+	       s390_builtin_types[T1])
+#define DEF_FN_TYPE_2(FN_TYPE, FLAGS, T1, T2)	\
+  DEF_FN_TYPE (FN_TYPE,				\
+	       FLAGS,				\
+	       s390_builtin_types[T1],		\
+	       s390_builtin_types[T2])
+#define DEF_FN_TYPE_3(FN_TYPE, FLAGS, T1, T2, T3)	\
+  DEF_FN_TYPE (FN_TYPE,					\
+	       FLAGS,					\
+	       s390_builtin_types[T1],			\
+	       s390_builtin_types[T2],			\
+	       s390_builtin_types[T3])
+#define DEF_FN_TYPE_4(FN_TYPE, FLAGS, T1, T2, T3, T4)	\
+  DEF_FN_TYPE (FN_TYPE,					\
+	       FLAGS,					\
+	       s390_builtin_types[T1],			\
+	       s390_builtin_types[T2],			\
+	       s390_builtin_types[T3],			\
+	       s390_builtin_types[T4])
+#define DEF_FN_TYPE_5(FN_TYPE, FLAGS, T1, T2, T3, T4, T5)	\
+  DEF_FN_TYPE (FN_TYPE,						\
+	       FLAGS,						\
+	       s390_builtin_types[T1],				\
+	       s390_builtin_types[T2],				\
+	       s390_builtin_types[T3],				\
+	       s390_builtin_types[T4],				\
+	       s390_builtin_types[T5])
+#define DEF_FN_TYPE_6(FN_TYPE, FLAGS, T1, T2, T3, T4, T5, T6)	\
+  DEF_FN_TYPE (FN_TYPE,						\
+	       FLAGS,						\
+	       s390_builtin_types[T1],				\
+	       s390_builtin_types[T2],				\
+	       s390_builtin_types[T3],				\
+	       s390_builtin_types[T4],				\
+	       s390_builtin_types[T5],				\
+	       s390_builtin_types[T6])
+DEF_TYPE (BT_INT, B_HTM | B_VX, integer_type_node, 0)
+DEF_TYPE (BT_VOID, 0, void_type_node, 0)
+DEF_TYPE (BT_FLTCONST, B_VX, float_type_node, 1)
+DEF_TYPE (BT_UINT64, B_HTM, c_uint64_type_node, 0)
+DEF_TYPE (BT_FLT, B_VX, float_type_node, 0)
+DEF_TYPE (BT_UINT, 0, unsigned_type_node, 0)
+DEF_TYPE (BT_VOIDCONST, B_VX, void_type_node, 1)
+DEF_TYPE (BT_ULONG, B_VX, long_unsigned_type_node, 0)
+DEF_TYPE (BT_USHORTCONST, B_VX, short_unsigned_type_node, 1)
+DEF_TYPE (BT_SHORTCONST, B_VX, short_integer_type_node, 1)
+DEF_TYPE (BT_INTCONST, B_VX, integer_type_node, 1)
+DEF_TYPE (BT_UCHARCONST, B_VX, unsigned_char_type_node, 1)
+DEF_TYPE (BT_UCHAR, B_VX, unsigned_char_type_node, 0)
+DEF_TYPE (BT_SCHARCONST, B_VX, signed_char_type_node, 1)
+DEF_TYPE (BT_SHORT, B_VX, short_integer_type_node, 0)
+DEF_TYPE (BT_LONG, B_VX, long_integer_type_node, 0)
+DEF_TYPE (BT_SCHAR, B_VX, signed_char_type_node, 0)
+DEF_TYPE (BT_ULONGLONGCONST, B_VX, long_long_unsigned_type_node, 1)
+DEF_TYPE (BT_USHORT, B_VX, short_unsigned_type_node, 0)
+DEF_TYPE (BT_LONGLONG, B_VX, long_long_integer_type_node, 0)
+DEF_TYPE (BT_DBLCONST, B_VX, double_type_node, 1)
+DEF_TYPE (BT_ULONGLONG, B_VX, long_long_unsigned_type_node, 0)
+DEF_TYPE (BT_DBL, B_VX, double_type_node, 0)
+DEF_TYPE (BT_LONGLONGCONST, B_VX, long_long_integer_type_node, 1)
+DEF_TYPE (BT_UINTCONST, B_VX, unsigned_type_node, 1)
+DEF_VECTOR_TYPE (BT_UV2DI, B_VX, BT_ULONGLONG, 2)
+DEF_VECTOR_TYPE (BT_V4SI, B_VX, BT_INT, 4)
+DEF_VECTOR_TYPE (BT_V8HI, B_VX, BT_SHORT, 8)
+DEF_VECTOR_TYPE (BT_UV4SI, B_VX, BT_UINT, 4)
+DEF_VECTOR_TYPE (BT_V16QI, B_VX, BT_SCHAR, 16)
+DEF_VECTOR_TYPE (BT_V2DF, B_VX, BT_DBL, 2)
+DEF_VECTOR_TYPE (BT_V2DI, B_VX, BT_LONGLONG, 2)
+DEF_VECTOR_TYPE (BT_UV8HI, B_VX, BT_USHORT, 8)
+DEF_VECTOR_TYPE (BT_UV16QI, B_VX, BT_UCHAR, 16)
+DEF_POINTER_TYPE (BT_UCHARPTR, B_VX, BT_UCHAR)
+DEF_POINTER_TYPE (BT_DBLCONSTPTR, B_VX, BT_DBLCONST)
+DEF_POINTER_TYPE (BT_VOIDPTR, B_HTM | B_VX, BT_VOID)
+DEF_POINTER_TYPE (BT_FLTPTR, B_VX, BT_FLT)
+DEF_POINTER_TYPE (BT_UINT64PTR, B_HTM, BT_UINT64)
+DEF_POINTER_TYPE (BT_SCHARPTR, B_VX, BT_SCHAR)
+DEF_POINTER_TYPE (BT_UINTCONSTPTR, B_VX, BT_UINTCONST)
+DEF_POINTER_TYPE (BT_ULONGLONGCONSTPTR, B_VX, BT_ULONGLONGCONST)
+DEF_POINTER_TYPE (BT_LONGLONGCONSTPTR, B_VX, BT_LONGLONGCONST)
+DEF_POINTER_TYPE (BT_SHORTPTR, B_VX, BT_SHORT)
+DEF_POINTER_TYPE (BT_USHORTPTR, B_VX, BT_USHORT)
+DEF_POINTER_TYPE (BT_INTPTR, B_VX, BT_INT)
+DEF_POINTER_TYPE (BT_INTCONSTPTR, B_VX, BT_INTCONST)
+DEF_POINTER_TYPE (BT_LONGLONGPTR, B_VX, BT_LONGLONG)
+DEF_POINTER_TYPE (BT_ULONGLONGPTR, B_VX, BT_ULONGLONG)
+DEF_POINTER_TYPE (BT_DBLPTR, B_VX, BT_DBL)
+DEF_POINTER_TYPE (BT_VOIDCONSTPTR, B_VX, BT_VOIDCONST)
+DEF_POINTER_TYPE (BT_USHORTCONSTPTR, B_VX, BT_USHORTCONST)
+DEF_POINTER_TYPE (BT_SHORTCONSTPTR, B_VX, BT_SHORTCONST)
+DEF_POINTER_TYPE (BT_UCHARCONSTPTR, B_VX, BT_UCHARCONST)
+DEF_POINTER_TYPE (BT_FLTCONSTPTR, B_VX, BT_FLTCONST)
+DEF_POINTER_TYPE (BT_SCHARCONSTPTR, B_VX, BT_SCHARCONST)
+DEF_POINTER_TYPE (BT_UINTPTR, B_VX, BT_UINT)
+DEF_DISTINCT_TYPE (BT_BLONGLONG, B_VX, BT_ULONGLONG)
+DEF_DISTINCT_TYPE (BT_BINT, B_VX, BT_UINT)
+DEF_DISTINCT_TYPE (BT_BSHORT, B_VX, BT_USHORT)
+DEF_DISTINCT_TYPE (BT_BCHAR, B_VX, BT_UCHAR)
+DEF_OPAQUE_VECTOR_TYPE (BT_OV2DI, B_VX, BT_LONGLONG, 2)
+DEF_OPAQUE_VECTOR_TYPE (BT_BV16QI, B_VX, BT_BCHAR, 16)
+DEF_OPAQUE_VECTOR_TYPE (BT_OV4SI, B_VX, BT_INT, 4)
+DEF_OPAQUE_VECTOR_TYPE (BT_OUV4SI, B_VX, BT_UINT, 4)
+DEF_OPAQUE_VECTOR_TYPE (BT_BV4SI, B_VX, BT_BINT, 4)
+DEF_OPAQUE_VECTOR_TYPE (BT_BV2DI, B_VX, BT_BLONGLONG, 2)
+DEF_OPAQUE_VECTOR_TYPE (BT_BV8HI, B_VX, BT_BSHORT, 8)
+DEF_FN_TYPE_1 (BT_FN_INT, B_HTM, BT_INT)
+DEF_FN_TYPE_1 (BT_FN_UINT, 0, BT_UINT)
+DEF_FN_TYPE_2 (BT_FN_INT_INT, B_VX, BT_INT, BT_INT)
+DEF_FN_TYPE_2 (BT_FN_INT_VOIDPTR, B_HTM, BT_INT, BT_VOIDPTR)
+DEF_FN_TYPE_2 (BT_FN_OV4SI_INT, B_VX, BT_OV4SI, BT_INT)
+DEF_FN_TYPE_2 (BT_FN_OV4SI_INTCONSTPTR, B_VX, BT_OV4SI, BT_INTCONSTPTR)
+DEF_FN_TYPE_2 (BT_FN_OV4SI_OV4SI, B_VX, BT_OV4SI, BT_OV4SI)
+DEF_FN_TYPE_2 (BT_FN_UV16QI_UCHAR, B_VX, BT_UV16QI, BT_UCHAR)
+DEF_FN_TYPE_2 (BT_FN_UV16QI_UCHARCONSTPTR, B_VX, BT_UV16QI, BT_UCHARCONSTPTR)
+DEF_FN_TYPE_2 (BT_FN_UV16QI_USHORT, B_VX, BT_UV16QI, BT_USHORT)
+DEF_FN_TYPE_2 (BT_FN_UV16QI_UV16QI, B_VX, BT_UV16QI, BT_UV16QI)
+DEF_FN_TYPE_2 (BT_FN_UV2DI_ULONGLONG, B_VX, BT_UV2DI, BT_ULONGLONG)
+DEF_FN_TYPE_2 (BT_FN_UV2DI_ULONGLONGCONSTPTR, B_VX, BT_UV2DI, BT_ULONGLONGCONSTPTR)
+DEF_FN_TYPE_2 (BT_FN_UV2DI_USHORT, B_VX, BT_UV2DI, BT_USHORT)
+DEF_FN_TYPE_2 (BT_FN_UV2DI_UV2DI, B_VX, BT_UV2DI, BT_UV2DI)
+DEF_FN_TYPE_2 (BT_FN_UV2DI_UV4SI, B_VX, BT_UV2DI, BT_UV4SI)
+DEF_FN_TYPE_2 (BT_FN_UV4SI_UINT, B_VX, BT_UV4SI, BT_UINT)
+DEF_FN_TYPE_2 (BT_FN_UV4SI_UINTCONSTPTR, B_VX, BT_UV4SI, BT_UINTCONSTPTR)
+DEF_FN_TYPE_2 (BT_FN_UV4SI_USHORT, B_VX, BT_UV4SI, BT_USHORT)
+DEF_FN_TYPE_2 (BT_FN_UV4SI_UV4SI, B_VX, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_2 (BT_FN_UV4SI_UV8HI, B_VX, BT_UV4SI, BT_UV8HI)
+DEF_FN_TYPE_2 (BT_FN_UV8HI_USHORT, B_VX, BT_UV8HI, BT_USHORT)
+DEF_FN_TYPE_2 (BT_FN_UV8HI_USHORTCONSTPTR, B_VX, BT_UV8HI, BT_USHORTCONSTPTR)
+DEF_FN_TYPE_2 (BT_FN_UV8HI_UV16QI, B_VX, BT_UV8HI, BT_UV16QI)
+DEF_FN_TYPE_2 (BT_FN_UV8HI_UV8HI, B_VX, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_2 (BT_FN_V16QI_SCHAR, B_VX, BT_V16QI, BT_SCHAR)
+DEF_FN_TYPE_2 (BT_FN_V16QI_UCHAR, B_VX, BT_V16QI, BT_UCHAR)
+DEF_FN_TYPE_2 (BT_FN_V16QI_V16QI, B_VX, BT_V16QI, BT_V16QI)
+DEF_FN_TYPE_2 (BT_FN_V2DF_DBL, B_VX, BT_V2DF, BT_DBL)
+DEF_FN_TYPE_2 (BT_FN_V2DF_FLTCONSTPTR, B_VX, BT_V2DF, BT_FLTCONSTPTR)
+DEF_FN_TYPE_2 (BT_FN_V2DF_V2DF, B_VX, BT_V2DF, BT_V2DF)
+DEF_FN_TYPE_2 (BT_FN_V2DI_SHORT, B_VX, BT_V2DI, BT_SHORT)
+DEF_FN_TYPE_2 (BT_FN_V2DI_V16QI, B_VX, BT_V2DI, BT_V16QI)
+DEF_FN_TYPE_2 (BT_FN_V2DI_V2DI, B_VX, BT_V2DI, BT_V2DI)
+DEF_FN_TYPE_2 (BT_FN_V2DI_V4SI, B_VX, BT_V2DI, BT_V4SI)
+DEF_FN_TYPE_2 (BT_FN_V2DI_V8HI, B_VX, BT_V2DI, BT_V8HI)
+DEF_FN_TYPE_2 (BT_FN_V4SI_SHORT, B_VX, BT_V4SI, BT_SHORT)
+DEF_FN_TYPE_2 (BT_FN_V4SI_V4SI, B_VX, BT_V4SI, BT_V4SI)
+DEF_FN_TYPE_2 (BT_FN_V4SI_V8HI, B_VX, BT_V4SI, BT_V8HI)
+DEF_FN_TYPE_2 (BT_FN_V8HI_SHORT, B_VX, BT_V8HI, BT_SHORT)
+DEF_FN_TYPE_2 (BT_FN_V8HI_V16QI, B_VX, BT_V8HI, BT_V16QI)
+DEF_FN_TYPE_2 (BT_FN_V8HI_V8HI, B_VX, BT_V8HI, BT_V8HI)
+DEF_FN_TYPE_2 (BT_FN_VOID_INT, B_HTM, BT_VOID, BT_INT)
+DEF_FN_TYPE_2 (BT_FN_VOID_UINT, 0, BT_VOID, BT_UINT)
+DEF_FN_TYPE_3 (BT_FN_DBL_V2DF_INT, B_VX, BT_DBL, BT_V2DF, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_INT_OV4SI_INT, B_VX, BT_INT, BT_OV4SI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_INT_OV4SI_OV4SI, B_VX, BT_INT, BT_OV4SI, BT_OV4SI)
+DEF_FN_TYPE_3 (BT_FN_INT_UV16QI_UV16QI, B_VX, BT_INT, BT_UV16QI, BT_UV16QI)
+DEF_FN_TYPE_3 (BT_FN_INT_UV2DI_UV2DI, B_VX, BT_INT, BT_UV2DI, BT_UV2DI)
+DEF_FN_TYPE_3 (BT_FN_INT_UV4SI_UV4SI, B_VX, BT_INT, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_3 (BT_FN_INT_UV8HI_UV8HI, B_VX, BT_INT, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_3 (BT_FN_INT_V16QI_V16QI, B_VX, BT_INT, BT_V16QI, BT_V16QI)
+DEF_FN_TYPE_3 (BT_FN_INT_V2DF_V2DF, B_VX, BT_INT, BT_V2DF, BT_V2DF)
+DEF_FN_TYPE_3 (BT_FN_INT_V2DI_V2DI, B_VX, BT_INT, BT_V2DI, BT_V2DI)
+DEF_FN_TYPE_3 (BT_FN_INT_V4SI_V4SI, B_VX, BT_INT, BT_V4SI, BT_V4SI)
+DEF_FN_TYPE_3 (BT_FN_INT_V8HI_V8HI, B_VX, BT_INT, BT_V8HI, BT_V8HI)
+DEF_FN_TYPE_3 (BT_FN_INT_VOIDPTR_INT, B_HTM, BT_INT, BT_VOIDPTR, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_OV2DI_LONGLONG_LONGLONG, B_VX, BT_OV2DI, BT_LONGLONG, BT_LONGLONG)
+DEF_FN_TYPE_3 (BT_FN_OV4SI_INTCONSTPTR_INT, B_VX, BT_OV4SI, BT_INTCONSTPTR, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_OV4SI_INTCONSTPTR_UINT, B_VX, BT_OV4SI, BT_INTCONSTPTR, BT_UINT)
+DEF_FN_TYPE_3 (BT_FN_OV4SI_INT_INT, B_VX, BT_OV4SI, BT_INT, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_OV4SI_OV4SI_INTPTR, B_VX, BT_OV4SI, BT_OV4SI, BT_INTPTR)
+DEF_FN_TYPE_3 (BT_FN_OV4SI_OV4SI_OV4SI, B_VX, BT_OV4SI, BT_OV4SI, BT_OV4SI)
+DEF_FN_TYPE_3 (BT_FN_OV4SI_OV4SI_UCHAR, B_VX, BT_OV4SI, BT_OV4SI, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_OV4SI_OV4SI_ULONG, B_VX, BT_OV4SI, BT_OV4SI, BT_ULONG)
+DEF_FN_TYPE_3 (BT_FN_UCHAR_UV16QI_INT, B_VX, BT_UCHAR, BT_UV16QI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UINT_UV4SI_INT, B_VX, BT_UINT, BT_UV4SI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UINT_VOIDCONSTPTR_INT, B_VX, BT_UINT, BT_VOIDCONSTPTR, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_ULONGLONG_UV2DI_INT, B_VX, BT_ULONGLONG, BT_UV2DI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_USHORT_UV8HI_INT, B_VX, BT_USHORT, BT_UV8HI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UCHARCONSTPTR_USHORT, B_VX, BT_UV16QI, BT_UCHARCONSTPTR, BT_USHORT)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UCHAR_INT, B_VX, BT_UV16QI, BT_UCHAR, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UCHAR_UCHAR, B_VX, BT_UV16QI, BT_UCHAR, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UV16QI_INTPTR, B_VX, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UV16QI_UCHAR, B_VX, BT_UV16QI, BT_UV16QI, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UV16QI_UINT, B_VX, BT_UV16QI, BT_UV16QI, BT_UINT)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UV16QI_UV16QI, B_VX, BT_UV16QI, BT_UV16QI, BT_UV16QI)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UV2DI_UV2DI, B_VX, BT_UV16QI, BT_UV2DI, BT_UV2DI)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UV4SI_UV4SI, B_VX, BT_UV16QI, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_3 (BT_FN_UV16QI_UV8HI_UV8HI, B_VX, BT_UV16QI, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_UCHAR_UCHAR, B_VX, BT_UV2DI, BT_UCHAR, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_ULONGLONG_INT, B_VX, BT_UV2DI, BT_ULONGLONG, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_UV2DI_UCHAR, B_VX, BT_UV2DI, BT_UV2DI, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_UV2DI_UINT, B_VX, BT_UV2DI, BT_UV2DI, BT_UINT)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_UV2DI_UV2DI, B_VX, BT_UV2DI, BT_UV2DI, BT_UV2DI)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_UV4SI_UV4SI, B_VX, BT_UV2DI, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_UV8HI_UV8HI, B_VX, BT_UV2DI, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_3 (BT_FN_UV2DI_V2DF_INT, B_VX, BT_UV2DI, BT_V2DF, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UCHAR_UCHAR, B_VX, BT_UV4SI, BT_UCHAR, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UINT_INT, B_VX, BT_UV4SI, BT_UINT, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UV16QI_UV16QI, B_VX, BT_UV4SI, BT_UV16QI, BT_UV16QI)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UV2DI_UV2DI, B_VX, BT_UV4SI, BT_UV2DI, BT_UV2DI)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UV4SI_INTPTR, B_VX, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UV4SI_UCHAR, B_VX, BT_UV4SI, BT_UV4SI, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UV4SI_UINT, B_VX, BT_UV4SI, BT_UV4SI, BT_UINT)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UV4SI_UV4SI, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_3 (BT_FN_UV4SI_UV8HI_UV8HI, B_VX, BT_UV4SI, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_UCHAR_UCHAR, B_VX, BT_UV8HI, BT_UCHAR, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_USHORT_INT, B_VX, BT_UV8HI, BT_USHORT, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_UV16QI_UV16QI, B_VX, BT_UV8HI, BT_UV16QI, BT_UV16QI)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_UV4SI_UV4SI, B_VX, BT_UV8HI, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_UV8HI_INTPTR, B_VX, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_UV8HI_UCHAR, B_VX, BT_UV8HI, BT_UV8HI, BT_UCHAR)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_UV8HI_UINT, B_VX, BT_UV8HI, BT_UV8HI, BT_UINT)
+DEF_FN_TYPE_3 (BT_FN_UV8HI_UV8HI_UV8HI, B_VX, BT_UV8HI, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_3 (BT_FN_V16QI_BV16QI_V16QI, B_VX, BT_V16QI, BT_BV16QI, BT_V16QI)
+DEF_FN_TYPE_3 (BT_FN_V16QI_UINT_VOIDCONSTPTR, B_VX, BT_V16QI, BT_UINT, BT_VOIDCONSTPTR)
+DEF_FN_TYPE_3 (BT_FN_V16QI_UV16QI_UV16QI, B_VX, BT_V16QI, BT_UV16QI, BT_UV16QI)
+DEF_FN_TYPE_3 (BT_FN_V16QI_V16QI_V16QI, B_VX, BT_V16QI, BT_V16QI, BT_V16QI)
+DEF_FN_TYPE_3 (BT_FN_V16QI_V8HI_V8HI, B_VX, BT_V16QI, BT_V8HI, BT_V8HI)
+DEF_FN_TYPE_3 (BT_FN_V2DF_DBL_INT, B_VX, BT_V2DF, BT_DBL, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_V2DF_UV2DI_INT, B_VX, BT_V2DF, BT_UV2DI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_V2DF_UV4SI_INT, B_VX, BT_V2DF, BT_UV4SI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_V2DF_V2DF_V2DF, B_VX, BT_V2DF, BT_V2DF, BT_V2DF)
+DEF_FN_TYPE_3 (BT_FN_V2DF_V2DI_INT, B_VX, BT_V2DF, BT_V2DI, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_V2DI_BV2DI_V2DI, B_VX, BT_V2DI, BT_BV2DI, BT_V2DI)
+DEF_FN_TYPE_3 (BT_FN_V2DI_UV2DI_UV2DI, B_VX, BT_V2DI, BT_UV2DI, BT_UV2DI)
+DEF_FN_TYPE_3 (BT_FN_V2DI_V2DF_INT, B_VX, BT_V2DI, BT_V2DF, BT_INT)
+DEF_FN_TYPE_3 (BT_FN_V2DI_V2DF_V2DF, B_VX, BT_V2DI, BT_V2DF, BT_V2DF)
+DEF_FN_TYPE_3 (BT_FN_V2DI_V2DI_V2DI, B_VX, BT_V2DI, BT_V2DI, BT_V2DI)
+DEF_FN_TYPE_3 (BT_FN_V2DI_V4SI_V4SI, B_VX, BT_V2DI, BT_V4SI, BT_V4SI)
+DEF_FN_TYPE_3 (BT_FN_V4SI_BV4SI_V4SI, B_VX, BT_V4SI, BT_BV4SI, BT_V4SI)
+DEF_FN_TYPE_3 (BT_FN_V4SI_INT_VOIDPTR, B_VX, BT_V4SI, BT_INT, BT_VOIDPTR)
+DEF_FN_TYPE_3 (BT_FN_V4SI_UV4SI_UV4SI, B_VX, BT_V4SI, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_3 (BT_FN_V4SI_V2DI_V2DI, B_VX, BT_V4SI, BT_V2DI, BT_V2DI)
+DEF_FN_TYPE_3 (BT_FN_V4SI_V4SI_V4SI, B_VX, BT_V4SI, BT_V4SI, BT_V4SI)
+DEF_FN_TYPE_3 (BT_FN_V4SI_V8HI_V8HI, B_VX, BT_V4SI, BT_V8HI, BT_V8HI)
+DEF_FN_TYPE_3 (BT_FN_V8HI_BV8HI_V8HI, B_VX, BT_V8HI, BT_BV8HI, BT_V8HI)
+DEF_FN_TYPE_3 (BT_FN_V8HI_UV8HI_UV8HI, B_VX, BT_V8HI, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_3 (BT_FN_V8HI_V16QI_V16QI, B_VX, BT_V8HI, BT_V16QI, BT_V16QI)
+DEF_FN_TYPE_3 (BT_FN_V8HI_V4SI_V4SI, B_VX, BT_V8HI, BT_V4SI, BT_V4SI)
+DEF_FN_TYPE_3 (BT_FN_V8HI_V8HI_V8HI, B_VX, BT_V8HI, BT_V8HI, BT_V8HI)
+DEF_FN_TYPE_3 (BT_FN_VOID_UINT64PTR_UINT64, B_HTM, BT_VOID, BT_UINT64PTR, BT_UINT64)
+DEF_FN_TYPE_3 (BT_FN_VOID_V2DF_FLTPTR, B_VX, BT_VOID, BT_V2DF, BT_FLTPTR)
+DEF_FN_TYPE_4 (BT_FN_INT_OV4SI_OV4SI_INTPTR, B_VX, BT_INT, BT_OV4SI, BT_OV4SI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_OV4SI_INT_OV4SI_INT, B_VX, BT_OV4SI, BT_INT, BT_OV4SI, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_OV4SI_OV4SI_OV4SI_INT, B_VX, BT_OV4SI, BT_OV4SI, BT_OV4SI, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_OV4SI_OV4SI_OV4SI_INTPTR, B_VX, BT_OV4SI, BT_OV4SI, BT_OV4SI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_OV4SI_OV4SI_OV4SI_OV4SI, B_VX, BT_OV4SI, BT_OV4SI, BT_OV4SI, BT_OV4SI)
+DEF_FN_TYPE_4 (BT_FN_OV4SI_OV4SI_OV4SI_UCHAR, B_VX, BT_OV4SI, BT_OV4SI, BT_OV4SI, BT_UCHAR)
+DEF_FN_TYPE_4 (BT_FN_OV4SI_OV4SI_OV4SI_ULONGLONG, B_VX, BT_OV4SI, BT_OV4SI, BT_OV4SI, BT_ULONGLONG)
+DEF_FN_TYPE_4 (BT_FN_UV16QI_UV16QI_UCHAR_INT, B_VX, BT_UV16QI, BT_UV16QI, BT_UCHAR, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV16QI_UV16QI_UV16QI_INT, B_VX, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV16QI_UV16QI_UV16QI_INTPTR, B_VX, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_UV16QI_UV16QI_UV16QI_UV16QI, B_VX, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI)
+DEF_FN_TYPE_4 (BT_FN_UV16QI_UV2DI_UV2DI_UV16QI, B_VX, BT_UV16QI, BT_UV2DI, BT_UV2DI, BT_UV16QI)
+DEF_FN_TYPE_4 (BT_FN_UV16QI_UV8HI_UV8HI_INTPTR, B_VX, BT_UV16QI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_UV2DI_UV2DI_ULONGLONG_INT, B_VX, BT_UV2DI, BT_UV2DI, BT_ULONGLONG, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV2DI_UV2DI_UV2DI_INT, B_VX, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV2DI_UV4SI_UV4SI_UV2DI, B_VX, BT_UV2DI, BT_UV4SI, BT_UV4SI, BT_UV2DI)
+DEF_FN_TYPE_4 (BT_FN_UV4SI_UV2DI_UV2DI_INTPTR, B_VX, BT_UV4SI, BT_UV2DI, BT_UV2DI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_UV4SI_UV4SI_UINT_INT, B_VX, BT_UV4SI, BT_UV4SI, BT_UINT, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV4SI_UV4SI_UV4SI_INT, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV4SI_UV4SI_UV4SI_INTPTR, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_UV4SI_UV4SI_UV4SI_UV4SI, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI)
+DEF_FN_TYPE_4 (BT_FN_UV4SI_UV8HI_UV8HI_UV4SI, B_VX, BT_UV4SI, BT_UV8HI, BT_UV8HI, BT_UV4SI)
+DEF_FN_TYPE_4 (BT_FN_UV8HI_UV16QI_UV16QI_UV8HI, B_VX, BT_UV8HI, BT_UV16QI, BT_UV16QI, BT_UV8HI)
+DEF_FN_TYPE_4 (BT_FN_UV8HI_UV4SI_UV4SI_INTPTR, B_VX, BT_UV8HI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_UV8HI_UV8HI_USHORT_INT, B_VX, BT_UV8HI, BT_UV8HI, BT_USHORT, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV8HI_UV8HI_UV8HI_INT, B_VX, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_UV8HI_UV8HI_UV8HI_INTPTR, B_VX, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_UV8HI_UV8HI_UV8HI_UV8HI, B_VX, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI)
+DEF_FN_TYPE_4 (BT_FN_V16QI_UV16QI_UV16QI_INTPTR, B_VX, BT_V16QI, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V16QI_V16QI_V16QI_INTPTR, B_VX, BT_V16QI, BT_V16QI, BT_V16QI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V16QI_V16QI_V16QI_V16QI, B_VX, BT_V16QI, BT_V16QI, BT_V16QI, BT_V16QI)
+DEF_FN_TYPE_4 (BT_FN_V16QI_V8HI_V8HI_INTPTR, B_VX, BT_V16QI, BT_V8HI, BT_V8HI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V2DF_V2DF_DBL_INT, B_VX, BT_V2DF, BT_V2DF, BT_DBL, BT_INT)
+DEF_FN_TYPE_4 (BT_FN_V2DF_V2DF_UCHAR_UCHAR, B_VX, BT_V2DF, BT_V2DF, BT_UCHAR, BT_UCHAR)
+DEF_FN_TYPE_4 (BT_FN_V2DF_V2DF_V2DF_V2DF, B_VX, BT_V2DF, BT_V2DF, BT_V2DF, BT_V2DF)
+DEF_FN_TYPE_4 (BT_FN_V2DI_UV2DI_UV2DI_INTPTR, B_VX, BT_V2DI, BT_UV2DI, BT_UV2DI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V2DI_V2DF_INT_INTPTR, B_VX, BT_V2DI, BT_V2DF, BT_INT, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V2DI_V2DF_V2DF_INTPTR, B_VX, BT_V2DI, BT_V2DF, BT_V2DF, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V2DI_V2DI_V2DI_INTPTR, B_VX, BT_V2DI, BT_V2DI, BT_V2DI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V2DI_V4SI_V4SI_V2DI, B_VX, BT_V2DI, BT_V4SI, BT_V4SI, BT_V2DI)
+DEF_FN_TYPE_4 (BT_FN_V4SI_UV4SI_UV4SI_INTPTR, B_VX, BT_V4SI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V4SI_V2DI_V2DI_INTPTR, B_VX, BT_V4SI, BT_V2DI, BT_V2DI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V4SI_V4SI_V4SI_INTPTR, B_VX, BT_V4SI, BT_V4SI, BT_V4SI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V4SI_V4SI_V4SI_V4SI, B_VX, BT_V4SI, BT_V4SI, BT_V4SI, BT_V4SI)
+DEF_FN_TYPE_4 (BT_FN_V4SI_V8HI_V8HI_V4SI, B_VX, BT_V4SI, BT_V8HI, BT_V8HI, BT_V4SI)
+DEF_FN_TYPE_4 (BT_FN_V8HI_UV8HI_UV8HI_INTPTR, B_VX, BT_V8HI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V8HI_V16QI_V16QI_V8HI, B_VX, BT_V8HI, BT_V16QI, BT_V16QI, BT_V8HI)
+DEF_FN_TYPE_4 (BT_FN_V8HI_V4SI_V4SI_INTPTR, B_VX, BT_V8HI, BT_V4SI, BT_V4SI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V8HI_V8HI_V8HI_INTPTR, B_VX, BT_V8HI, BT_V8HI, BT_V8HI, BT_INTPTR)
+DEF_FN_TYPE_4 (BT_FN_V8HI_V8HI_V8HI_V8HI, B_VX, BT_V8HI, BT_V8HI, BT_V8HI, BT_V8HI)
+DEF_FN_TYPE_4 (BT_FN_VOID_OV4SI_INT_VOIDPTR, B_VX, BT_VOID, BT_OV4SI, BT_INT, BT_VOIDPTR)
+DEF_FN_TYPE_4 (BT_FN_VOID_OV4SI_VOIDPTR_UINT, B_VX, BT_VOID, BT_OV4SI, BT_VOIDPTR, BT_UINT)
+DEF_FN_TYPE_4 (BT_FN_VOID_V16QI_UINT_VOIDPTR, B_VX, BT_VOID, BT_V16QI, BT_UINT, BT_VOIDPTR)
+DEF_FN_TYPE_5 (BT_FN_OV4SI_OV4SI_OUV4SI_INTCONSTPTR_UCHAR, B_VX, BT_OV4SI, BT_OV4SI, BT_OUV4SI, BT_INTCONSTPTR, BT_UCHAR)
+DEF_FN_TYPE_5 (BT_FN_OV4SI_OV4SI_OV4SI_OV4SI_INTPTR, B_VX, BT_OV4SI, BT_OV4SI, BT_OV4SI, BT_OV4SI, BT_INTPTR)
+DEF_FN_TYPE_5 (BT_FN_UV16QI_UV16QI_UV16QI_INT_INTPTR, B_VX, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INT, BT_INTPTR)
+DEF_FN_TYPE_5 (BT_FN_UV16QI_UV16QI_UV16QI_UV16QI_INT, B_VX, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INT)
+DEF_FN_TYPE_5 (BT_FN_UV2DI_UV2DI_UV2DI_ULONGLONGCONSTPTR_UCHAR, B_VX, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_ULONGLONGCONSTPTR, BT_UCHAR)
+DEF_FN_TYPE_5 (BT_FN_UV2DI_UV2DI_UV2DI_UV2DI_INT, B_VX, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_INT)
+DEF_FN_TYPE_5 (BT_FN_UV4SI_UV4SI_UV4SI_INT_INTPTR, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INT, BT_INTPTR)
+DEF_FN_TYPE_5 (BT_FN_UV4SI_UV4SI_UV4SI_UINTCONSTPTR_UCHAR, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UINTCONSTPTR, BT_UCHAR)
+DEF_FN_TYPE_5 (BT_FN_UV4SI_UV4SI_UV4SI_UV4SI_INT, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INT)
+DEF_FN_TYPE_5 (BT_FN_UV8HI_UV8HI_UV8HI_INT_INTPTR, B_VX, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INT, BT_INTPTR)
+DEF_FN_TYPE_5 (BT_FN_UV8HI_UV8HI_UV8HI_UV8HI_INT, B_VX, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INT)
+DEF_FN_TYPE_5 (BT_FN_VOID_UV2DI_UV2DI_ULONGLONGPTR_ULONGLONG, B_VX, BT_VOID, BT_UV2DI, BT_UV2DI, BT_ULONGLONGPTR, BT_ULONGLONG)
+DEF_FN_TYPE_5 (BT_FN_VOID_UV4SI_UV4SI_UINTPTR_ULONGLONG, B_VX, BT_VOID, BT_UV4SI, BT_UV4SI, BT_UINTPTR, BT_ULONGLONG)
+DEF_FN_TYPE_5 (BT_FN_VOID_V4SI_V4SI_INTPTR_ULONGLONG, B_VX, BT_VOID, BT_V4SI, BT_V4SI, BT_INTPTR, BT_ULONGLONG)
+DEF_FN_TYPE_6 (BT_FN_UV16QI_UV16QI_UV16QI_UV16QI_INT_INTPTR, B_VX, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INT, BT_INTPTR)
+DEF_FN_TYPE_6 (BT_FN_UV4SI_UV4SI_UV4SI_UV4SI_INT_INTPTR, B_VX, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INT, BT_INTPTR)
+DEF_FN_TYPE_6 (BT_FN_UV8HI_UV8HI_UV8HI_UV8HI_INT_INTPTR, B_VX, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INT, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI, BT_BV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_BV16QI, BT_BV16QI, BT_BV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_BV16QI_BV16QI, BT_BV16QI, BT_BV16QI, BT_BV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_BV16QI_INTPTR, BT_BV16QI, BT_BV16QI, BT_BV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_BV16QI_UV16QI, BT_BV16QI, BT_BV16QI, BT_BV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_INTPTR, BT_BV16QI, BT_BV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_UCHAR, BT_BV16QI, BT_BV16QI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_UV16QI, BT_BV16QI, BT_BV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_UV4SI, BT_BV16QI, BT_BV16QI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_BV16QI_BV16QI_UV8HI, BT_BV16QI, BT_BV16QI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_BV16QI_BV8HI_BV8HI, BT_BV16QI, BT_BV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_BV16QI_UV16QI_UV16QI, BT_BV16QI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_UV16QI_UV16QI_INTPTR, BT_BV16QI, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV16QI_UV16QI_UV16QI_UV16QI, BT_BV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_UV16QI_UV16QI_UV16QI_INTPTR, BT_BV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV16QI_V16QI_V16QI, BT_BV16QI, BT_V16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_BV16QI_V16QI_V16QI_INTPTR, BT_BV16QI, BT_V16QI, BT_V16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_BV2DI, BT_BV2DI, BT_BV2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_BV2DI_BV2DI, BT_BV2DI, BT_BV2DI, BT_BV2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_BV2DI_INT, BT_BV2DI, BT_BV2DI, BT_BV2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_BV2DI_UV16QI, BT_BV2DI, BT_BV2DI, BT_BV2DI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_BV2DI_UV2DI, BT_BV2DI, BT_BV2DI, BT_BV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_UCHAR, BT_BV2DI, BT_BV2DI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_UV16QI, BT_BV2DI, BT_BV2DI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_UV2DI_ULONGLONGCONSTPTR_UCHAR, BT_BV2DI, BT_BV2DI, BT_UV2DI, BT_ULONGLONGCONSTPTR, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_UV4SI, BT_BV2DI, BT_BV2DI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_BV2DI_BV2DI_UV8HI, BT_BV2DI, BT_BV2DI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_BV2DI_BV4SI, BT_BV2DI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_BV2DI_UV2DI_UV2DI, BT_BV2DI, BT_UV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_BV2DI_V2DF_V2DF, BT_BV2DI, BT_V2DF, BT_V2DF)
+DEF_OV_TYPE (BT_OV_BV2DI_V2DI_V2DI, BT_BV2DI, BT_V2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV2DI_BV2DI, BT_BV4SI, BT_BV2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI, BT_BV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_BV4SI, BT_BV4SI, BT_BV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_BV4SI_BV4SI, BT_BV4SI, BT_BV4SI, BT_BV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_BV4SI_INTPTR, BT_BV4SI, BT_BV4SI, BT_BV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_BV4SI_UV16QI, BT_BV4SI, BT_BV4SI, BT_BV4SI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_BV4SI_UV4SI, BT_BV4SI, BT_BV4SI, BT_BV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_INTPTR, BT_BV4SI, BT_BV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_UCHAR, BT_BV4SI, BT_BV4SI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_UV16QI, BT_BV4SI, BT_BV4SI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_UV4SI, BT_BV4SI, BT_BV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_UV4SI_UINTCONSTPTR_UCHAR, BT_BV4SI, BT_BV4SI, BT_UV4SI, BT_UINTCONSTPTR, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_BV4SI_BV4SI_UV8HI, BT_BV4SI, BT_BV4SI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_BV4SI_BV8HI, BT_BV4SI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_BV4SI_UV4SI_UV4SI, BT_BV4SI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_UV4SI_UV4SI_INTPTR, BT_BV4SI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV4SI_UV4SI_UV4SI_UV4SI, BT_BV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_UV4SI_UV4SI_UV4SI_INTPTR, BT_BV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV4SI_V4SI_V4SI, BT_BV4SI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_BV4SI_V4SI_V4SI_INTPTR, BT_BV4SI, BT_V4SI, BT_V4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV8HI_BV16QI, BT_BV8HI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV4SI_BV4SI, BT_BV8HI, BT_BV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI, BT_BV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_BV8HI, BT_BV8HI, BT_BV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_BV8HI_BV8HI, BT_BV8HI, BT_BV8HI, BT_BV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_BV8HI_INTPTR, BT_BV8HI, BT_BV8HI, BT_BV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_BV8HI_UV16QI, BT_BV8HI, BT_BV8HI, BT_BV8HI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_BV8HI_UV8HI, BT_BV8HI, BT_BV8HI, BT_BV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_INTPTR, BT_BV8HI, BT_BV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_UCHAR, BT_BV8HI, BT_BV8HI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_UV16QI, BT_BV8HI, BT_BV8HI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_UV4SI, BT_BV8HI, BT_BV8HI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_BV8HI_BV8HI_UV8HI, BT_BV8HI, BT_BV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_UV8HI_UV8HI, BT_BV8HI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_UV8HI_UV8HI_INTPTR, BT_BV8HI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV8HI_UV8HI_UV8HI_UV8HI, BT_BV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_UV8HI_UV8HI_UV8HI_INTPTR, BT_BV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_BV8HI_V8HI_V8HI, BT_BV8HI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_BV8HI_V8HI_V8HI_INTPTR, BT_BV8HI, BT_V8HI, BT_V8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_DBL_V2DF_INT, BT_DBL, BT_V2DF, BT_INT)
+DEF_OV_TYPE (BT_OV_INT_BV16QI_BV16QI, BT_INT, BT_BV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_INT_BV16QI_UV16QI, BT_INT, BT_BV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_INT_BV16QI_V16QI, BT_INT, BT_BV16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_INT_BV2DI_BV2DI, BT_INT, BT_BV2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_INT_BV2DI_UV2DI, BT_INT, BT_BV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_INT_BV2DI_V2DI, BT_INT, BT_BV2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_INT_BV4SI_BV4SI, BT_INT, BT_BV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_INT_BV4SI_UV4SI, BT_INT, BT_BV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_INT_BV4SI_V4SI, BT_INT, BT_BV4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_INT_BV8HI_BV8HI, BT_INT, BT_BV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_INT_BV8HI_UV8HI, BT_INT, BT_BV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_INT_BV8HI_V8HI, BT_INT, BT_BV8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_INT_UV16QI_BV16QI, BT_INT, BT_UV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_INT_UV16QI_UV16QI, BT_INT, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_INT_UV2DI_BV2DI, BT_INT, BT_UV2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_INT_UV2DI_UV2DI, BT_INT, BT_UV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_INT_UV4SI_BV4SI, BT_INT, BT_UV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_INT_UV4SI_UV4SI, BT_INT, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_INT_UV8HI_BV8HI, BT_INT, BT_UV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_INT_UV8HI_UV8HI, BT_INT, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_INT_V16QI_BV16QI, BT_INT, BT_V16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_INT_V16QI_UV16QI, BT_INT, BT_V16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_INT_V16QI_V16QI, BT_INT, BT_V16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_INT_V2DF_UV2DI, BT_INT, BT_V2DF, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_INT_V2DF_V2DF, BT_INT, BT_V2DF, BT_V2DF)
+DEF_OV_TYPE (BT_OV_INT_V2DI_BV2DI, BT_INT, BT_V2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_INT_V2DI_UV2DI, BT_INT, BT_V2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_INT_V2DI_V2DI, BT_INT, BT_V2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_INT_V4SI_BV4SI, BT_INT, BT_V4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_INT_V4SI_INT, BT_INT, BT_V4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_INT_V4SI_UV4SI, BT_INT, BT_V4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_INT_V4SI_V4SI, BT_INT, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_INT_V8HI_BV8HI, BT_INT, BT_V8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_INT_V8HI_UV8HI, BT_INT, BT_V8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_INT_V8HI_V8HI, BT_INT, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_LONGLONG_V2DI_INT, BT_LONGLONG, BT_V2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_SCHAR_V16QI_INT, BT_SCHAR, BT_V16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_SHORT_V8HI_INT, BT_SHORT, BT_V8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_UCHAR_BV16QI_INT, BT_UCHAR, BT_BV16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_UCHAR_UV16QI_INT, BT_UCHAR, BT_UV16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_UINT_BV4SI_INT, BT_UINT, BT_BV4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_UINT_UV4SI_INT, BT_UINT, BT_UV4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_ULONGLONG_BV2DI_INT, BT_ULONGLONG, BT_BV2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_ULONGLONG_UV2DI_INT, BT_ULONGLONG, BT_UV2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_USHORT_BV8HI_INT, BT_USHORT, BT_BV8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_USHORT_UV8HI_INT, BT_USHORT, BT_UV8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV16QI_BV16QI_BV16QI, BT_UV16QI, BT_BV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_BV16QI_BV16QI_INTPTR, BT_UV16QI, BT_BV16QI, BT_BV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV16QI_BV16QI_UV16QI, BT_UV16QI, BT_BV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_LONG_UCHARPTR, BT_UV16QI, BT_LONG, BT_UCHARPTR)
+DEF_OV_TYPE (BT_OV_UV16QI_UCHAR, BT_UV16QI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV16QI_UCHARCONSTPTR, BT_UV16QI, BT_UCHARCONSTPTR)
+DEF_OV_TYPE (BT_OV_UV16QI_UCHARCONSTPTR_UINT, BT_UV16QI, BT_UCHARCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_UV16QI_UCHARCONSTPTR_USHORT, BT_UV16QI, BT_UCHARCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_UV16QI_UCHAR_BV16QI_INT, BT_UV16QI, BT_UCHAR, BT_BV16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV16QI_UCHAR_INT, BT_UV16QI, BT_UCHAR, BT_INT)
+DEF_OV_TYPE (BT_OV_UV16QI_UCHAR_UV16QI_INT, BT_UV16QI, BT_UCHAR, BT_UV16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_BV16QI, BT_UV16QI, BT_UV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_INTPTR, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UCHAR, BT_UV16QI, BT_UV16QI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_ULONG, BT_UV16QI, BT_UV16QI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI_BV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI_INT, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI_INTPTR, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI_UCHAR, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI_ULONGLONG, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI_UV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV16QI_UV16QI_INTPTR, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_UV16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV4SI, BT_UV16QI, BT_UV16QI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_UV8HI, BT_UV16QI, BT_UV16QI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV16QI_V16QI, BT_UV16QI, BT_UV16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV2DI_UV2DI, BT_UV16QI, BT_UV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV4SI_UV4SI, BT_UV16QI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV8HI_UV8HI, BT_UV16QI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV16QI_UV8HI_UV8HI_INTPTR, BT_UV16QI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV16QI_V16QI, BT_UV16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_UV16QI_V8HI_V8HI, BT_UV16QI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_UV2DI_BV2DI_UV2DI, BT_UV2DI, BT_BV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_LONG_ULONGLONGPTR, BT_UV2DI, BT_LONG, BT_ULONGLONGPTR)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONG, BT_UV2DI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONGCONSTPTR, BT_UV2DI, BT_ULONGLONGCONSTPTR)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONGCONSTPTR_UINT, BT_UV2DI, BT_ULONGLONGCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONGCONSTPTR_USHORT, BT_UV2DI, BT_ULONGLONGCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONG_BV2DI_INT, BT_UV2DI, BT_ULONGLONG, BT_BV2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONG_INT, BT_UV2DI, BT_ULONGLONG, BT_INT)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONG_ULONGLONG, BT_UV2DI, BT_ULONGLONG, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_UV2DI_ULONGLONG_UV2DI_INT, BT_UV2DI, BT_ULONGLONG, BT_UV2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI, BT_UV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_BV2DI, BT_UV2DI, BT_UV2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UCHAR, BT_UV2DI, BT_UV2DI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_ULONG, BT_UV2DI, BT_UV2DI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV16QI, BT_UV2DI, BT_UV2DI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI, BT_UV2DI, BT_UV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI_BV2DI, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI_INT, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI_UCHAR, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI_ULONGLONG, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI_ULONGLONGCONSTPTR_UCHAR, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_ULONGLONGCONSTPTR, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI_UV16QI, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV2DI_UV2DI, BT_UV2DI, BT_UV2DI, BT_UV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV4SI, BT_UV2DI, BT_UV2DI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_UV8HI, BT_UV2DI, BT_UV2DI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV2DI_V2DI, BT_UV2DI, BT_UV2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV4SI, BT_UV2DI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV4SI_UV4SI, BT_UV2DI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV4SI_UV4SI_UV2DI, BT_UV2DI, BT_UV4SI, BT_UV4SI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_UV2DI_UV8HI_UV8HI, BT_UV2DI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV2DI_V2DI, BT_UV2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_UV4SI_BV4SI_BV4SI, BT_UV4SI, BT_BV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_BV4SI_BV4SI_INTPTR, BT_UV4SI, BT_BV4SI, BT_BV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV4SI_BV4SI_UV4SI, BT_UV4SI, BT_BV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_LONG_UINTPTR, BT_UV4SI, BT_LONG, BT_UINTPTR)
+DEF_OV_TYPE (BT_OV_UV4SI_UINT, BT_UV4SI, BT_UINT)
+DEF_OV_TYPE (BT_OV_UV4SI_UINTCONSTPTR, BT_UV4SI, BT_UINTCONSTPTR)
+DEF_OV_TYPE (BT_OV_UV4SI_UINTCONSTPTR_UINT, BT_UV4SI, BT_UINTCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_UV4SI_UINTCONSTPTR_USHORT, BT_UV4SI, BT_UINTCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_UV4SI_UINT_BV4SI_INT, BT_UV4SI, BT_UINT, BT_BV4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV4SI_UINT_INT, BT_UV4SI, BT_UINT, BT_INT)
+DEF_OV_TYPE (BT_OV_UV4SI_UINT_UV4SI_INT, BT_UV4SI, BT_UINT, BT_UV4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV4SI_UV16QI_UV16QI, BT_UV4SI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV2DI_UV2DI, BT_UV4SI, BT_UV2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV2DI_UV2DI_INTPTR, BT_UV4SI, BT_UV2DI, BT_UV2DI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_BV4SI, BT_UV4SI, BT_UV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_INTPTR, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UCHAR, BT_UV4SI, BT_UV4SI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_ULONG, BT_UV4SI, BT_UV4SI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV16QI, BT_UV4SI, BT_UV4SI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_BV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_INT, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_INTPTR, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_UCHAR, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_UINTCONSTPTR_UCHAR, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UINTCONSTPTR, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_ULONGLONG, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_UV16QI, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV4SI_UV4SI_INTPTR, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_UV8HI, BT_UV4SI, BT_UV4SI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV4SI_V4SI, BT_UV4SI, BT_UV4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV8HI, BT_UV4SI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV8HI_UV8HI, BT_UV4SI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV4SI_UV8HI_UV8HI_UV4SI, BT_UV4SI, BT_UV8HI, BT_UV8HI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV4SI_V2DI_V2DI, BT_UV4SI, BT_V2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_UV4SI_V4SI, BT_UV4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_UV8HI_BV8HI_BV8HI, BT_UV8HI, BT_BV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_BV8HI_BV8HI_INTPTR, BT_UV8HI, BT_BV8HI, BT_BV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV8HI_BV8HI_UV8HI, BT_UV8HI, BT_BV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_LONG_USHORTPTR, BT_UV8HI, BT_LONG, BT_USHORTPTR)
+DEF_OV_TYPE (BT_OV_UV8HI_USHORT, BT_UV8HI, BT_USHORT)
+DEF_OV_TYPE (BT_OV_UV8HI_USHORTCONSTPTR, BT_UV8HI, BT_USHORTCONSTPTR)
+DEF_OV_TYPE (BT_OV_UV8HI_USHORTCONSTPTR_UINT, BT_UV8HI, BT_USHORTCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_UV8HI_USHORTCONSTPTR_USHORT, BT_UV8HI, BT_USHORTCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_UV8HI_USHORT_BV8HI_INT, BT_UV8HI, BT_USHORT, BT_BV8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV8HI_USHORT_INT, BT_UV8HI, BT_USHORT, BT_INT)
+DEF_OV_TYPE (BT_OV_UV8HI_USHORT_UV8HI_INT, BT_UV8HI, BT_USHORT, BT_UV8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV8HI_UV16QI, BT_UV8HI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV16QI_UV16QI, BT_UV8HI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV16QI_UV16QI_UV8HI, BT_UV8HI, BT_UV16QI, BT_UV16QI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV4SI_UV4SI, BT_UV8HI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV4SI_UV4SI_INTPTR, BT_UV8HI, BT_UV4SI, BT_UV4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_BV8HI, BT_UV8HI, BT_UV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_INTPTR, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UCHAR, BT_UV8HI, BT_UV8HI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_ULONG, BT_UV8HI, BT_UV8HI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV16QI, BT_UV8HI, BT_UV8HI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV4SI, BT_UV8HI, BT_UV8HI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_BV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_INT, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_INTPTR, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_UCHAR, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_ULONGLONG, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_UV16QI, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_UV8HI_UV8HI_INTPTR, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_UV8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_UV8HI_UV8HI_V8HI, BT_UV8HI, BT_UV8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_UV8HI_V4SI_V4SI, BT_UV8HI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_UV8HI_V8HI, BT_UV8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V16QI_BV16QI_V16QI, BT_V16QI, BT_BV16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V16QI_LONG_SCHARPTR, BT_V16QI, BT_LONG, BT_SCHARPTR)
+DEF_OV_TYPE (BT_OV_V16QI_SCHAR, BT_V16QI, BT_SCHAR)
+DEF_OV_TYPE (BT_OV_V16QI_SCHARCONSTPTR, BT_V16QI, BT_SCHARCONSTPTR)
+DEF_OV_TYPE (BT_OV_V16QI_SCHARCONSTPTR_UINT, BT_V16QI, BT_SCHARCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_V16QI_SCHARCONSTPTR_USHORT, BT_V16QI, BT_SCHARCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_V16QI_SCHAR_INT, BT_V16QI, BT_SCHAR, BT_INT)
+DEF_OV_TYPE (BT_OV_V16QI_SCHAR_V16QI_INT, BT_V16QI, BT_SCHAR, BT_V16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_V16QI_UV16QI_V16QI_V16QI, BT_V16QI, BT_UV16QI, BT_V16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI, BT_V16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_BV16QI, BT_V16QI, BT_V16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_INTPTR, BT_V16QI, BT_V16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_UCHAR, BT_V16QI, BT_V16QI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_ULONG, BT_V16QI, BT_V16QI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_UV16QI, BT_V16QI, BT_V16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_UV16QI_UCHAR, BT_V16QI, BT_V16QI, BT_UV16QI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_UV16QI_UV16QI, BT_V16QI, BT_V16QI, BT_UV16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_UV4SI, BT_V16QI, BT_V16QI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_UV8HI, BT_V16QI, BT_V16QI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_V16QI, BT_V16QI, BT_V16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_V16QI_BV16QI, BT_V16QI, BT_V16QI, BT_V16QI, BT_BV16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_V16QI_INT, BT_V16QI, BT_V16QI, BT_V16QI, BT_INT)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_V16QI_INTPTR, BT_V16QI, BT_V16QI, BT_V16QI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_V16QI_ULONGLONG, BT_V16QI, BT_V16QI, BT_V16QI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_V16QI_UV16QI, BT_V16QI, BT_V16QI, BT_V16QI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V16QI_V16QI_V16QI, BT_V16QI, BT_V16QI, BT_V16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V16QI_V8HI_V8HI, BT_V16QI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V16QI_V8HI_V8HI_INTPTR, BT_V16QI, BT_V8HI, BT_V8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V2DF_BV2DI_V2DF, BT_V2DF, BT_BV2DI, BT_V2DF)
+DEF_OV_TYPE (BT_OV_V2DF_DBL, BT_V2DF, BT_DBL)
+DEF_OV_TYPE (BT_OV_V2DF_DBLCONSTPTR, BT_V2DF, BT_DBLCONSTPTR)
+DEF_OV_TYPE (BT_OV_V2DF_DBLCONSTPTR_UINT, BT_V2DF, BT_DBLCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_V2DF_DBLCONSTPTR_USHORT, BT_V2DF, BT_DBLCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_V2DF_DBL_INT, BT_V2DF, BT_DBL, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DF_DBL_V2DF_INT, BT_V2DF, BT_DBL, BT_V2DF, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DF_LONG_DBLPTR, BT_V2DF, BT_LONG, BT_DBLPTR)
+DEF_OV_TYPE (BT_OV_V2DF_UV2DI_INT, BT_V2DF, BT_UV2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF, BT_V2DF, BT_V2DF)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_BV2DI, BT_V2DF, BT_V2DF, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_UCHAR, BT_V2DF, BT_V2DF, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_UV2DI, BT_V2DF, BT_V2DF, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_UV2DI_DBLCONSTPTR_UCHAR, BT_V2DF, BT_V2DF, BT_UV2DI, BT_DBLCONSTPTR, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_V2DF, BT_V2DF, BT_V2DF, BT_V2DF)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_V2DF_BV2DI, BT_V2DF, BT_V2DF, BT_V2DF, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_V2DF_INT, BT_V2DF, BT_V2DF, BT_V2DF, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_V2DF_ULONGLONG, BT_V2DF, BT_V2DF, BT_V2DF, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_V2DF_UV16QI, BT_V2DF, BT_V2DF, BT_V2DF, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_V2DF_UV2DI, BT_V2DF, BT_V2DF, BT_V2DF, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_V2DF_V2DF_V2DI, BT_V2DF, BT_V2DF, BT_V2DI)
+DEF_OV_TYPE (BT_OV_V2DF_V2DI_INT, BT_V2DF, BT_V2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DI_BV2DI_V2DI, BT_V2DI, BT_BV2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_V2DI_LONGLONG, BT_V2DI, BT_LONGLONG)
+DEF_OV_TYPE (BT_OV_V2DI_LONGLONGCONSTPTR, BT_V2DI, BT_LONGLONGCONSTPTR)
+DEF_OV_TYPE (BT_OV_V2DI_LONGLONGCONSTPTR_UINT, BT_V2DI, BT_LONGLONGCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_V2DI_LONGLONGCONSTPTR_USHORT, BT_V2DI, BT_LONGLONGCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_V2DI_LONGLONG_INT, BT_V2DI, BT_LONGLONG, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DI_LONGLONG_LONGLONG, BT_V2DI, BT_LONGLONG, BT_LONGLONG)
+DEF_OV_TYPE (BT_OV_V2DI_LONGLONG_V2DI_INT, BT_V2DI, BT_LONGLONG, BT_V2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DI_LONG_LONGLONGPTR, BT_V2DI, BT_LONG, BT_LONGLONGPTR)
+DEF_OV_TYPE (BT_OV_V2DI_V16QI, BT_V2DI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI, BT_V2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_BV2DI, BT_V2DI, BT_V2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_UCHAR, BT_V2DI, BT_V2DI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_ULONG, BT_V2DI, BT_V2DI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_UV16QI, BT_V2DI, BT_V2DI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_UV2DI, BT_V2DI, BT_V2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_UV2DI_LONGLONGCONSTPTR_UCHAR, BT_V2DI, BT_V2DI, BT_UV2DI, BT_LONGLONGCONSTPTR, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_UV2DI_UCHAR, BT_V2DI, BT_V2DI, BT_UV2DI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_UV4SI, BT_V2DI, BT_V2DI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_UV8HI, BT_V2DI, BT_V2DI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_V2DI, BT_V2DI, BT_V2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_V2DI_BV2DI, BT_V2DI, BT_V2DI, BT_V2DI, BT_BV2DI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_V2DI_INT, BT_V2DI, BT_V2DI, BT_V2DI, BT_INT)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_V2DI_ULONGLONG, BT_V2DI, BT_V2DI, BT_V2DI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_V2DI_UV16QI, BT_V2DI, BT_V2DI, BT_V2DI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V2DI_V2DI_V2DI_UV2DI, BT_V2DI, BT_V2DI, BT_V2DI, BT_UV2DI)
+DEF_OV_TYPE (BT_OV_V2DI_V4SI, BT_V2DI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V2DI_V4SI_V4SI, BT_V2DI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V2DI_V4SI_V4SI_V2DI, BT_V2DI, BT_V4SI, BT_V4SI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_V2DI_V8HI, BT_V2DI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V4SI_BV4SI_V4SI, BT_V4SI, BT_BV4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V4SI_INT, BT_V4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_V4SI_INTCONSTPTR, BT_V4SI, BT_INTCONSTPTR)
+DEF_OV_TYPE (BT_OV_V4SI_INTCONSTPTR_UINT, BT_V4SI, BT_INTCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_V4SI_INTCONSTPTR_USHORT, BT_V4SI, BT_INTCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_V4SI_INT_INT, BT_V4SI, BT_INT, BT_INT)
+DEF_OV_TYPE (BT_OV_V4SI_INT_V4SI_INT, BT_V4SI, BT_INT, BT_V4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_V4SI_LONG_INTPTR, BT_V4SI, BT_LONG, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V4SI_UV4SI_V4SI_V4SI, BT_V4SI, BT_UV4SI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V2DI_V2DI, BT_V4SI, BT_V2DI, BT_V2DI)
+DEF_OV_TYPE (BT_OV_V4SI_V2DI_V2DI_INTPTR, BT_V4SI, BT_V2DI, BT_V2DI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_BV4SI, BT_V4SI, BT_V4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_INTPTR, BT_V4SI, BT_V4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_UCHAR, BT_V4SI, BT_V4SI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_ULONG, BT_V4SI, BT_V4SI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_UV16QI, BT_V4SI, BT_V4SI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_UV4SI, BT_V4SI, BT_V4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_UV4SI_INTCONSTPTR_UCHAR, BT_V4SI, BT_V4SI, BT_UV4SI, BT_INTCONSTPTR, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_UV4SI_UCHAR, BT_V4SI, BT_V4SI, BT_UV4SI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_UV4SI_UV4SI, BT_V4SI, BT_V4SI, BT_UV4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_UV8HI, BT_V4SI, BT_V4SI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI, BT_V4SI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI_BV4SI, BT_V4SI, BT_V4SI, BT_V4SI, BT_BV4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI_INT, BT_V4SI, BT_V4SI, BT_V4SI, BT_INT)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI_INTPTR, BT_V4SI, BT_V4SI, BT_V4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI_ULONGLONG, BT_V4SI, BT_V4SI, BT_V4SI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI_UV16QI, BT_V4SI, BT_V4SI, BT_V4SI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI_UV4SI, BT_V4SI, BT_V4SI, BT_V4SI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V4SI_V4SI_V4SI, BT_V4SI, BT_V4SI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V4SI_V8HI, BT_V4SI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V4SI_V8HI_V8HI, BT_V4SI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V4SI_V8HI_V8HI_V4SI, BT_V4SI, BT_V8HI, BT_V8HI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V8HI_BV8HI_V8HI, BT_V8HI, BT_BV8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V8HI_LONG_SHORTPTR, BT_V8HI, BT_LONG, BT_SHORTPTR)
+DEF_OV_TYPE (BT_OV_V8HI_SHORT, BT_V8HI, BT_SHORT)
+DEF_OV_TYPE (BT_OV_V8HI_SHORTCONSTPTR, BT_V8HI, BT_SHORTCONSTPTR)
+DEF_OV_TYPE (BT_OV_V8HI_SHORTCONSTPTR_UINT, BT_V8HI, BT_SHORTCONSTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_V8HI_SHORTCONSTPTR_USHORT, BT_V8HI, BT_SHORTCONSTPTR, BT_USHORT)
+DEF_OV_TYPE (BT_OV_V8HI_SHORT_INT, BT_V8HI, BT_SHORT, BT_INT)
+DEF_OV_TYPE (BT_OV_V8HI_SHORT_V8HI_INT, BT_V8HI, BT_SHORT, BT_V8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_V8HI_UV8HI_V8HI_V8HI, BT_V8HI, BT_UV8HI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V16QI, BT_V8HI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V8HI_V16QI_V16QI, BT_V8HI, BT_V16QI, BT_V16QI)
+DEF_OV_TYPE (BT_OV_V8HI_V16QI_V16QI_V8HI, BT_V8HI, BT_V16QI, BT_V16QI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V4SI_V4SI, BT_V8HI, BT_V4SI, BT_V4SI)
+DEF_OV_TYPE (BT_OV_V8HI_V4SI_V4SI_INTPTR, BT_V8HI, BT_V4SI, BT_V4SI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_BV8HI, BT_V8HI, BT_V8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_INTPTR, BT_V8HI, BT_V8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_UCHAR, BT_V8HI, BT_V8HI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_ULONG, BT_V8HI, BT_V8HI, BT_ULONG)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_UV16QI, BT_V8HI, BT_V8HI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_UV4SI, BT_V8HI, BT_V8HI, BT_UV4SI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_UV8HI, BT_V8HI, BT_V8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_UV8HI_UCHAR, BT_V8HI, BT_V8HI, BT_UV8HI, BT_UCHAR)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_UV8HI_UV8HI, BT_V8HI, BT_V8HI, BT_UV8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI, BT_V8HI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI_BV8HI, BT_V8HI, BT_V8HI, BT_V8HI, BT_BV8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI_INT, BT_V8HI, BT_V8HI, BT_V8HI, BT_INT)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI_INTPTR, BT_V8HI, BT_V8HI, BT_V8HI, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI_ULONGLONG, BT_V8HI, BT_V8HI, BT_V8HI, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI_UV16QI, BT_V8HI, BT_V8HI, BT_V8HI, BT_UV16QI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI_UV8HI, BT_V8HI, BT_V8HI, BT_V8HI, BT_UV8HI)
+DEF_OV_TYPE (BT_OV_V8HI_V8HI_V8HI_V8HI, BT_V8HI, BT_V8HI, BT_V8HI, BT_V8HI)
+DEF_OV_TYPE (BT_OV_VOID_BV2DI_UV2DI_ULONGLONGPTR_ULONGLONG, BT_VOID, BT_BV2DI, BT_UV2DI, BT_ULONGLONGPTR, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_VOID_BV4SI_UV4SI_UINTPTR_ULONGLONG, BT_VOID, BT_BV4SI, BT_UV4SI, BT_UINTPTR, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_VOID_UV16QI_LONG_UCHARPTR, BT_VOID, BT_UV16QI, BT_LONG, BT_UCHARPTR)
+DEF_OV_TYPE (BT_OV_VOID_UV16QI_UCHARPTR_UINT, BT_VOID, BT_UV16QI, BT_UCHARPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_UV2DI_LONG_ULONGLONGPTR, BT_VOID, BT_UV2DI, BT_LONG, BT_ULONGLONGPTR)
+DEF_OV_TYPE (BT_OV_VOID_UV2DI_ULONGLONGPTR_UINT, BT_VOID, BT_UV2DI, BT_ULONGLONGPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_UV2DI_UV2DI_ULONGLONGPTR_ULONGLONG, BT_VOID, BT_UV2DI, BT_UV2DI, BT_ULONGLONGPTR, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_VOID_UV4SI_LONG_UINTPTR, BT_VOID, BT_UV4SI, BT_LONG, BT_UINTPTR)
+DEF_OV_TYPE (BT_OV_VOID_UV4SI_UINTPTR_UINT, BT_VOID, BT_UV4SI, BT_UINTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_UV4SI_UV4SI_UINTPTR_ULONGLONG, BT_VOID, BT_UV4SI, BT_UV4SI, BT_UINTPTR, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_VOID_UV8HI_LONG_USHORTPTR, BT_VOID, BT_UV8HI, BT_LONG, BT_USHORTPTR)
+DEF_OV_TYPE (BT_OV_VOID_UV8HI_USHORTPTR_UINT, BT_VOID, BT_UV8HI, BT_USHORTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_V16QI_LONG_SCHARPTR, BT_VOID, BT_V16QI, BT_LONG, BT_SCHARPTR)
+DEF_OV_TYPE (BT_OV_VOID_V16QI_SCHARPTR_UINT, BT_VOID, BT_V16QI, BT_SCHARPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_V2DF_DBLPTR_UINT, BT_VOID, BT_V2DF, BT_DBLPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_V2DF_LONG_DBLPTR, BT_VOID, BT_V2DF, BT_LONG, BT_DBLPTR)
+DEF_OV_TYPE (BT_OV_VOID_V2DF_UV2DI_DBLPTR_ULONGLONG, BT_VOID, BT_V2DF, BT_UV2DI, BT_DBLPTR, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_VOID_V2DI_LONGLONGPTR_UINT, BT_VOID, BT_V2DI, BT_LONGLONGPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_V2DI_LONG_LONGLONGPTR, BT_VOID, BT_V2DI, BT_LONG, BT_LONGLONGPTR)
+DEF_OV_TYPE (BT_OV_VOID_V2DI_UV2DI_LONGLONGPTR_ULONGLONG, BT_VOID, BT_V2DI, BT_UV2DI, BT_LONGLONGPTR, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_VOID_V4SI_INTPTR_UINT, BT_VOID, BT_V4SI, BT_INTPTR, BT_UINT)
+DEF_OV_TYPE (BT_OV_VOID_V4SI_LONG_INTPTR, BT_VOID, BT_V4SI, BT_LONG, BT_INTPTR)
+DEF_OV_TYPE (BT_OV_VOID_V4SI_UV4SI_INTPTR_ULONGLONG, BT_VOID, BT_V4SI, BT_UV4SI, BT_INTPTR, BT_ULONGLONG)
+DEF_OV_TYPE (BT_OV_VOID_V8HI_LONG_SHORTPTR, BT_VOID, BT_V8HI, BT_LONG, BT_SHORTPTR)
+DEF_OV_TYPE (BT_OV_VOID_V8HI_SHORTPTR_UINT, BT_VOID, BT_V8HI, BT_SHORTPTR, BT_UINT)
--- gcc/config/s390/s390.c	2015-06-18 16:33:04.000000000 +0200
+++ gcc/config/s390/s390.c	2016-05-11 19:11:44.333028400 +0200
@@ -52,6 +52,10 @@ along with GCC; see the file COPYING3.
 #include "params.h"
 #include "cfgloop.h"
 #include "opts.h"
+#include "intl.h"
+#include "plugin-api.h"
+#include "cgraph.h"
+#include "tm-constrs.h"
 
 /* Define the specific costs for a given cpu.  */
 
@@ -288,6 +292,19 @@ extern int reload_completed;
 
 /* Kept up to date using the SCHED_VARIABLE_ISSUE hook.  */
 static rtx last_scheduled_insn;
+#define MAX_SCHED_UNITS 3
+static int last_scheduled_unit_distance[MAX_SCHED_UNITS];
+
+/* The maximum score added for an instruction whose unit hasn't been
+   in use for MAX_SCHED_MIX_DISTANCE steps.  Increase this value to
+   give instruction mix scheduling more priority over instruction
+   grouping.  */
+#define MAX_SCHED_MIX_SCORE      8
+
+/* The maximum distance up to which individual scores will be
+   calculated.  Everything beyond this gives MAX_SCHED_MIX_SCORE.
+   Increase this with the OOO windows size of the machine.  */
+#define MAX_SCHED_MIX_DISTANCE 100
 
 /* Structure used to hold the components of a S/390 memory
    address.  A legitimate address on S/390 is of the general
@@ -387,6 +404,7 @@ struct GTY(()) machine_function
 /* Number of GPRs and FPRs used for argument passing.  */
 #define GP_ARG_NUM_REG 5
 #define FP_ARG_NUM_REG (TARGET_64BIT? 4 : 2)
+#define VEC_ARG_NUM_REG 8
 
 /* A couple of shortcuts.  */
 #define CONST_OK_FOR_J(x) \
@@ -407,6 +425,539 @@ struct GTY(()) machine_function
    bytes on a z10 (or higher) CPU.  */
 #define PREDICT_DISTANCE (TARGET_Z10 ? 384 : 2048)
 
+
+/* Indicate which ABI has been used for passing vector args.
+   0 - no vector type arguments have been passed where the ABI is relevant
+   1 - the old ABI has been used
+   2 - a vector type argument has been passed either in a vector register
+       or on the stack by value  */
+static int s390_vector_abi = 0;
+
+/* Set the vector ABI marker if TYPE is subject to the vector ABI
+   switch.  The vector ABI affects only vector data types.  There are
+   two aspects of the vector ABI relevant here:
+
+   1. vectors >= 16 bytes have an alignment of 8 bytes with the new
+   ABI and natural alignment with the old.
+
+   2. vector <= 16 bytes are passed in VRs or by value on the stack
+   with the new ABI but by reference on the stack with the old.
+
+   If ARG_P is true TYPE is used for a function argument or return
+   value.  The ABI marker then is set for all vector data types.  If
+   ARG_P is false only type 1 vectors are being checked.  */
+
+static void
+s390_check_type_for_vector_abi (const_tree type, bool arg_p, bool in_struct_p)
+{
+  static htab_t visited_types_hash
+    = htab_create (37, htab_hash_pointer, htab_eq_pointer, free);
+  void **slot;
+
+  if (s390_vector_abi)
+    return;
+
+  if (type == NULL_TREE || TREE_CODE (type) == ERROR_MARK)
+    return;
+
+  slot = htab_find_slot (visited_types_hash, type, INSERT);
+  if (*slot)
+    return;
+
+  *slot = CONST_CAST_TREE (type);
+
+  if (TREE_CODE (type) == VECTOR_TYPE)
+    {
+      int type_size = int_size_in_bytes (type);
+
+      /* Outside arguments only the alignment is changing and this
+	 only happens for vector types >= 16 bytes.  */
+      if (!arg_p && type_size < 16)
+	return;
+
+      /* In arguments vector types > 16 are passed as before (GCC
+	 never enforced the bigger alignment for arguments which was
+	 required by the old vector ABI).  However, it might still be
+	 ABI relevant due to the changed alignment if it is a struct
+	 member.  */
+      if (arg_p && type_size > 16 && !in_struct_p)
+	return;
+
+      s390_vector_abi = TARGET_VX_ABI ? 2 : 1;
+    }
+  else if (POINTER_TYPE_P (type) || TREE_CODE (type) == ARRAY_TYPE)
+    {
+      /* ARRAY_TYPE: Since with neither of the ABIs we have more than
+	 natural alignment there will never be ABI dependent padding
+	 in an array type.  That's why we do not set in_struct_p to
+	 true here.  */
+      s390_check_type_for_vector_abi (TREE_TYPE (type), arg_p, in_struct_p);
+    }
+  else if (TREE_CODE (type) == FUNCTION_TYPE || TREE_CODE (type) == METHOD_TYPE)
+    {
+      tree arg_chain;
+
+      /* Check the return type.  */
+      s390_check_type_for_vector_abi (TREE_TYPE (type), true, false);
+
+      for (arg_chain = TYPE_ARG_TYPES (type);
+	   arg_chain;
+	   arg_chain = TREE_CHAIN (arg_chain))
+	s390_check_type_for_vector_abi (TREE_VALUE (arg_chain), true, false);
+    }
+  else if (RECORD_OR_UNION_TYPE_P (type))
+    {
+      tree field;
+
+      for (field = TYPE_FIELDS (type); field; field = DECL_CHAIN (field))
+	{
+	  if (TREE_CODE (field) != FIELD_DECL)
+	    continue;
+
+	  s390_check_type_for_vector_abi (TREE_TYPE (field), arg_p, true);
+	}
+    }
+}
+
+
+/* System z builtins.  */
+
+#include "s390-builtins.h"
+
+const unsigned int bflags_builtin[S390_BUILTIN_MAX + 1] =
+  {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(NAME, PATTERN, ATTRS, BFLAGS, ...) BFLAGS,
+#define OB_DEF(...)
+#define OB_DEF_VAR(...)
+#include "s390-builtins.def"
+    0
+  };
+
+const unsigned int opflags_builtin[S390_BUILTIN_MAX + 1] =
+  {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(NAME, PATTERN, ATTRS, BFLAGS, OPFLAGS, ...) OPFLAGS,
+#define OB_DEF(...)
+#define OB_DEF_VAR(...)
+#include "s390-builtins.def"
+    0
+  };
+
+const unsigned int bflags_overloaded_builtin[S390_OVERLOADED_BUILTIN_MAX + 1] =
+  {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(NAME, FIRST_VAR_NAME, LAST_VAR_NAME, BFLAGS, ...) BFLAGS,
+#define OB_DEF_VAR(...)
+#include "s390-builtins.def"
+    0
+  };
+
+const unsigned int
+opflags_overloaded_builtin_var[S390_OVERLOADED_BUILTIN_VAR_MAX + 1] =
+  {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(...)
+#define OB_DEF_VAR(NAME, PATTERN, FLAGS, FNTYPE) FLAGS,
+#include "s390-builtins.def"
+    0
+  };
+
+tree s390_builtin_types[BT_MAX];
+tree s390_builtin_fn_types[BT_FN_MAX];
+tree s390_builtin_decls[S390_BUILTIN_MAX +
+			S390_OVERLOADED_BUILTIN_MAX +
+			S390_OVERLOADED_BUILTIN_VAR_MAX];
+
+static enum insn_code const code_for_builtin[S390_BUILTIN_MAX + 1] = {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(NAME, PATTERN, ...) CODE_FOR_##PATTERN,
+#define OB_DEF(...)
+#define OB_DEF_VAR(...)
+
+#include "s390-builtins.def"
+  CODE_FOR_nothing
+};
+
+static void
+s390_init_builtins (void)
+{
+  /* These definitions are being used in s390-builtins.def.  */
+  tree returns_twice_attr = tree_cons (get_identifier ("returns_twice"),
+				       NULL, NULL);
+  tree noreturn_attr = tree_cons (get_identifier ("noreturn"), NULL, NULL);
+  tree c_uint64_type_node;
+  unsigned int bflags_mask = (BFLAGS_MASK_INIT);
+
+  bflags_mask |= (TARGET_VX)  ? B_VX  : 0;
+  bflags_mask |= (TARGET_HTM) ? B_HTM : 0;
+
+  /* The uint64_type_node from tree.c is not compatible to the C99
+     uint64_t data type.  What we want is c_uint64_type_node from
+     c-common.c.  But since backend code is not supposed to interface
+     with the frontend we recreate it here.  */
+  if (TARGET_64BIT)
+    c_uint64_type_node = long_unsigned_type_node;
+  else
+    c_uint64_type_node = long_long_unsigned_type_node;
+
+#undef DEF_TYPE
+#define DEF_TYPE(INDEX, BFLAGS, NODE, CONST_P)		\
+  if ((BFLAGS) == 0 || ((BFLAGS) & bflags_mask))	\
+    s390_builtin_types[INDEX] = (!CONST_P) ?		\
+      (NODE) : build_type_variant ((NODE), 1, 0);
+
+#undef DEF_POINTER_TYPE
+#define DEF_POINTER_TYPE(INDEX, BFLAGS, INDEX_BASE)			\
+  if ((BFLAGS) == 0 || ((BFLAGS) & bflags_mask))			\
+    s390_builtin_types[INDEX] =						\
+      build_pointer_type (s390_builtin_types[INDEX_BASE]);
+
+#undef DEF_DISTINCT_TYPE
+#define DEF_DISTINCT_TYPE(INDEX, BFLAGS, INDEX_BASE)			\
+  if ((BFLAGS) == 0 || ((BFLAGS) & bflags_mask))			\
+    s390_builtin_types[INDEX] =						\
+      build_distinct_type_copy (s390_builtin_types[INDEX_BASE]);
+
+#undef DEF_VECTOR_TYPE
+#define DEF_VECTOR_TYPE(INDEX, BFLAGS, INDEX_BASE, ELEMENTS)		\
+  if ((BFLAGS) == 0 || ((BFLAGS) & bflags_mask))			\
+    s390_builtin_types[INDEX] =						\
+      build_vector_type (s390_builtin_types[INDEX_BASE], ELEMENTS);
+
+#undef DEF_OPAQUE_VECTOR_TYPE
+#define DEF_OPAQUE_VECTOR_TYPE(INDEX, BFLAGS, INDEX_BASE, ELEMENTS)	\
+  if ((BFLAGS) == 0 || ((BFLAGS) & bflags_mask))			\
+    s390_builtin_types[INDEX] =						\
+      build_opaque_vector_type (s390_builtin_types[INDEX_BASE], ELEMENTS);
+
+#undef DEF_FN_TYPE
+#define DEF_FN_TYPE(INDEX, BFLAGS, args...)			\
+  if ((BFLAGS) == 0 || ((BFLAGS) & bflags_mask))		\
+    s390_builtin_fn_types[INDEX] =				\
+    build_function_type_list (args, NULL_TREE);
+#undef DEF_OV_TYPE
+#define DEF_OV_TYPE(...)
+#include "s390-builtin-types.def"
+
+#undef B_DEF
+#define B_DEF(NAME, PATTERN, ATTRS, BFLAGS, OPFLAGS, FNTYPE)		\
+  if (((BFLAGS) & ~bflags_mask) == 0)					\
+    s390_builtin_decls[S390_BUILTIN_##NAME] =				\
+      add_builtin_function ("__builtin_" #NAME,				\
+			    s390_builtin_fn_types[FNTYPE],		\
+			    S390_BUILTIN_##NAME,			\
+			    BUILT_IN_MD,				\
+			    NULL,					\
+			    ATTRS);
+#undef OB_DEF
+#define OB_DEF(NAME, FIRST_VAR_NAME, LAST_VAR_NAME, BFLAGS, FNTYPE)	\
+  if (((BFLAGS) & ~bflags_mask) == 0)					\
+    s390_builtin_decls[S390_OVERLOADED_BUILTIN_##NAME + S390_BUILTIN_MAX] = \
+      add_builtin_function ("__builtin_" #NAME,				\
+			    s390_builtin_fn_types[FNTYPE],		\
+			    S390_OVERLOADED_BUILTIN_##NAME + S390_BUILTIN_MAX, \
+			    BUILT_IN_MD,				\
+			    NULL,					\
+			    0);
+#undef OB_DEF_VAR
+#define OB_DEF_VAR(...)
+#include "s390-builtins.def"
+
+}
+
+/* Return true if ARG is appropriate as argument number ARGNUM of
+   builtin DECL.  The operand flags from s390-builtins.def have to
+   passed as OP_FLAGS.  */
+bool
+s390_const_operand_ok (tree arg, int argnum, int op_flags, tree decl)
+{
+  if (O_UIMM_P (op_flags))
+    {
+      int bitwidths[] = { 1, 2, 3, 4, 5, 8, 12, 16, 32 };
+      int bitwidth = bitwidths[op_flags - O_U1];
+
+      if (!host_integerp (arg, 1)
+	  || ((unsigned HOST_WIDE_INT) tree_low_cst (arg, 1)
+	      > ((unsigned HOST_WIDE_INT)1 << bitwidth) - 1))
+	{
+	  error("constant argument %d for builtin %qF is out of range (0.."
+		HOST_WIDE_INT_PRINT_UNSIGNED ")",
+		argnum, decl,
+		((unsigned HOST_WIDE_INT)1 << bitwidth) - 1);
+	  return false;
+	}
+    }
+
+  if (O_SIMM_P (op_flags))
+    {
+      int bitwidths[] = { 2, 3, 4, 5, 8, 12, 16, 32 };
+      int bitwidth = bitwidths[op_flags - O_S2];
+
+      if (!host_integerp (arg, 0)
+	  || tree_low_cst (arg, 0) < -((HOST_WIDE_INT)1 << (bitwidth - 1))
+	  || (tree_low_cst (arg, 0)
+	      > (((HOST_WIDE_INT)1 << (bitwidth - 1)) - 1)))
+	{
+	  error("constant argument %d for builtin %qF is out of range ("
+		HOST_WIDE_INT_PRINT_DEC ".."
+		HOST_WIDE_INT_PRINT_DEC ")",
+		argnum, decl,
+		-((HOST_WIDE_INT)1 << (bitwidth - 1)),
+		((HOST_WIDE_INT)1 << (bitwidth - 1)) - 1);
+	  return false;
+	}
+    }
+  return true;
+}
+
+/* Expand an expression EXP that calls a built-in function,
+   with result going to TARGET if that's convenient
+   (and in mode MODE if that's convenient).
+   SUBTARGET may be used as the target for computing one of EXP's operands.
+   IGNORE is nonzero if the value is to be ignored.  */
+
+static rtx
+s390_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,
+		     enum machine_mode mode ATTRIBUTE_UNUSED,
+		     int ignore ATTRIBUTE_UNUSED)
+{
+#define MAX_ARGS 5
+
+  tree fndecl = TREE_OPERAND (CALL_EXPR_FN (exp), 0);
+  unsigned int fcode = DECL_FUNCTION_CODE (fndecl);
+  enum insn_code icode;
+  rtx op[MAX_ARGS], pat;
+  int arity;
+  bool nonvoid;
+  tree arg;
+  call_expr_arg_iterator iter;
+  unsigned int all_op_flags = opflags_for_builtin (fcode);
+  enum machine_mode last_vec_mode = VOIDmode;
+
+  if (TARGET_DEBUG_ARG)
+    {
+      fprintf (stderr,
+	       "s390_expand_builtin, code = %4d, %s\n",
+	       (int)fcode, IDENTIFIER_POINTER (DECL_NAME (fndecl)));
+    }
+
+  if (fcode >= S390_OVERLOADED_BUILTIN_VAR_OFFSET
+      && fcode < S390_ALL_BUILTIN_MAX)
+    {
+      gcc_unreachable ();
+    }
+  else if (fcode < S390_OVERLOADED_BUILTIN_OFFSET)
+    {
+      icode = code_for_builtin[fcode];
+      /* Set a flag in the machine specific cfun part in order to support
+	 saving/restoring of FPRs.  */
+      if (fcode == S390_BUILTIN_tbegin || fcode == S390_BUILTIN_tbegin_retry)
+	cfun->machine->tbegin_p = true;
+    }
+  else if (fcode < S390_OVERLOADED_BUILTIN_VAR_OFFSET)
+    {
+      error ("Unresolved overloaded builtin");
+      return const0_rtx;
+    }
+  else
+    internal_error ("bad builtin fcode");
+
+  if (icode == 0)
+    internal_error ("bad builtin icode");
+
+  nonvoid = TREE_TYPE (TREE_TYPE (fndecl)) != void_type_node;
+
+  if (nonvoid)
+    {
+      enum machine_mode tmode = insn_data[icode].operand[0].mode;
+      if (!target
+	  || GET_MODE (target) != tmode
+	  || !(*insn_data[icode].operand[0].predicate) (target, tmode))
+	target = gen_reg_rtx (tmode);
+
+      /* There are builtins (e.g. vec_promote) with no vector
+	 arguments but an element selector.  So we have to also look
+	 at the vector return type when emitting the modulo
+	 operation.  */
+      if (VECTOR_MODE_P (insn_data[icode].operand[0].mode))
+	last_vec_mode = insn_data[icode].operand[0].mode;
+    }
+
+  arity = 0;
+  FOR_EACH_CALL_EXPR_ARG (arg, iter, exp)
+    {
+      const struct insn_operand_data *insn_op;
+      unsigned int op_flags = all_op_flags & ((1 << O_SHIFT) - 1);
+
+      all_op_flags = all_op_flags >> O_SHIFT;
+
+      if (arg == error_mark_node)
+	return NULL_RTX;
+      if (arity >= MAX_ARGS)
+	return NULL_RTX;
+
+      if (O_IMM_P (op_flags)
+	  && TREE_CODE (arg) != INTEGER_CST)
+	{
+	  error ("constant value required for builtin %qF argument %d",
+		 fndecl, arity + 1);
+	  return const0_rtx;
+	}
+
+      if (!s390_const_operand_ok (arg, arity + 1, op_flags, fndecl))
+	return const0_rtx;
+
+      insn_op = &insn_data[icode].operand[arity + nonvoid];
+      op[arity] = expand_expr (arg, NULL_RTX, insn_op->mode, EXPAND_NORMAL);
+
+      /* expand_expr truncates constants to the target mode only if it
+	 is "convenient".  However, our checks below rely on this
+	 being done.  */
+      if (CONST_INT_P (op[arity])
+	  && SCALAR_INT_MODE_P (insn_op->mode)
+	  && GET_MODE (op[arity]) != insn_op->mode)
+	op[arity] = GEN_INT (trunc_int_for_mode (INTVAL (op[arity]),
+						 insn_op->mode));
+
+      /* Wrap the expanded RTX for pointer types into a MEM expr with
+	 the proper mode.  This allows us to use e.g. (match_operand
+	 "memory_operand"..) in the insn patterns instead of (mem
+	 (match_operand "address_operand)).  This is helpful for
+	 patterns not just accepting MEMs.  */
+      if (POINTER_TYPE_P (TREE_TYPE (arg))
+	  && insn_op->predicate != address_operand)
+	op[arity] = gen_rtx_MEM (insn_op->mode, op[arity]);
+
+      /* Expand the module operation required on element selectors.  */
+      if (op_flags == O_ELEM)
+	{
+	  gcc_assert (last_vec_mode != VOIDmode);
+	  op[arity] = simplify_expand_binop (SImode, code_to_optab (AND),
+					     op[arity],
+					     GEN_INT (GET_MODE_NUNITS (last_vec_mode) - 1),
+					     NULL_RTX, 1, OPTAB_DIRECT);
+	}
+
+      /* Record the vector mode used for an element selector.  This assumes:
+	 1. There is no builtin with two different vector modes and an element selector
+         2. The element selector comes after the vector type it is referring to.
+	 This currently the true for all the builtins but FIXME we
+	 should better check for that.  */
+      if (VECTOR_MODE_P (insn_op->mode))
+	last_vec_mode = insn_op->mode;
+
+      if (insn_op->predicate (op[arity], insn_op->mode))
+	{
+	  arity++;
+	  continue;
+	}
+
+      if (MEM_P (op[arity])
+	  && insn_op->predicate == memory_operand
+	  && (GET_MODE (XEXP (op[arity], 0)) == Pmode
+	      || GET_MODE (XEXP (op[arity], 0)) == VOIDmode))
+	{
+	  op[arity] = replace_equiv_address (op[arity],
+					     copy_to_mode_reg (Pmode,
+					       XEXP (op[arity], 0)));
+	}
+      else if (GET_MODE (op[arity]) == insn_op->mode
+	       || GET_MODE (op[arity]) == VOIDmode
+	       || (insn_op->predicate == address_operand
+		   && GET_MODE (op[arity]) == Pmode))
+	{
+	  /* An address_operand usually has VOIDmode in the expander
+	     so we cannot use this.  */
+	  enum machine_mode target_mode =
+	    (insn_op->predicate == address_operand
+	     ? Pmode : insn_op->mode);
+	  op[arity] = copy_to_mode_reg (target_mode, op[arity]);
+	}
+
+      if (!insn_op->predicate (op[arity], insn_op->mode))
+	{
+	  error ("Invalid argument %d for builtin %qF", arity + 1, fndecl);
+	  return const0_rtx;
+	}
+      arity++;
+    }
+
+  if (last_vec_mode != VOIDmode && !TARGET_VX)
+    {
+      error ("Vector type builtin %qF is not supported without -mvx "
+	     "(default with -march=z13).",
+	     fndecl);
+      return const0_rtx;
+    }
+
+  switch (arity)
+    {
+    case 0:
+      pat = GEN_FCN (icode) (target);
+      break;
+    case 1:
+      if (nonvoid)
+        pat = GEN_FCN (icode) (target, op[0]);
+      else
+	pat = GEN_FCN (icode) (op[0]);
+      break;
+    case 2:
+      if (nonvoid)
+	pat = GEN_FCN (icode) (target, op[0], op[1]);
+      else
+	pat = GEN_FCN (icode) (op[0], op[1]);
+      break;
+    case 3:
+      if (nonvoid)
+	pat = GEN_FCN (icode) (target, op[0], op[1], op[2]);
+      else
+	pat = GEN_FCN (icode) (op[0], op[1], op[2]);
+      break;
+    case 4:
+      if (nonvoid)
+	pat = GEN_FCN (icode) (target, op[0], op[1], op[2], op[3]);
+      else
+	pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3]);
+      break;
+    case 5:
+      if (nonvoid)
+	pat = GEN_FCN (icode) (target, op[0], op[1], op[2], op[3], op[4]);
+      else
+	pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4]);
+      break;
+    case 6:
+      if (nonvoid)
+	pat = GEN_FCN (icode) (target, op[0], op[1], op[2], op[3], op[4], op[5]);
+      else
+	pat = GEN_FCN (icode) (op[0], op[1], op[2], op[3], op[4], op[5]);
+      break;
+    default:
+      gcc_unreachable ();
+    }
+  if (!pat)
+    return NULL_RTX;
+  emit_insn (pat);
+
+  if (nonvoid)
+    return target;
+  else
+    return const0_rtx;
+}
+
+
 static const int s390_hotpatch_hw_max = 1000000;
 static int s390_hotpatch_hw_before_label = 0;
 static int s390_hotpatch_hw_after_label = 0;
@@ -458,9 +1009,43 @@ s390_handle_hotpatch_attribute (tree *no
   return NULL_TREE;
 }
 
+/* Expand the s390_vector_bool type attribute.  */
+
+static tree
+s390_handle_vectorbool_attribute (tree *node, tree name ATTRIBUTE_UNUSED,
+				  tree args ATTRIBUTE_UNUSED,
+				  int flags ATTRIBUTE_UNUSED, bool *no_add_attrs)
+{
+  tree type = *node, result = NULL_TREE;
+  enum machine_mode mode;
+
+  while (POINTER_TYPE_P (type)
+	 || TREE_CODE (type) == FUNCTION_TYPE
+	 || TREE_CODE (type) == METHOD_TYPE
+	 || TREE_CODE (type) == ARRAY_TYPE)
+    type = TREE_TYPE (type);
+
+  mode = TYPE_MODE (type);
+  switch (mode)
+    {
+    case DImode: case V2DImode: result = s390_builtin_types[BT_BV2DI]; break;
+    case SImode: case V4SImode: result = s390_builtin_types[BT_BV4SI]; break;
+    case HImode: case V8HImode: result = s390_builtin_types[BT_BV8HI]; break;
+    case QImode: case V16QImode: result = s390_builtin_types[BT_BV16QI];
+    default: break;
+    }
+
+  *no_add_attrs = true;  /* No need to hang on to the attribute.  */
+
+  if (result)
+    *node = lang_hooks.types.reconstruct_complex_type (*node, result);
+
+  return NULL_TREE;
+}
+
 static const struct attribute_spec s390_attribute_table[] = {
-  { "hotpatch", 2, 2, true, false, false, s390_handle_hotpatch_attribute, false
-  },
+  { "hotpatch", 2, 2, true, false, false, s390_handle_hotpatch_attribute, false },
+  { "s390_vector_bool", 0, 0, false, true, false, s390_handle_vectorbool_attribute, true },
   /* End element.  */
   { NULL,        0, 0, false, false, false, NULL, false }
 };
@@ -524,6 +1109,35 @@ s390_scalar_mode_supported_p (enum machi
   return default_scalar_mode_supported_p (mode);
 }
 
+/* Return true if the back end supports vector mode MODE.  */
+static bool
+s390_vector_mode_supported_p (enum machine_mode mode)
+{
+  enum machine_mode inner;
+
+  if (!VECTOR_MODE_P (mode)
+      || !TARGET_VX
+      || GET_MODE_SIZE (mode) > 16)
+    return false;
+
+  inner = GET_MODE_INNER (mode);
+
+  switch (inner)
+    {
+    case QImode:
+    case HImode:
+    case SImode:
+    case DImode:
+    case TImode:
+    case SFmode:
+    case DFmode:
+    case TFmode:
+      return true;
+    default:
+      return false;
+    }
+}
+
 /* Set the has_landing_pad_p flag in struct machine_function to VALUE.  */
 
 void
@@ -595,6 +1209,11 @@ s390_match_ccmode_set (rtx set, enum mac
     case CCT1mode:
     case CCT2mode:
     case CCT3mode:
+    case CCVEQmode:
+    case CCVHmode:
+    case CCVHUmode:
+    case CCVFHmode:
+    case CCVFHEmode:
       if (req_mode != set_mode)
         return 0;
       break;
@@ -695,6 +1314,29 @@ s390_tm_ccmode (rtx op1, rtx op2, bool m
 enum machine_mode
 s390_select_ccmode (enum rtx_code code, rtx op0, rtx op1)
 {
+  if (TARGET_VX
+      && register_operand (op0, DFmode)
+      && register_operand (op1, DFmode))
+    {
+      /* LT, LE, UNGT, UNGE require swapping OP0 and OP1.  Either
+	 s390_emit_compare or s390_canonicalize_comparison will take
+	 care of it.  */
+      switch (code)
+	{
+	case EQ:
+	case NE:
+	  return CCVEQmode;
+	case GT:
+	case UNLE:
+	  return CCVFHmode;
+	case GE:
+	case UNLT:
+	  return CCVFHEmode;
+	default:
+	  ;
+	}
+    }
+
   switch (code)
     {
       case EQ:
@@ -972,8 +1614,73 @@ s390_canonicalize_comparison (int *code,
       rtx tem = *op0; *op0 = *op1; *op1 = tem;
       *code = (int)swap_condition ((enum rtx_code)*code);
     }
+
+  /* Using the scalar variants of vector instructions for 64 bit FP
+     comparisons might require swapping the operands.  */
+  if (TARGET_VX
+      && register_operand (*op0, DFmode)
+      && register_operand (*op1, DFmode)
+      && (*code == LT || *code == LE || *code == UNGT || *code == UNGE))
+    {
+      rtx tmp;
+
+      switch (*code)
+	{
+	case LT:   *code = GT; break;
+	case LE:   *code = GE; break;
+	case UNGT: *code = UNLE; break;
+	case UNGE: *code = UNLT; break;
+	default: ;
+	}
+      tmp = *op0; *op0 = *op1; *op1 = tmp;
+    }
+}
+
+/* Helper function for s390_emit_compare.  If possible emit a 64 bit
+   FP compare using the single element variant of vector instructions.
+   Replace CODE with the comparison code to be used in the CC reg
+   compare and return the condition code register RTX in CC.  */
+
+static bool
+s390_expand_vec_compare_scalar (enum rtx_code *code, rtx cmp1, rtx cmp2,
+				rtx *cc)
+{
+  enum machine_mode cmp_mode;
+  bool swap_p = false;
+
+  switch (*code)
+    {
+    case EQ:   cmp_mode = CCVEQmode;  break;
+    case NE:   cmp_mode = CCVEQmode;  break;
+    case GT:   cmp_mode = CCVFHmode;  break;
+    case GE:   cmp_mode = CCVFHEmode; break;
+    case UNLE: cmp_mode = CCVFHmode;  break;
+    case UNLT: cmp_mode = CCVFHEmode; break;
+    case LT:   cmp_mode = CCVFHmode;  *code = GT;   swap_p = true; break;
+    case LE:   cmp_mode = CCVFHEmode; *code = GE;   swap_p = true; break;
+    case UNGE: cmp_mode = CCVFHmode;  *code = UNLE; swap_p = true; break;
+    case UNGT: cmp_mode = CCVFHEmode; *code = UNLT; swap_p = true; break;
+    default: return false;
+    }
+
+  if (swap_p)
+    {
+      rtx tmp = cmp2;
+      cmp2 = cmp1;
+      cmp1 = tmp;
+    }
+  *cc = gen_rtx_REG (cmp_mode, CC_REGNUM);
+  emit_insn (gen_rtx_PARALLEL (VOIDmode,
+	       gen_rtvec (2,
+			  gen_rtx_SET (VOIDmode, *cc,
+				       gen_rtx_COMPARE (cmp_mode, cmp1,
+							cmp2)),
+			  gen_rtx_CLOBBER (VOIDmode,
+					   gen_rtx_SCRATCH (V2DImode)))));
+  return true;
 }
 
+
 /* Emit a compare instruction suitable to implement the comparison
    OP0 CODE OP1.  Return the correct condition RTL to be placed in
    the IF_THEN_ELSE of the conditional branch testing the result.  */
@@ -984,10 +1691,18 @@ s390_emit_compare (enum rtx_code code, r
   enum machine_mode mode = s390_select_ccmode (code, op0, op1);
   rtx cc;
 
-  /* Do not output a redundant compare instruction if a compare_and_swap
-     pattern already computed the result and the machine modes are compatible.  */
-  if (GET_MODE_CLASS (GET_MODE (op0)) == MODE_CC)
-    {
+  if (TARGET_VX
+      && register_operand (op0, DFmode)
+      && register_operand (op1, DFmode)
+      && s390_expand_vec_compare_scalar (&code, op0, op1, &cc))
+    {
+      /* Work has been done by s390_expand_vec_compare_scalar already.  */
+    }
+  else if (GET_MODE_CLASS (GET_MODE (op0)) == MODE_CC)
+    {
+      /* Do not output a redundant compare instruction if a
+	 compare_and_swap pattern already computed the result and the
+	 machine modes are compatible.  */
       gcc_assert (s390_cc_modes_compatible (GET_MODE (op0), mode)
 		  == GET_MODE (op0));
       cc = op0;
@@ -1222,6 +1937,93 @@ s390_branch_condition_mask (rtx code)
         }
       break;
 
+      /* Vector comparison modes.  */
+
+    case CCVEQmode:
+      switch (GET_CODE (code))
+	{
+	case EQ:        return CC0;
+	case NE:        return CC3;
+	default:        return -1;
+	}
+
+    case CCVEQANYmode:
+      switch (GET_CODE (code))
+	{
+	case EQ:        return CC0 | CC1;
+	case NE:        return CC3 | CC1;
+	default:        return -1;
+	}
+
+      /* Integer vector compare modes.  */
+
+    case CCVHmode:
+      switch (GET_CODE (code))
+	{
+	case GT:        return CC0;
+	case LE:        return CC3;
+	default:        return -1;
+	}
+
+    case CCVHANYmode:
+      switch (GET_CODE (code))
+	{
+	case GT:        return CC0 | CC1;
+	case LE:        return CC3 | CC1;
+	default:        return -1;
+	}
+
+    case CCVHUmode:
+      switch (GET_CODE (code))
+	{
+	case GTU:       return CC0;
+	case LEU:       return CC3;
+	default:        return -1;
+	}
+
+    case CCVHUANYmode:
+      switch (GET_CODE (code))
+	{
+	case GTU:       return CC0 | CC1;
+	case LEU:       return CC3 | CC1;
+	default:        return -1;
+	}
+
+      /* FP vector compare modes.  */
+
+    case CCVFHmode:
+      switch (GET_CODE (code))
+	{
+	case GT:        return CC0;
+	case UNLE:      return CC3;
+	default:        return -1;
+	}
+
+    case CCVFHANYmode:
+      switch (GET_CODE (code))
+	{
+	case GT:        return CC0 | CC1;
+	case UNLE:      return CC3 | CC1;
+	default:        return -1;
+	}
+
+    case CCVFHEmode:
+      switch (GET_CODE (code))
+	{
+	case GE:        return CC0;
+	case UNLT:      return CC3;
+	default:        return -1;
+	}
+
+    case CCVFHEANYmode:
+      switch (GET_CODE (code))
+	{
+	case GE:        return CC0 | CC1;
+	case UNLT:      return CC3 | CC1;
+	default:        return -1;
+	}
+
+
     case CCRAWmode:
       switch (GET_CODE (code))
 	{
@@ -1421,6 +2223,9 @@ s390_contiguous_bitmask_p (unsigned HOST
   /* Calculate a mask for all bits beyond the contiguous bits.  */
   mask = (-1LL & ~(((1ULL << (tmp_length + tmp_pos - 1)) << 1) - 1));
 
+  if ((unsigned)size < sizeof (HOST_WIDE_INT) * BITS_PER_UNIT)
+    mask &= ((unsigned HOST_WIDE_INT) 1 << size) - 1;
+
   if (mask & in)
     return false;
 
@@ -1436,6 +2241,128 @@ s390_contiguous_bitmask_p (unsigned HOST
   return true;
 }
 
+/* Return true if OP is a constant vector with the same constant in
+   all its elements.  */
+
+bool
+s390_const_vec_duplicate_p (rtx op)
+{
+  if (!VECTOR_MODE_P (GET_MODE (op))
+      || GET_CODE (op) != CONST_VECTOR
+      || !CONST_INT_P (XVECEXP (op, 0, 0)))
+    return false;
+
+  if (GET_MODE_NUNITS (GET_MODE (op)) > 1)
+    {
+      int i;
+
+      for (i = 1; i < GET_MODE_NUNITS (GET_MODE (op)); ++i)
+	if (!rtx_equal_p (XVECEXP (op, 0, i), XVECEXP (op, 0, 0)))
+	  return false;
+    }
+  return true;
+}
+
+/* Return true if OP contains the same contiguous bitfield in *all*
+   its elements.  START and END can be used to obtain the start and
+   end position of the bitfield.
+
+   START/STOP give the position of the first/last bit of the bitfield
+   counting from the lowest order bit starting with zero.  In order to
+   use these values for S/390 instructions this has to be converted to
+   "bits big endian" style.  */
+
+bool
+s390_contiguous_bitmask_vector_p (rtx op, int *start, int *end)
+{
+  unsigned HOST_WIDE_INT mask;
+  int length, size;
+
+  if (!VECTOR_MODE_P (GET_MODE (op))
+      || GET_CODE (op) != CONST_VECTOR
+      || !CONST_INT_P (XVECEXP (op, 0, 0)))
+    return false;
+
+  if (GET_MODE_NUNITS (GET_MODE (op)) > 1)
+    {
+      int i;
+
+      for (i = 1; i < GET_MODE_NUNITS (GET_MODE (op)); ++i)
+	if (!rtx_equal_p (XVECEXP (op, 0, i), XVECEXP (op, 0, 0)))
+	  return false;
+    }
+
+  size = GET_MODE_UNIT_BITSIZE (GET_MODE (op));
+
+  /* We cannot deal with V1TI/V1TF. This would require a vgmq.  */
+  if (size > 64)
+    return false;
+
+  mask = UINTVAL (XVECEXP (op, 0, 0));
+  if (s390_contiguous_bitmask_p (mask, size, start,
+				 end != NULL ? &length : NULL))
+    {
+      if (end != NULL)
+	*end = *start + length - 1;
+      return true;
+    }
+  /* 0xff00000f style immediates can be covered by swapping start and
+     end indices in vgm.  */
+  if (s390_contiguous_bitmask_p (~mask, size, start,
+				 end != NULL ? &length : NULL))
+    {
+      if (end != NULL)
+	*end = *start - 1;
+      if (start != NULL)
+	*start = *start + length;
+      return true;
+    }
+  return false;
+}
+
+/* Return true if C consists only of byte chunks being either 0 or
+   0xff.  If MASK is !=NULL a byte mask is generated which is
+   appropriate for the vector generate byte mask instruction.  */
+
+bool
+s390_bytemask_vector_p (rtx op, unsigned *mask)
+{
+  int i;
+  unsigned tmp_mask = 0;
+  int nunit, unit_size;
+
+  if (!VECTOR_MODE_P (GET_MODE (op))
+      || GET_CODE (op) != CONST_VECTOR
+      || !CONST_INT_P (XVECEXP (op, 0, 0)))
+    return false;
+
+  nunit = GET_MODE_NUNITS (GET_MODE (op));
+  unit_size = GET_MODE_UNIT_SIZE (GET_MODE (op));
+
+  for (i = 0; i < nunit; i++)
+    {
+      unsigned HOST_WIDE_INT c;
+      int j;
+
+      if (!CONST_INT_P (XVECEXP (op, 0, i)))
+	return false;
+
+      c = UINTVAL (XVECEXP (op, 0, i));
+      for (j = 0; j < unit_size; j++)
+	{
+	  if ((c & 0xff) != 0 && (c & 0xff) != 0xff)
+	    return false;
+	  tmp_mask |= (c & 1) << ((nunit - 1 - i) * unit_size + j);
+	  c = c >> BITS_PER_UNIT;
+	}
+    }
+
+  if (mask != NULL)
+    *mask = tmp_mask;
+
+  return true;
+}
+
 /* Check whether a rotate of ROTL followed by an AND of CONTIG is
    equivalent to a shift followed by the AND.  In particular, CONTIG
    should not overlap the (rotated) bit 0/bit 63 gap.  Negative values
@@ -1461,8 +2388,8 @@ s390_extzv_shift_ok (int bitsize, int ro
 bool
 s390_split_ok_p (rtx dst, rtx src, enum machine_mode mode, int first_subword)
 {
-  /* Floating point registers cannot be split.  */
-  if (FP_REG_P (src) || FP_REG_P (dst))
+  /* Floating point and vector registers cannot be split.  */
+  if (FP_REG_P (src) || FP_REG_P (dst) || VECTOR_REG_P (src) || VECTOR_REG_P (dst))
     return false;
 
   /* We don't need to split if operands are directly accessible.  */
@@ -1752,6 +2679,22 @@ s390_option_override (void)
   if (!(target_flags_explicit & MASK_OPT_HTM) && TARGET_CPU_HTM && TARGET_ZARCH)
     target_flags |= MASK_OPT_HTM;
 
+  if (target_flags_explicit & MASK_OPT_VX)
+    {
+      if (TARGET_OPT_VX)
+	{
+	  if (!TARGET_CPU_VX)
+	    error ("hardware vector support not available on %s",
+		   s390_arch_string);
+	  if (TARGET_SOFT_FLOAT)
+	    error ("hardware vector support not available with -msoft-float");
+	}
+    }
+  else if (TARGET_CPU_VX)
+    /* Enable vector support if available and not explicitly disabled
+       by user.  E.g. with -m31 -march=z13 -mzarch */
+    target_flags |= MASK_OPT_VX;
+
   if (TARGET_HARD_DFP && !TARGET_DFP)
     {
       if (target_flags_explicit & MASK_HARD_DFP)
@@ -1791,6 +2734,7 @@ s390_option_override (void)
       s390_cost = &z196_cost;
       break;
     case PROCESSOR_2827_ZEC12:
+    case PROCESSOR_2964_Z13:
       s390_cost = &zEC12_cost;
       break;
     default:
@@ -1818,7 +2762,8 @@ s390_option_override (void)
 
   if (s390_tune == PROCESSOR_2097_Z10
       || s390_tune == PROCESSOR_2817_Z196
-      || s390_tune == PROCESSOR_2827_ZEC12)
+      || s390_tune == PROCESSOR_2827_ZEC12
+      || s390_tune == PROCESSOR_2964_Z13)
     {
       maybe_set_param_value (PARAM_MAX_UNROLLED_INSNS, 100,
 			     global_options.x_param_values,
@@ -1882,16 +2827,20 @@ s390_option_override (void)
 /* Map for smallest class containing reg regno.  */
 
 const enum reg_class regclass_map[FIRST_PSEUDO_REGISTER] =
-{ GENERAL_REGS, ADDR_REGS, ADDR_REGS, ADDR_REGS,
-  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,
-  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,
-  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,
-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,
-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,
-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,
-  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,
-  ADDR_REGS,    CC_REGS,   ADDR_REGS, ADDR_REGS,
-  ACCESS_REGS,	ACCESS_REGS
+{ GENERAL_REGS, ADDR_REGS, ADDR_REGS, ADDR_REGS,  /*  0 */
+  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,  /*  4 */
+  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,  /*  8 */
+  ADDR_REGS,    ADDR_REGS, ADDR_REGS, ADDR_REGS,  /* 12 */
+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 16 */
+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 20 */
+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 24 */
+  FP_REGS,      FP_REGS,   FP_REGS,   FP_REGS,    /* 28 */
+  ADDR_REGS,    CC_REGS,   ADDR_REGS, ADDR_REGS,  /* 32 */
+  ACCESS_REGS,	ACCESS_REGS, VEC_REGS, VEC_REGS,  /* 36 */
+  VEC_REGS, VEC_REGS, VEC_REGS, VEC_REGS,         /* 40 */
+  VEC_REGS, VEC_REGS, VEC_REGS, VEC_REGS,         /* 44 */
+  VEC_REGS, VEC_REGS, VEC_REGS, VEC_REGS,         /* 48 */
+  VEC_REGS, VEC_REGS                              /* 52 */
 };
 
 /* Return attribute type of insn.  */
@@ -2933,6 +3882,19 @@ legitimate_pic_operand_p (rtx op)
 static bool
 s390_legitimate_constant_p (enum machine_mode mode, rtx op)
 {
+  if (TARGET_VX && VECTOR_MODE_P (mode) && GET_CODE (op) == CONST_VECTOR)
+    {
+      if (GET_MODE_SIZE (mode) != 16)
+	return 0;
+
+      if (!satisfies_constraint_j00 (op)
+	  && !satisfies_constraint_jm1 (op)
+	  && !satisfies_constraint_jKK (op)
+	  && !satisfies_constraint_jxx (op)
+	  && !satisfies_constraint_jyy (op))
+	return 0;
+    }
+
   /* Accept all non-symbolic constants.  */
   if (!SYMBOLIC_CONST (op))
     return 1;
@@ -2969,6 +3931,7 @@ s390_cannot_force_const_mem (enum machin
     {
     case CONST_INT:
     case CONST_DOUBLE:
+    case CONST_VECTOR:
       /* Accept all non-symbolic constants.  */
       return false;
 
@@ -3101,6 +4064,25 @@ legitimate_reload_fp_constant_p (rtx op)
   return false;
 }
 
+/* Returns true if the constant value OP is a legitimate vector operand
+   during and after reload.
+   This function accepts all constants which can be loaded directly
+   into an VR.  */
+
+static bool
+legitimate_reload_vector_constant_p (rtx op)
+{
+  if (TARGET_VX && GET_MODE_SIZE (GET_MODE (op)) == 16
+      && (satisfies_constraint_j00 (op)
+	  || satisfies_constraint_jm1 (op)
+	  || satisfies_constraint_jKK (op)
+	  || satisfies_constraint_jxx (op)
+	  || satisfies_constraint_jyy (op)))
+    return true;
+
+  return false;
+}
+
 /* Given an rtx OP being reloaded into a reg required to be in class RCLASS,
    return the class of reg to actually use.  */
 
@@ -3111,6 +4093,7 @@ s390_preferred_reload_class (rtx op, reg
     {
       /* Constants we cannot reload into general registers
 	 must be forced into the literal pool.  */
+      case CONST_VECTOR:
       case CONST_DOUBLE:
       case CONST_INT:
 	if (reg_class_subset_p (GENERAL_REGS, rclass)
@@ -3122,6 +4105,10 @@ s390_preferred_reload_class (rtx op, reg
 	else if (reg_class_subset_p (FP_REGS, rclass)
 		 && legitimate_reload_fp_constant_p (op))
 	  return FP_REGS;
+	else if (reg_class_subset_p (VEC_REGS, rclass)
+		 && legitimate_reload_vector_constant_p (op))
+	  return VEC_REGS;
+
 	return NO_REGS;
 
       /* If a symbolic constant or a PLUS is reloaded,
@@ -3245,6 +4232,7 @@ s390_reload_symref_address (rtx reg, rtx
   /* Reload might have pulled a constant out of the literal pool.
      Force it back in.  */
   if (CONST_INT_P (mem) || GET_CODE (mem) == CONST_DOUBLE
+      || GET_CODE (mem) == CONST_VECTOR
       || GET_CODE (mem) == CONST)
     mem = force_const_mem (GET_MODE (reg), mem);
 
@@ -3284,6 +4272,30 @@ s390_secondary_reload (bool in_p, rtx x,
   if (reg_classes_intersect_p (CC_REGS, rclass))
     return GENERAL_REGS;
 
+  if (TARGET_VX)
+    {
+      /* The vst/vl vector move instructions allow only for short
+	 displacements.  */
+      if (MEM_P (x)
+	  && GET_CODE (XEXP (x, 0)) == PLUS
+	  && GET_CODE (XEXP (XEXP (x, 0), 1)) == CONST_INT
+	  && !SHORT_DISP_IN_RANGE(INTVAL (XEXP (XEXP (x, 0), 1)))
+	  && reg_class_subset_p (rclass, VEC_REGS)
+	  && (!reg_class_subset_p (rclass, FP_REGS)
+	      || (GET_MODE_SIZE (mode) > 8
+		  && s390_class_max_nregs (FP_REGS, mode) == 1)))
+	{
+	  if (in_p)
+	    sri->icode = (TARGET_64BIT ?
+			  CODE_FOR_reloaddi_la_in :
+			  CODE_FOR_reloadsi_la_in);
+	  else
+	    sri->icode = (TARGET_64BIT ?
+			  CODE_FOR_reloaddi_la_out :
+			  CODE_FOR_reloadsi_la_out);
+	}
+    }
+
   if (TARGET_Z10)
     {
       HOST_WIDE_INT offset;
@@ -3299,17 +4311,15 @@ s390_secondary_reload (bool in_p, rtx x,
 	sri->icode = ((mode == DImode) ? CODE_FOR_reloaddi_larl_odd_addend_z10
 		      : CODE_FOR_reloadsi_larl_odd_addend_z10);
 
-      /* On z10 we need a scratch register when moving QI, TI or floating
-	 point mode values from or to a memory location with a SYMBOL_REF
-	 or if the symref addend of a SI or DI move is not aligned to the
-	 width of the access.  */
+      /* Handle all the (mem (symref)) accesses we cannot use the z10
+	 instructions for.  */
       if (MEM_P (x)
 	  && s390_loadrelative_operand_p (XEXP (x, 0), NULL, NULL)
-	  && (mode == QImode || mode == TImode || FLOAT_MODE_P (mode)
-	      || (!TARGET_ZARCH && mode == DImode)
-	      || ((mode == HImode || mode == SImode || mode == DImode)
-		  && (!s390_check_symref_alignment (XEXP (x, 0),
-						    GET_MODE_SIZE (mode))))))
+	  && (mode == QImode
+	      || !reg_class_subset_p (rclass, GENERAL_REGS)
+	      || GET_MODE_SIZE (mode) > UNITS_PER_WORD
+	      || !s390_check_symref_alignment (XEXP (x, 0),
+					       GET_MODE_SIZE (mode))))
 	{
 #define __SECONDARY_RELOAD_CASE(M,m)					\
 	  case M##mode:							\
@@ -3334,7 +4344,27 @@ s390_secondary_reload (bool in_p, rtx x,
 	      __SECONDARY_RELOAD_CASE (SD, sd);
 	      __SECONDARY_RELOAD_CASE (DD, dd);
 	      __SECONDARY_RELOAD_CASE (TD, td);
-
+	      __SECONDARY_RELOAD_CASE (V1QI, v1qi);
+	      __SECONDARY_RELOAD_CASE (V2QI, v2qi);
+	      __SECONDARY_RELOAD_CASE (V4QI, v4qi);
+	      __SECONDARY_RELOAD_CASE (V8QI, v8qi);
+	      __SECONDARY_RELOAD_CASE (V16QI, v16qi);
+	      __SECONDARY_RELOAD_CASE (V1HI, v1hi);
+	      __SECONDARY_RELOAD_CASE (V2HI, v2hi);
+	      __SECONDARY_RELOAD_CASE (V4HI, v4hi);
+	      __SECONDARY_RELOAD_CASE (V8HI, v8hi);
+	      __SECONDARY_RELOAD_CASE (V1SI, v1si);
+	      __SECONDARY_RELOAD_CASE (V2SI, v2si);
+	      __SECONDARY_RELOAD_CASE (V4SI, v4si);
+	      __SECONDARY_RELOAD_CASE (V1DI, v1di);
+	      __SECONDARY_RELOAD_CASE (V2DI, v2di);
+	      __SECONDARY_RELOAD_CASE (V1TI, v1ti);
+	      __SECONDARY_RELOAD_CASE (V1SF, v1sf);
+	      __SECONDARY_RELOAD_CASE (V2SF, v2sf);
+	      __SECONDARY_RELOAD_CASE (V4SF, v4sf);
+	      __SECONDARY_RELOAD_CASE (V1DF, v1df);
+	      __SECONDARY_RELOAD_CASE (V2DF, v2df);
+	      __SECONDARY_RELOAD_CASE (V1TF, v1tf);
 	    default:
 	      gcc_unreachable ();
 	    }
@@ -3371,12 +4401,12 @@ s390_secondary_reload (bool in_p, rtx x,
 	{
 	  if (in_p)
 	    sri->icode = (TARGET_64BIT ?
-			  CODE_FOR_reloaddi_nonoffmem_in :
-			  CODE_FOR_reloadsi_nonoffmem_in);
+			  CODE_FOR_reloaddi_la_in :
+			  CODE_FOR_reloadsi_la_in);
 	  else
 	    sri->icode = (TARGET_64BIT ?
-			  CODE_FOR_reloaddi_nonoffmem_out :
-			  CODE_FOR_reloadsi_nonoffmem_out);
+			  CODE_FOR_reloaddi_la_out :
+			  CODE_FOR_reloadsi_la_out);
 	}
     }
 
@@ -4610,6 +5640,256 @@ s390_expand_cmpmem (rtx target, rtx op0,
   return true;
 }
 
+/* Emit a conditional jump to LABEL for condition code mask MASK using
+   comparsion operator COMPARISON.  Return the emitted jump insn.  */
+
+static rtx
+s390_emit_ccraw_jump (HOST_WIDE_INT mask, enum rtx_code comparison, rtx label)
+{
+  rtx temp;
+
+  gcc_assert (comparison == EQ || comparison == NE);
+  gcc_assert (mask > 0 && mask < 15);
+
+  temp = gen_rtx_fmt_ee (comparison, VOIDmode,
+			 gen_rtx_REG (CCRAWmode, CC_REGNUM), GEN_INT (mask));
+  temp = gen_rtx_IF_THEN_ELSE (VOIDmode, temp,
+			       gen_rtx_LABEL_REF (VOIDmode, label), pc_rtx);
+  temp = gen_rtx_SET (VOIDmode, pc_rtx, temp);
+  return emit_jump_insn (temp);
+}
+
+/* Emit the instructions to implement strlen of STRING and store the
+   result in TARGET.  The string has the known ALIGNMENT.  This
+   version uses vector instructions and is therefore not appropriate
+   for targets prior to z13.  */
+
+void
+s390_expand_vec_strlen (rtx target, rtx string, rtx alignment)
+{
+  int very_unlikely = REG_BR_PROB_BASE / 100 - 1;
+  int very_likely = REG_BR_PROB_BASE - 1;
+  rtx highest_index_to_load_reg = gen_reg_rtx (Pmode);
+  rtx str_reg = gen_reg_rtx (V16QImode);
+  rtx str_addr_base_reg = gen_reg_rtx (Pmode);
+  rtx str_idx_reg = gen_reg_rtx (Pmode);
+  rtx result_reg = gen_reg_rtx (V16QImode);
+  rtx is_aligned_label = gen_label_rtx ();
+  rtx into_loop_label = NULL_RTX;
+  rtx loop_start_label = gen_label_rtx ();
+  rtx temp;
+  rtx len = gen_reg_rtx (QImode);
+  rtx cond;
+
+  s390_load_address (str_addr_base_reg, XEXP (string, 0));
+  emit_move_insn (str_idx_reg, const0_rtx);
+
+  if (INTVAL (alignment) < 16)
+    {
+      /* Check whether the address happens to be aligned properly so
+	 jump directly to the aligned loop.  */
+      emit_cmp_and_jump_insns (gen_rtx_AND (Pmode,
+					    str_addr_base_reg, GEN_INT (15)),
+			       const0_rtx, EQ, NULL_RTX,
+			       Pmode, 1, is_aligned_label);
+
+      temp = gen_reg_rtx (Pmode);
+      temp = expand_binop (Pmode, and_optab, str_addr_base_reg,
+			   GEN_INT (15), temp, 1, OPTAB_DIRECT);
+      gcc_assert (REG_P (temp));
+      highest_index_to_load_reg =
+	expand_binop (Pmode, sub_optab, GEN_INT (15), temp,
+		      highest_index_to_load_reg, 1, OPTAB_DIRECT);
+      gcc_assert (REG_P (highest_index_to_load_reg));
+      emit_insn (gen_vllv16qi (str_reg,
+		   convert_to_mode (SImode, highest_index_to_load_reg, 1),
+		   gen_rtx_MEM (BLKmode, str_addr_base_reg)));
+
+      into_loop_label = gen_label_rtx ();
+      s390_emit_jump (into_loop_label, NULL_RTX);
+      emit_barrier ();
+    }
+
+  emit_label (is_aligned_label);
+  LABEL_NUSES (is_aligned_label) = INTVAL (alignment) < 16 ? 2 : 1;
+
+  /* Reaching this point we are only performing 16 bytes aligned
+     loads.  */
+  emit_move_insn (highest_index_to_load_reg, GEN_INT (15));
+
+  emit_label (loop_start_label);
+  LABEL_NUSES (loop_start_label) = 1;
+
+  /* Load 16 bytes of the string into VR.  */
+  emit_move_insn (str_reg,
+		  gen_rtx_MEM (V16QImode,
+			       gen_rtx_PLUS (Pmode, str_idx_reg,
+					     str_addr_base_reg)));
+  if (into_loop_label != NULL_RTX)
+    {
+      emit_label (into_loop_label);
+      LABEL_NUSES (into_loop_label) = 1;
+    }
+
+  /* Increment string index by 16 bytes.  */
+  expand_binop (Pmode, add_optab, str_idx_reg, GEN_INT (16),
+		str_idx_reg, 1, OPTAB_DIRECT);
+
+  emit_insn (gen_vec_vfenesv16qi (result_reg, str_reg, str_reg,
+				  GEN_INT (VSTRING_FLAG_ZS | VSTRING_FLAG_CS)));
+
+  add_reg_note (s390_emit_ccraw_jump (8, NE, loop_start_label),
+		REG_BR_PROB, GEN_INT (very_likely));
+  emit_insn (gen_vec_extractv16qi (len, result_reg, GEN_INT (7)));
+
+  /* If the string pointer wasn't aligned we have loaded less then 16
+     bytes and the remaining bytes got filled with zeros (by vll).
+     Now we have to check whether the resulting index lies within the
+     bytes actually part of the string.  */
+
+  cond = s390_emit_compare (GT, convert_to_mode (Pmode, len, 1),
+			    highest_index_to_load_reg);
+  s390_load_address (highest_index_to_load_reg,
+		     gen_rtx_PLUS (Pmode, highest_index_to_load_reg,
+				   const1_rtx));
+  if (TARGET_64BIT)
+    emit_insn (gen_movdicc (str_idx_reg, cond,
+			    highest_index_to_load_reg, str_idx_reg));
+  else
+    emit_insn (gen_movsicc (str_idx_reg, cond,
+			    highest_index_to_load_reg, str_idx_reg));
+
+  add_reg_note (s390_emit_jump (is_aligned_label, cond), REG_BR_PROB,
+		GEN_INT (very_unlikely));
+
+  expand_binop (Pmode, add_optab, str_idx_reg,
+		GEN_INT (-16), str_idx_reg, 1, OPTAB_DIRECT);
+  /* FIXME: len is already zero extended - so avoid the llgcr emitted
+     here.  */
+  temp = expand_binop (Pmode, add_optab, str_idx_reg,
+		       convert_to_mode (Pmode, len, 1),
+		       target, 1, OPTAB_DIRECT);
+  if (temp != target)
+    emit_move_insn (target, temp);
+}
+
+void
+s390_expand_vec_movstr (rtx result, rtx dst, rtx src)
+{
+  int very_unlikely = REG_BR_PROB_BASE / 100 - 1;
+  rtx temp = gen_reg_rtx (Pmode);
+  rtx src_addr = XEXP (src, 0);
+  rtx dst_addr = XEXP (dst, 0);
+  rtx src_addr_reg = gen_reg_rtx (Pmode);
+  rtx dst_addr_reg = gen_reg_rtx (Pmode);
+  rtx offset = gen_reg_rtx (Pmode);
+  rtx vsrc = gen_reg_rtx (V16QImode);
+  rtx vpos = gen_reg_rtx (V16QImode);
+  rtx loadlen = gen_reg_rtx (SImode);
+  rtx gpos_qi = gen_reg_rtx(QImode);
+  rtx gpos = gen_reg_rtx (SImode);
+  rtx done_label = gen_label_rtx ();
+  rtx loop_label = gen_label_rtx ();
+  rtx exit_label = gen_label_rtx ();
+  rtx full_label = gen_label_rtx ();
+
+  /* Perform a quick check for string ending on the first up to 16
+     bytes and exit early if successful.  */
+
+  emit_insn (gen_vlbb (vsrc, src, GEN_INT (6)));
+  emit_insn (gen_lcbb (loadlen, src_addr, GEN_INT (6)));
+  emit_insn (gen_vfenezv16qi (vpos, vsrc, vsrc));
+  emit_insn (gen_vec_extractv16qi (gpos_qi, vpos, GEN_INT (7)));
+  emit_move_insn (gpos, gen_rtx_SUBREG (SImode, gpos_qi, 0));
+  /* gpos is the byte index if a zero was found and 16 otherwise.
+     So if it is lower than the loaded bytes we have a hit.  */
+  emit_cmp_and_jump_insns (gpos, loadlen, GE, NULL_RTX, SImode, 1,
+			   full_label);
+  emit_insn (gen_vstlv16qi (vsrc, gpos, dst));
+
+  force_expand_binop (Pmode, add_optab, dst_addr, gpos, result,
+		      1, OPTAB_DIRECT);
+  emit_jump (exit_label);
+  emit_barrier ();
+
+  emit_label (full_label);
+  LABEL_NUSES (full_label) = 1;
+
+  /* Calculate `offset' so that src + offset points to the last byte
+     before 16 byte alignment.  */
+
+  /* temp = src_addr & 0xf */
+  force_expand_binop (Pmode, and_optab, src_addr, GEN_INT (15), temp,
+		      1, OPTAB_DIRECT);
+
+  /* offset = 0xf - temp */
+  emit_move_insn (offset, GEN_INT (15));
+  force_expand_binop (Pmode, sub_optab, offset, temp, offset,
+		      1, OPTAB_DIRECT);
+
+  /* Store `offset' bytes in the dstination string.  The quick check
+     has loaded at least `offset' bytes into vsrc.  */
+
+  emit_insn (gen_vstlv16qi (vsrc, gen_lowpart (SImode, offset), dst));
+
+  /* Advance to the next byte to be loaded.  */
+  force_expand_binop (Pmode, add_optab, offset, const1_rtx, offset,
+		      1, OPTAB_DIRECT);
+
+  /* Make sure the addresses are single regs which can be used as a
+     base.  */
+  emit_move_insn (src_addr_reg, src_addr);
+  emit_move_insn (dst_addr_reg, dst_addr);
+
+  /* MAIN LOOP */
+
+  emit_label (loop_label);
+  LABEL_NUSES (loop_label) = 1;
+
+  emit_move_insn (vsrc,
+		  gen_rtx_MEM (V16QImode,
+			       gen_rtx_PLUS (Pmode, src_addr_reg, offset)));
+
+  emit_insn (gen_vec_vfenesv16qi (vpos, vsrc, vsrc,
+				  GEN_INT (VSTRING_FLAG_ZS | VSTRING_FLAG_CS)));
+  add_reg_note (s390_emit_ccraw_jump (8, EQ, done_label),
+		REG_BR_PROB, GEN_INT (very_unlikely));
+
+  emit_move_insn (gen_rtx_MEM (V16QImode,
+			       gen_rtx_PLUS (Pmode, dst_addr_reg, offset)),
+		  vsrc);
+  /* offset += 16 */
+  force_expand_binop (Pmode, add_optab, offset, GEN_INT (16),
+		      offset,  1, OPTAB_DIRECT);
+
+  emit_jump (loop_label);
+  emit_barrier ();
+
+  /* REGULAR EXIT */
+
+  /* We are done.  Add the offset of the zero character to the dst_addr
+     pointer to get the result.  */
+
+  emit_label (done_label);
+  LABEL_NUSES (done_label) = 1;
+
+  force_expand_binop (Pmode, add_optab, dst_addr_reg, offset, dst_addr_reg,
+		      1, OPTAB_DIRECT);
+
+  emit_insn (gen_vec_extractv16qi (gpos_qi, vpos, GEN_INT (7)));
+  emit_move_insn (gpos, gen_rtx_SUBREG (SImode, gpos_qi, 0));
+
+  emit_insn (gen_vstlv16qi (vsrc, gpos, gen_rtx_MEM (BLKmode, dst_addr_reg)));
+
+  force_expand_binop (Pmode, add_optab, dst_addr_reg, gpos, result,
+		      1, OPTAB_DIRECT);
+
+  /* EARLY EXIT */
+
+  emit_label (exit_label);
+  LABEL_NUSES (exit_label) = 1;
+}
+
 
 /* Expand conditional increment or decrement using alc/slb instructions.
    Should generate code setting DST to either SRC or SRC + INCREMENT,
@@ -4964,6 +6244,304 @@ s390_expand_mask_and_shift (rtx val, enu
 			      NULL_RTX, 1, OPTAB_DIRECT);
 }
 
+/* Generate a vector comparison COND of CMP_OP1 and CMP_OP2 and store
+   the result in TARGET.  */
+
+void
+s390_expand_vec_compare (rtx target, enum rtx_code cond,
+			 rtx cmp_op1, rtx cmp_op2)
+{
+  enum machine_mode mode = GET_MODE (target);
+  bool neg_p = false, swap_p = false;
+  rtx tmp;
+
+  if (GET_MODE (cmp_op1) == V2DFmode)
+    {
+      switch (cond)
+	{
+	  /* NE a != b -> !(a == b) */
+	case NE:   cond = EQ; neg_p = true;                break;
+	  /* UNGT a u> b -> !(b >= a) */
+	case UNGT: cond = GE; neg_p = true; swap_p = true; break;
+	  /* UNGE a u>= b -> !(b > a) */
+	case UNGE: cond = GT; neg_p = true; swap_p = true; break;
+	  /* LE: a <= b -> b >= a */
+	case LE:   cond = GE;               swap_p = true; break;
+	  /* UNLE: a u<= b -> !(a > b) */
+	case UNLE: cond = GT; neg_p = true;                break;
+	  /* LT: a < b -> b > a */
+	case LT:   cond = GT;               swap_p = true; break;
+	  /* UNLT: a u< b -> !(a >= b) */
+	case UNLT: cond = GE; neg_p = true;                break;
+	case UNEQ:
+	  emit_insn (gen_vec_cmpuneqv2df (target, cmp_op1, cmp_op2));
+	  return;
+	case LTGT:
+	  emit_insn (gen_vec_cmpltgtv2df (target, cmp_op1, cmp_op2));
+	  return;
+	case ORDERED:
+	  emit_insn (gen_vec_orderedv2df (target, cmp_op1, cmp_op2));
+	  return;
+	case UNORDERED:
+	  emit_insn (gen_vec_unorderedv2df (target, cmp_op1, cmp_op2));
+	  return;
+	default: break;
+	}
+    }
+  else
+    {
+      switch (cond)
+	{
+	  /* NE: a != b -> !(a == b) */
+	case NE:  cond = EQ;  neg_p = true;                break;
+	  /* GE: a >= b -> !(b > a) */
+	case GE:  cond = GT;  neg_p = true; swap_p = true; break;
+	  /* GEU: a >= b -> !(b > a) */
+	case GEU: cond = GTU; neg_p = true; swap_p = true; break;
+	  /* LE: a <= b -> !(a > b) */
+	case LE:  cond = GT;  neg_p = true;                break;
+	  /* LEU: a <= b -> !(a > b) */
+	case LEU: cond = GTU; neg_p = true;                break;
+	  /* LT: a < b -> b > a */
+	case LT:  cond = GT;                swap_p = true; break;
+	  /* LTU: a < b -> b > a */
+	case LTU: cond = GTU;               swap_p = true; break;
+	default: break;
+	}
+    }
+
+  if (swap_p)
+    {
+      tmp = cmp_op1; cmp_op1 = cmp_op2; cmp_op2 = tmp;
+    }
+
+  emit_insn (gen_rtx_SET (VOIDmode,
+			  target, gen_rtx_fmt_ee (cond,
+						  mode,
+						  cmp_op1, cmp_op2)));
+  if (neg_p)
+    emit_insn (gen_rtx_SET (VOIDmode, target, gen_rtx_NOT (mode, target)));
+}
+
+/* Expand the comparison CODE of CMP1 and CMP2 and copy 1 or 0 into
+   TARGET if either all (ALL_P is true) or any (ALL_P is false) of the
+   elements in CMP1 and CMP2 fulfill the comparison.  */
+void
+s390_expand_vec_compare_cc (rtx target, enum rtx_code code,
+			    rtx cmp1, rtx cmp2, bool all_p)
+{
+  enum rtx_code new_code = code;
+  enum machine_mode cmp_mode, full_cmp_mode, scratch_mode;
+  rtx tmp_reg = gen_reg_rtx (SImode);
+  bool swap_p = false;
+
+  if (GET_MODE_CLASS (GET_MODE (cmp1)) == MODE_VECTOR_INT)
+    {
+      switch (code)
+	{
+	case EQ:  cmp_mode = CCVEQmode; break;
+	case NE:  cmp_mode = CCVEQmode; break;
+	case GT:  cmp_mode = CCVHmode;  break;
+	case GE:  cmp_mode = CCVHmode;  new_code = LE; swap_p = true; break;
+	case LT:  cmp_mode = CCVHmode;  new_code = GT; swap_p = true; break;
+	case LE:  cmp_mode = CCVHmode;  new_code = LE; break;
+	case GTU: cmp_mode = CCVHUmode; break;
+	case GEU: cmp_mode = CCVHUmode; new_code = LEU; swap_p = true; break;
+	case LTU: cmp_mode = CCVHUmode; new_code = GTU; swap_p = true; break;
+	case LEU: cmp_mode = CCVHUmode; new_code = LEU; break;
+	default: gcc_unreachable ();
+	}
+      scratch_mode = GET_MODE (cmp1);
+    }
+  else if (GET_MODE (cmp1) == V2DFmode)
+    {
+      switch (code)
+	{
+	case EQ:   cmp_mode = CCVEQmode;  break;
+	case NE:   cmp_mode = CCVEQmode;  break;
+	case GT:   cmp_mode = CCVFHmode;  break;
+	case GE:   cmp_mode = CCVFHEmode; break;
+	case UNLE: cmp_mode = CCVFHmode;  break;
+	case UNLT: cmp_mode = CCVFHEmode; break;
+	case LT:   cmp_mode = CCVFHmode;  new_code = GT; swap_p = true; break;
+	case LE:   cmp_mode = CCVFHEmode; new_code = GE; swap_p = true; break;
+	default: gcc_unreachable ();
+	}
+      scratch_mode = V2DImode;
+    }
+  else
+    gcc_unreachable ();
+
+  if (!all_p)
+    switch (cmp_mode)
+      {
+      case CCVEQmode:  full_cmp_mode = CCVEQANYmode;  break;
+      case CCVHmode:   full_cmp_mode = CCVHANYmode;   break;
+      case CCVHUmode:  full_cmp_mode = CCVHUANYmode;  break;
+      case CCVFHmode:  full_cmp_mode = CCVFHANYmode;  break;
+      case CCVFHEmode: full_cmp_mode = CCVFHEANYmode; break;
+      default: gcc_unreachable ();
+      }
+  else
+    /* The modes without ANY match the ALL modes.  */
+    full_cmp_mode = cmp_mode;
+
+  if (swap_p)
+    {
+      rtx tmp = cmp2;
+      cmp2 = cmp1;
+      cmp1 = tmp;
+    }
+
+  emit_insn (gen_rtx_PARALLEL (VOIDmode,
+	       gen_rtvec (2, gen_rtx_SET (
+			       VOIDmode,
+			       gen_rtx_REG (cmp_mode, CC_REGNUM),
+			       gen_rtx_COMPARE (cmp_mode, cmp1, cmp2)),
+			  gen_rtx_CLOBBER (VOIDmode,
+					   gen_rtx_SCRATCH (scratch_mode)))));
+  emit_move_insn (target, const0_rtx);
+  emit_move_insn (tmp_reg, const1_rtx);
+
+  emit_move_insn (target,
+		  gen_rtx_IF_THEN_ELSE (SImode,
+		    gen_rtx_fmt_ee (new_code, VOIDmode,
+				    gen_rtx_REG (full_cmp_mode, CC_REGNUM),
+				    const0_rtx),
+		      target, tmp_reg));
+}
+
+/* Generate a vector comparison expression loading either elements of
+   THEN or ELS into TARGET depending on the comparison COND of CMP_OP1
+   and CMP_OP2.  */
+
+void
+s390_expand_vcond (rtx target, rtx then, rtx els,
+		   enum rtx_code cond, rtx cmp_op1, rtx cmp_op2)
+{
+  rtx tmp;
+  enum machine_mode result_mode;
+  rtx result_target;
+
+  /* We always use an integral type vector to hold the comparison
+     result.  */
+  result_mode = GET_MODE (cmp_op1) == V2DFmode ? V2DImode : GET_MODE (cmp_op1);
+  result_target = gen_reg_rtx (result_mode);
+
+  /* Alternatively this could be done by reload by lowering the cmp*
+     predicates.  But it appears to be better for scheduling etc. to
+     have that in early.  */
+  if (!REG_P (cmp_op1))
+    cmp_op1 = force_reg (GET_MODE (cmp_op1), cmp_op1);
+
+  if (!REG_P (cmp_op2))
+    cmp_op2 = force_reg (GET_MODE (cmp_op2), cmp_op2);
+
+  s390_expand_vec_compare (result_target, cond,
+			   cmp_op1, cmp_op2);
+
+  /* If the results are supposed to be either -1 or 0 we are done
+     since this is what our compare instructions generate anyway.  */
+  if (all_ones_operand (then, GET_MODE (then))
+      && const0_operand (els, GET_MODE (els)))
+    {
+      emit_move_insn (target, gen_rtx_SUBREG (GET_MODE (target),
+					      result_target, 0));
+      return;
+    }
+
+  /* Otherwise we will do a vsel afterwards.  */
+  /* This gets triggered e.g.
+     with gcc.c-torture/compile/pr53410-1.c */
+  if (!REG_P (then))
+    then = force_reg (GET_MODE (target), then);
+
+  if (!REG_P (els))
+    els = force_reg (GET_MODE (target), els);
+
+  tmp = gen_rtx_fmt_ee (EQ, VOIDmode,
+			result_target,
+			CONST0_RTX (result_mode));
+
+  /* We compared the result against zero above so we have to swap then
+     and els here.  */
+  tmp = gen_rtx_IF_THEN_ELSE (GET_MODE (target), tmp, els, then);
+
+  gcc_assert (GET_MODE (target) == GET_MODE (then));
+  emit_insn (gen_rtx_SET (VOIDmode, target, tmp));
+}
+
+/* Emit the RTX necessary to initialize the vector TARGET with values
+   in VALS.  */
+void
+s390_expand_vec_init (rtx target, rtx vals)
+{
+  enum machine_mode mode = GET_MODE (target);
+  enum machine_mode inner_mode = GET_MODE_INNER (mode);
+  int n_elts = GET_MODE_NUNITS (mode);
+  bool all_same = true, all_regs = true, all_const_int = true;
+  rtx x;
+  int i;
+
+  for (i = 0; i < n_elts; ++i)
+    {
+      x = XVECEXP (vals, 0, i);
+
+      if (!CONST_INT_P (x))
+	all_const_int = false;
+
+      if (i > 0 && !rtx_equal_p (x, XVECEXP (vals, 0, 0)))
+	all_same = false;
+
+      if (!REG_P (x))
+	all_regs = false;
+    }
+
+  /* Use vector gen mask or vector gen byte mask if possible.  */
+  if (all_same && all_const_int
+      && (XVECEXP (vals, 0, 0) == const0_rtx
+	  || s390_contiguous_bitmask_vector_p (XVECEXP (vals, 0, 0),
+					       NULL, NULL)
+	  || s390_bytemask_vector_p (XVECEXP (vals, 0, 0), NULL)))
+    {
+      emit_insn (gen_rtx_SET (VOIDmode, target,
+			      gen_rtx_CONST_VECTOR (mode, XVEC (vals, 0))));
+      return;
+    }
+
+  if (all_same)
+    {
+      emit_insn (gen_rtx_SET (VOIDmode, target,
+			      gen_rtx_VEC_DUPLICATE (mode,
+						     XVECEXP (vals, 0, 0))));
+      return;
+    }
+
+  if (all_regs && REG_P (target) && n_elts == 2 && inner_mode == DImode)
+    {
+      /* Use vector load pair.  */
+      emit_insn (gen_rtx_SET (VOIDmode, target,
+			      gen_rtx_VEC_CONCAT (mode,
+						  XVECEXP (vals, 0, 0),
+						  XVECEXP (vals, 0, 1))));
+      return;
+    }
+
+  /* We are about to set the vector elements one by one.  Zero out the
+     full register first in order to help the data flow framework to
+     detect it as full VR set.  */
+  emit_insn (gen_rtx_SET (VOIDmode, target, CONST0_RTX (mode)));
+
+  /* Unfortunately the vec_init expander is not allowed to fail.  So
+     we have to implement the fallback ourselves.  */
+  for (i = 0; i < n_elts; i++)
+    emit_insn (gen_rtx_SET (VOIDmode, target,
+			    gen_rtx_UNSPEC (mode,
+					    gen_rtvec (3, XVECEXP (vals, 0, i),
+						       GEN_INT (i), target),
+					    UNSPEC_VEC_SET)));
+}
+
 /* Structure to hold the initial parameters for a compare_and_swap operation
    in HImode and QImode.  */
 
@@ -5259,12 +6837,37 @@ s390_output_dwarf_dtprel (FILE *file, in
   fputs ("@DTPOFF", file);
 }
 
+/* Return the proper mode for REGNO being represented in the dwarf
+   unwind table.  */
+enum machine_mode
+s390_dwarf_frame_reg_mode (int regno)
+{
+  enum machine_mode save_mode = default_dwarf_frame_reg_mode (regno);
+
+  /* The rightmost 64 bits of vector registers are call-clobbered.  */
+  if (GET_MODE_SIZE (save_mode) > 8)
+    save_mode = DImode;
+
+  return save_mode;
+}
+
 #ifdef TARGET_ALTERNATE_LONG_DOUBLE_MANGLING
 /* Implement TARGET_MANGLE_TYPE.  */
 
 static const char *
 s390_mangle_type (const_tree type)
 {
+  type = TYPE_MAIN_VARIANT (type);
+
+  if (TREE_CODE (type) != VOID_TYPE && TREE_CODE (type) != BOOLEAN_TYPE
+      && TREE_CODE (type) != INTEGER_TYPE && TREE_CODE (type) != REAL_TYPE)
+    return NULL;
+
+  if (type == s390_builtin_types[BT_BV16QI]) return "U6__boolc";
+  if (type == s390_builtin_types[BT_BV8HI]) return "U6__bools";
+  if (type == s390_builtin_types[BT_BV4SI]) return "U6__booli";
+  if (type == s390_builtin_types[BT_BV2DI]) return "U6__booll";
+
   if (TYPE_MAIN_VARIANT (type) == long_double_type_node
       && TARGET_LONG_DOUBLE_128)
     return "g";
@@ -5627,24 +7230,26 @@ print_operand_address (FILE *file, rtx a
     'J': print tls_load/tls_gdcall/tls_ldcall suffix
     'M': print the second word of a TImode operand.
     'N': print the second word of a DImode operand.
-    'O': print only the displacement of a memory reference.
-    'R': print only the base register of a memory reference.
+    'O': print only the displacement of a memory reference or address.
+    'R': print only the base register of a memory reference or address.
     'S': print S-type memory reference (base+displacement).
     'Y': print shift count operand.
 
     'b': print integer X as if it's an unsigned byte.
     'c': print integer X as if it's an signed byte.
-    'e': "end" of DImode contiguous bitmask X.
-    'f': "end" of SImode contiguous bitmask X.
+    'e': "end" contiguous bitmask X in either DImode or vector inner mode.
+    'f': "end" contiguous bitmask X in SImode.
     'h': print integer X as if it's a signed halfword.
     'i': print the first nonzero HImode part of X.
     'j': print the first HImode part unequal to -1 of X.
     'k': print the first nonzero SImode part of X.
     'm': print the first SImode part unequal to -1 of X.
     'o': print integer X as if it's an unsigned 32bit word.
-    's': "start" of DImode contiguous bitmask X.
-    't': "start" of SImode contiguous bitmask X.
+    's': "start" of contiguous bitmask X in either DImode or vector inner mode.
+    't': CONST_INT: "start" of contiguous bitmask X in SImode.
+         CONST_VECTOR: Generate a bitmask for vgbm instruction.
     'x': print integer X as if it's an unsigned halfword.
+    'v': print register number as vector register (v1 instead of f1).
 */
 
 void
@@ -5701,14 +7306,7 @@ print_operand (FILE *file, rtx x, int co
         struct s390_address ad;
 	int ret;
 
-	if (!MEM_P (x))
-	  {
-	    output_operand_lossage ("memory reference expected for "
-				    "'O' output modifier");
-	    return;
-	  }
-
-	ret = s390_decompose_address (XEXP (x, 0), &ad);
+	ret = s390_decompose_address (MEM_P (x) ? XEXP (x, 0) : x, &ad);
 
 	if (!ret
 	    || (ad.base && !REGNO_OK_FOR_BASE_P (REGNO (ad.base)))
@@ -5730,14 +7328,7 @@ print_operand (FILE *file, rtx x, int co
         struct s390_address ad;
 	int ret;
 
-	if (!MEM_P (x))
-	  {
-	    output_operand_lossage ("memory reference expected for "
-				    "'R' output modifier");
-	    return;
-	  }
-
-	ret = s390_decompose_address (XEXP (x, 0), &ad);
+	ret = s390_decompose_address (MEM_P (x) ? XEXP (x, 0) : x, &ad);
 
 	if (!ret
 	    || (ad.base && !REGNO_OK_FOR_BASE_P (REGNO (ad.base)))
@@ -5815,7 +7406,17 @@ print_operand (FILE *file, rtx x, int co
   switch (GET_CODE (x))
     {
     case REG:
-      fprintf (file, "%s", reg_names[REGNO (x)]);
+      /* Print FP regs as fx instead of vx when they are accessed
+	 through non-vector mode.  */
+      if (code == 'v'
+	  || VECTOR_NOFP_REG_P (x)
+	  || (FP_REG_P (x) && VECTOR_MODE_P (GET_MODE (x)))
+	  || (VECTOR_REG_P (x)
+	      && (GET_MODE_SIZE (GET_MODE (x)) /
+		  s390_class_max_nregs (FP_REGS, GET_MODE (x))) > 8))
+	fprintf (file, "%%v%s", reg_names[REGNO (x)] + 2);
+      else
+	fprintf (file, "%s", reg_names[REGNO (x)]);
       break;
 
     case MEM:
@@ -5902,6 +7503,44 @@ print_operand (FILE *file, rtx x, int co
 				    code);
 	}
       break;
+    case CONST_VECTOR:
+      switch (code)
+	{
+	case 'h':
+	  gcc_assert (s390_const_vec_duplicate_p (x));
+	  fprintf (file, HOST_WIDE_INT_PRINT_DEC,
+		   ((INTVAL (XVECEXP (x, 0, 0)) & 0xffff) ^ 0x8000) - 0x8000);
+	  break;
+	case 'e':
+	case 's':
+	  {
+	    int start, stop, inner_len;
+	    bool ok;
+
+	    inner_len = GET_MODE_UNIT_BITSIZE (GET_MODE (x));
+	    ok = s390_contiguous_bitmask_vector_p (x, &start, &stop);
+	    gcc_assert (ok);
+	    if (code == 's' || code == 't')
+	      ival = inner_len - stop - 1;
+	    else
+	      ival = inner_len - start - 1;
+	    fprintf (file, HOST_WIDE_INT_PRINT_DEC, ival);
+	  }
+	  break;
+	case 't':
+	  {
+	    unsigned mask;
+	    bool ok = s390_bytemask_vector_p (x, &mask);
+	    gcc_assert (ok);
+	    fprintf (file, "%u", mask);
+	  }
+	  break;
+
+	default:
+	  output_operand_lossage ("invalid constant vector for output "
+				  "modifier '%c'", code);
+	}
+      break;
 
     default:
       if (code == 0)
@@ -6051,7 +7690,8 @@ s390_adjust_priority (rtx insn ATTRIBUTE
       && s390_tune != PROCESSOR_2094_Z9_109
       && s390_tune != PROCESSOR_2097_Z10
       && s390_tune != PROCESSOR_2817_Z196
-      && s390_tune != PROCESSOR_2827_ZEC12)
+      && s390_tune != PROCESSOR_2827_ZEC12
+      && s390_tune != PROCESSOR_2964_Z13)
     return priority;
 
   switch (s390_safe_attr_type (insn))
@@ -6459,14 +8099,20 @@ replace_ltrel_base (rtx *x)
 /* We keep a list of constants which we have to add to internal
    constant tables in the middle of large functions.  */
 
-#define NR_C_MODES 11
+#define NR_C_MODES 32
 enum machine_mode constant_modes[NR_C_MODES] =
 {
   TFmode, TImode, TDmode,
+  V16QImode, V8HImode, V4SImode, V2DImode, V1TImode,
+  V4SFmode, V2DFmode, V1TFmode,
   DFmode, DImode, DDmode,
+  V8QImode, V4HImode, V2SImode, V1DImode, V2SFmode, V1DFmode,
   SFmode, SImode, SDmode,
+  V4QImode, V2HImode, V1SImode,  V1SFmode,
   HImode,
-  QImode
+  V2QImode, V1HImode,
+  QImode,
+  V1QImode
 };
 
 struct constant
@@ -7490,6 +9136,23 @@ s390_output_pool_entry (rtx exp, enum ma
       mark_symbol_refs_as_used (exp);
       break;
 
+    case MODE_VECTOR_INT:
+    case MODE_VECTOR_FLOAT:
+      {
+	int i;
+	enum machine_mode inner_mode;
+	gcc_assert (GET_CODE (exp) == CONST_VECTOR);
+
+	inner_mode = GET_MODE_INNER (GET_MODE (exp));
+	for (i = 0; i < XVECLEN (exp, 0); i++)
+	  s390_output_pool_entry (XVECEXP (exp, 0, i),
+				  inner_mode,
+				  i == 0
+				  ? align
+				  : GET_MODE_BITSIZE (inner_mode));
+      }
+      break;
+
     default:
       gcc_unreachable ();
     }
@@ -8205,9 +9868,25 @@ s390_update_frame_layout (void)
 bool
 s390_hard_regno_mode_ok (unsigned int regno, enum machine_mode mode)
 {
+  if (!TARGET_VX && VECTOR_NOFP_REGNO_P (regno))
+    return false;
+
   switch (REGNO_REG_CLASS (regno))
     {
+    case VEC_REGS:
+      return ((GET_MODE_CLASS (mode) == MODE_INT
+	       && s390_class_max_nregs (VEC_REGS, mode) == 1)
+	      || mode == DFmode
+	      || s390_vector_mode_supported_p (mode));
+      break;
     case FP_REGS:
+      if (TARGET_VX
+	  && ((GET_MODE_CLASS (mode) == MODE_INT
+	       && s390_class_max_nregs (FP_REGS, mode) == 1)
+	      || mode == DFmode
+	      || s390_vector_mode_supported_p (mode)))
+	return true;
+
       if (REGNO_PAIR_OK (regno, mode))
 	{
 	  if (mode == SImode || mode == DImode)
@@ -8269,19 +9948,86 @@ s390_hard_regno_rename_ok (unsigned int
 int
 s390_class_max_nregs (enum reg_class rclass, enum machine_mode mode)
 {
+  int reg_size;
+  bool reg_pair_required_p = false;
+
   switch (rclass)
     {
     case FP_REGS:
+    case VEC_REGS:
+      reg_size = TARGET_VX ? 16 : 8;
+
+      /* TF and TD modes would fit into a VR but we put them into a
+	 register pair since we do not have 128bit FP instructions on
+	 full VRs.  */
+      if (TARGET_VX
+	  && SCALAR_FLOAT_MODE_P (mode)
+	  && GET_MODE_SIZE (mode) >= 16)
+	reg_pair_required_p = true;
+
+      /* Even if complex types would fit into a single FPR/VR we force
+	 them into a register pair to deal with the parts more easily.
+	 (FIXME: What about complex ints?)  */
       if (GET_MODE_CLASS (mode) == MODE_COMPLEX_FLOAT)
-	return 2 * ((GET_MODE_SIZE (mode) / 2 + 8 - 1) / 8);
-      else
-	return (GET_MODE_SIZE (mode) + 8 - 1) / 8;
+	reg_pair_required_p = true;
+      break;
     case ACCESS_REGS:
-      return (GET_MODE_SIZE (mode) + 4 - 1) / 4;
+      reg_size = 4;
+      break;
     default:
+      reg_size = UNITS_PER_WORD;
       break;
     }
-  return (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;
+
+  if (reg_pair_required_p)
+    return 2 * ((GET_MODE_SIZE (mode) / 2 + reg_size - 1) / reg_size);
+
+  return (GET_MODE_SIZE (mode) + reg_size - 1) / reg_size;
+}
+
+/* Return TRUE if changing mode from FROM to TO should not be allowed
+   for register class CLASS.  */
+
+int
+s390_cannot_change_mode_class (enum machine_mode from_mode,
+			       enum machine_mode to_mode,
+			       enum reg_class rclass)
+{
+  enum machine_mode small_mode;
+  enum machine_mode big_mode;
+
+  if (GET_MODE_SIZE (from_mode) == GET_MODE_SIZE (to_mode))
+    return 0;
+
+  if (GET_MODE_SIZE (from_mode) < GET_MODE_SIZE (to_mode))
+    {
+      small_mode = from_mode;
+      big_mode = to_mode;
+    }
+  else
+    {
+      small_mode = to_mode;
+      big_mode = from_mode;
+    }
+
+  /* Values residing in VRs are little-endian style.  All modes are
+     placed left-aligned in an VR.  This means that we cannot allow
+     switching between modes with differing sizes.  Also if the vector
+     facility is available we still place TFmode values in VR register
+     pairs, since the only instructions we have operating on TFmodes
+     only deal with register pairs.  Therefore we have to allow DFmode
+     subregs of TFmodes to enable the TFmode splitters.  */
+  if (reg_classes_intersect_p (VEC_REGS, rclass)
+      && (GET_MODE_SIZE (small_mode) < 8
+	  || s390_class_max_nregs (VEC_REGS, big_mode) == 1))
+    return 1;
+
+  /* Likewise for access registers, since they have only half the
+     word size on 64-bit.  */
+  if (reg_classes_intersect_p (ACCESS_REGS, rclass))
+    return 1;
+
+  return 0;
 }
 
 /* Return true if register FROM can be eliminated via register TO.  */
@@ -9112,6 +10858,23 @@ s390_emit_epilogue (bool sibcall)
 }
 
 
+/* The VX ABI differs for vararg functions.  Therefore we need the
+   prototype of the callee to be available when passing vector type
+   values.  */
+static const char *
+s390_invalid_arg_for_unprototyped_fn (const_tree typelist, const_tree funcdecl, const_tree val)
+{
+  return ((TARGET_VX_ABI
+	   && typelist == 0
+	   && TREE_CODE (TREE_TYPE (val)) == VECTOR_TYPE
+	   && (funcdecl == NULL_TREE
+	       || (TREE_CODE (funcdecl) == FUNCTION_DECL
+		   && DECL_BUILT_IN_CLASS (funcdecl) != BUILT_IN_MD)))
+	  ? N_("Vector argument passed to unprototyped function")
+	  : NULL);
+}
+
+
 /* Return the size in bytes of a function argument of
    type TYPE and/or mode MODE.  At least one of TYPE or
    MODE must be specified.  */
@@ -9126,8 +10889,57 @@ s390_function_arg_size (enum machine_mod
   if (mode != BLKmode)
     return GET_MODE_SIZE (mode);
 
-  /* If we have neither type nor mode, abort */
-  gcc_unreachable ();
+  /* If we have neither type nor mode, abort */
+  gcc_unreachable ();
+}
+
+/* Return true if a function argument of type TYPE and mode MODE
+   is to be passed in a vector register, if available.  */
+
+bool
+s390_function_arg_vector (enum machine_mode mode, const_tree type)
+{
+  if (!TARGET_VX_ABI)
+    return false;
+
+  if (s390_function_arg_size (mode, type) > 16)
+    return false;
+
+  /* No type info available for some library calls ...  */
+  if (!type)
+    return VECTOR_MODE_P (mode);
+
+  /* The ABI says that record types with a single member are treated
+     just like that member would be.  */
+  while (TREE_CODE (type) == RECORD_TYPE)
+    {
+      tree field, single = NULL_TREE;
+
+      for (field = TYPE_FIELDS (type); field; field = DECL_CHAIN (field))
+	{
+	  if (TREE_CODE (field) != FIELD_DECL)
+	    continue;
+
+	  if (single == NULL_TREE)
+	    single = TREE_TYPE (field);
+	  else
+	    return false;
+	}
+
+      if (single == NULL_TREE)
+	return false;
+      else
+	{
+	  /* If the field declaration adds extra byte due to
+	     e.g. padding this is not accepted as vector type.  */
+	  if (int_size_in_bytes (single) <= 0
+	      || int_size_in_bytes (single) != int_size_in_bytes (type))
+	    return false;
+	  type = single;
+	}
+    }
+
+  return TREE_CODE (type) == VECTOR_TYPE;
 }
 
 /* Return true if a function argument of type TYPE and mode MODE
@@ -9136,8 +10948,7 @@ s390_function_arg_size (enum machine_mod
 static bool
 s390_function_arg_float (enum machine_mode mode, const_tree type)
 {
-  int size = s390_function_arg_size (mode, type);
-  if (size > 8)
+  if (s390_function_arg_size (mode, type) > 8)
     return false;
 
   /* Soft-float changes the ABI: no floating-point registers are used.  */
@@ -9220,20 +11031,24 @@ s390_pass_by_reference (cumulative_args_
 			bool named ATTRIBUTE_UNUSED)
 {
   int size = s390_function_arg_size (mode, type);
+
+  if (s390_function_arg_vector (mode, type))
+    return false;
+
   if (size > 8)
     return true;
 
   if (type)
     {
       if (AGGREGATE_TYPE_P (type) && exact_log2 (size) < 0)
-        return 1;
+        return true;
 
       if (TREE_CODE (type) == COMPLEX_TYPE
 	  || TREE_CODE (type) == VECTOR_TYPE)
-        return 1;
+	return true;
     }
 
-  return 0;
+  return false;
 }
 
 /* Update the data in CUM to advance over an argument of mode MODE and
@@ -9244,11 +11059,21 @@ s390_pass_by_reference (cumulative_args_
 
 static void
 s390_function_arg_advance (cumulative_args_t cum_v, enum machine_mode mode,
-			   const_tree type, bool named ATTRIBUTE_UNUSED)
+			   const_tree type, bool named)
 {
   CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
 
-  if (s390_function_arg_float (mode, type))
+  if (s390_function_arg_vector (mode, type))
+    {
+      /* We are called for unnamed vector stdarg arguments which are
+	 passed on the stack.  In this case this hook does not have to
+	 do anything since stack arguments are tracked by common
+	 code.  */
+      if (!named)
+	return;
+      cum->vrs += 1;
+    }
+  else if (s390_function_arg_float (mode, type))
     {
       cum->fprs += 1;
     }
@@ -9282,14 +11107,26 @@ s390_function_arg_advance (cumulative_ar
 
 static rtx
 s390_function_arg (cumulative_args_t cum_v, enum machine_mode mode,
-		   const_tree type, bool named ATTRIBUTE_UNUSED)
+		   const_tree type, bool named)
 {
   CUMULATIVE_ARGS *cum = get_cumulative_args (cum_v);
 
-  if (s390_function_arg_float (mode, type))
+  if (!named)
+    s390_check_type_for_vector_abi (type, true, false);
+
+  if (s390_function_arg_vector (mode, type))
+    {
+      /* Vector arguments being part of the ellipsis are passed on the
+	 stack.  */
+      if (!named || (cum->vrs + 1 > VEC_ARG_NUM_REG))
+	return NULL_RTX;
+
+      return gen_rtx_REG (mode, cum->vrs + FIRST_VEC_ARG_REGNO);
+    }
+  else if (s390_function_arg_float (mode, type))
     {
       if (cum->fprs + 1 > FP_ARG_NUM_REG)
-	return 0;
+	return NULL_RTX;
       else
 	return gen_rtx_REG (mode, cum->fprs + 16);
     }
@@ -9299,7 +11136,7 @@ s390_function_arg (cumulative_args_t cum
       int n_gprs = (size + UNITS_PER_LONG - 1) / UNITS_PER_LONG;
 
       if (cum->gprs + n_gprs > GP_ARG_NUM_REG)
-	return 0;
+	return NULL_RTX;
       else if (n_gprs == 1 || UNITS_PER_WORD == UNITS_PER_LONG)
 	return gen_rtx_REG (mode, cum->gprs + 2);
       else if (n_gprs == 2)
@@ -9342,6 +11179,12 @@ s390_return_in_memory (const_tree type,
       || TREE_CODE (type) == REAL_TYPE)
     return int_size_in_bytes (type) > 8;
 
+  /* vector types which fit into a VR.  */
+  if (TARGET_VX_ABI
+      && TREE_CODE (type) == VECTOR_TYPE
+      && int_size_in_bytes (type) <= 16)
+    return false;
+
   /* Aggregates and similar constructs are always returned
      in memory.  */
   if (AGGREGATE_TYPE_P (type)
@@ -9384,6 +11227,12 @@ s390_function_and_libcall_value (enum ma
 				 const_tree fntype_or_decl,
 				 bool outgoing ATTRIBUTE_UNUSED)
 {
+  /* For vector return types it is important to use the RET_TYPE
+     argument whenever available since the middle-end might have
+     changed the mode to a scalar mode.  */
+  bool vector_ret_type_p = ((ret_type && TREE_CODE (ret_type) == VECTOR_TYPE)
+			    || (!ret_type && VECTOR_MODE_P (mode)));
+
   /* For normal functions perform the promotion as
      promote_function_mode would do.  */
   if (ret_type)
@@ -9393,10 +11242,14 @@ s390_function_and_libcall_value (enum ma
 				    fntype_or_decl, 1);
     }
 
-  gcc_assert (GET_MODE_CLASS (mode) == MODE_INT || SCALAR_FLOAT_MODE_P (mode));
-  gcc_assert (GET_MODE_SIZE (mode) <= 8);
-
-  if (TARGET_HARD_FLOAT && SCALAR_FLOAT_MODE_P (mode))
+  gcc_assert (GET_MODE_CLASS (mode) == MODE_INT
+	      || SCALAR_FLOAT_MODE_P (mode)
+	      || (TARGET_VX_ABI && vector_ret_type_p));
+  gcc_assert (GET_MODE_SIZE (mode) <= (TARGET_VX_ABI ? 16 : 8));
+
+  if (TARGET_VX_ABI && vector_ret_type_p)
+    return gen_rtx_REG (mode, FIRST_VEC_ARG_REGNO);
+  else if (TARGET_HARD_FLOAT && SCALAR_FLOAT_MODE_P (mode))
     return gen_rtx_REG (mode, 16);
   else if (GET_MODE_SIZE (mode) <= UNITS_PER_LONG
 	   || UNITS_PER_LONG == UNITS_PER_WORD)
@@ -9560,9 +11413,13 @@ s390_va_start (tree valist, rtx nextarg
       expand_expr (t, const0_rtx, VOIDmode, EXPAND_NORMAL);
     }
 
-  /* Find the overflow area.  */
+  /* Find the overflow area.
+     FIXME: This currently is too pessimistic when the vector ABI is
+     enabled.  In that case we *always* set up the overflow area
+     pointer.  */
   if (n_gpr + cfun->va_list_gpr_size > GP_ARG_NUM_REG
-      || n_fpr + cfun->va_list_fpr_size > FP_ARG_NUM_REG)
+      || n_fpr + cfun->va_list_fpr_size > FP_ARG_NUM_REG
+      || TARGET_VX_ABI)
     {
       t = make_tree (TREE_TYPE (ovf), virtual_incoming_args_rtx);
 
@@ -9604,6 +11461,9 @@ s390_va_start (tree valist, rtx nextarg
        ret = args.reg_save_area[args.gpr+8]
      else
        ret = *args.overflow_arg_area++;
+   } else if (vector value) {
+       ret = *args.overflow_arg_area;
+       args.overflow_arg_area += size / 8;
    } else if (float value) {
      if (args.fgpr < 2)
        ret = args.reg_save_area[args.fpr+64]
@@ -9623,7 +11483,10 @@ s390_gimplify_va_arg (tree valist, tree
   tree f_gpr, f_fpr, f_ovf, f_sav;
   tree gpr, fpr, ovf, sav, reg, t, u;
   int indirect_p, size, n_reg, sav_ofs, sav_scale, max_reg;
-  tree lab_false, lab_over, addr;
+  tree lab_false, lab_over;
+  tree addr = create_tmp_var (ptr_type_node, "addr");
+  bool left_align_p; /* How a value < UNITS_PER_LONG is aligned within
+			a stack slot.  */
 
   f_gpr = TYPE_FIELDS (TREE_TYPE (va_list_type_node));
   f_fpr = DECL_CHAIN (f_gpr);
@@ -9642,6 +11505,8 @@ s390_gimplify_va_arg (tree valist, tree
 
   size = int_size_in_bytes (type);
 
+  s390_check_type_for_vector_abi (type, true, false);
+
   if (pass_by_reference (NULL, TYPE_MODE (type), type, false))
     {
       if (TARGET_DEBUG_ARG)
@@ -9662,6 +11527,23 @@ s390_gimplify_va_arg (tree valist, tree
       sav_scale = UNITS_PER_LONG;
       size = UNITS_PER_LONG;
       max_reg = GP_ARG_NUM_REG - n_reg;
+      left_align_p = false;
+    }
+  else if (s390_function_arg_vector (TYPE_MODE (type), type))
+    {
+      if (TARGET_DEBUG_ARG)
+	{
+	  fprintf (stderr, "va_arg: vector type");
+	  debug_tree (type);
+	}
+
+      indirect_p = 0;
+      reg = NULL_TREE;
+      n_reg = 0;
+      sav_ofs = 0;
+      sav_scale = 8;
+      max_reg = 0;
+      left_align_p = true;
     }
   else if (s390_function_arg_float (TYPE_MODE (type), type))
     {
@@ -9678,6 +11560,7 @@ s390_gimplify_va_arg (tree valist, tree
       sav_ofs = 16 * UNITS_PER_LONG;
       sav_scale = 8;
       max_reg = FP_ARG_NUM_REG - n_reg;
+      left_align_p = false;
     }
   else
     {
@@ -9702,53 +11585,74 @@ s390_gimplify_va_arg (tree valist, tree
 
       sav_scale = UNITS_PER_LONG;
       max_reg = GP_ARG_NUM_REG - n_reg;
+      left_align_p = false;
     }
 
   /* Pull the value out of the saved registers ...  */
 
-  lab_false = create_artificial_label (UNKNOWN_LOCATION);
-  lab_over = create_artificial_label (UNKNOWN_LOCATION);
-  addr = create_tmp_var (ptr_type_node, "addr");
-
-  t = fold_convert (TREE_TYPE (reg), size_int (max_reg));
-  t = build2 (GT_EXPR, boolean_type_node, reg, t);
-  u = build1 (GOTO_EXPR, void_type_node, lab_false);
-  t = build3 (COND_EXPR, void_type_node, t, u, NULL_TREE);
-  gimplify_and_add (t, pre_p);
-
-  t = fold_build_pointer_plus_hwi (sav, sav_ofs);
-  u = build2 (MULT_EXPR, TREE_TYPE (reg), reg,
-	      fold_convert (TREE_TYPE (reg), size_int (sav_scale)));
-  t = fold_build_pointer_plus (t, u);
+  if (reg != NULL_TREE)
+    {
+      /*
+	if (reg > ((typeof (reg))max_reg))
+          goto lab_false;
 
-  gimplify_assign (addr, t, pre_p);
+        addr = sav + sav_ofs + reg * save_scale;
+
+	goto lab_over;
+
+        lab_false:
+      */
 
-  gimple_seq_add_stmt (pre_p, gimple_build_goto (lab_over));
+      lab_false = create_artificial_label (UNKNOWN_LOCATION);
+      lab_over = create_artificial_label (UNKNOWN_LOCATION);
 
-  gimple_seq_add_stmt (pre_p, gimple_build_label (lab_false));
+      t = fold_convert (TREE_TYPE (reg), size_int (max_reg));
+      t = build2 (GT_EXPR, boolean_type_node, reg, t);
+      u = build1 (GOTO_EXPR, void_type_node, lab_false);
+      t = build3 (COND_EXPR, void_type_node, t, u, NULL_TREE);
+      gimplify_and_add (t, pre_p);
 
+      t = fold_build_pointer_plus_hwi (sav, sav_ofs);
+      u = build2 (MULT_EXPR, TREE_TYPE (reg), reg,
+		  fold_convert (TREE_TYPE (reg), size_int (sav_scale)));
+      t = fold_build_pointer_plus (t, u);
+
+      gimplify_assign (addr, t, pre_p);
+
+      gimple_seq_add_stmt (pre_p, gimple_build_goto (lab_over));
+
+      gimple_seq_add_stmt (pre_p, gimple_build_label (lab_false));
+    }
 
   /* ... Otherwise out of the overflow area.  */
 
   t = ovf;
-  if (size < UNITS_PER_LONG)
+  if (size < UNITS_PER_LONG && !left_align_p)
     t = fold_build_pointer_plus_hwi (t, UNITS_PER_LONG - size);
 
   gimplify_expr (&t, pre_p, NULL, is_gimple_val, fb_rvalue);
 
   gimplify_assign (addr, t, pre_p);
 
-  t = fold_build_pointer_plus_hwi (t, size);
+  if (size < UNITS_PER_LONG && left_align_p)
+    t = fold_build_pointer_plus_hwi (t, UNITS_PER_LONG);
+  else
+    t = fold_build_pointer_plus_hwi (t, size);
+
   gimplify_assign (ovf, t, pre_p);
 
-  gimple_seq_add_stmt (pre_p, gimple_build_label (lab_over));
+  if (reg != NULL_TREE)
+    gimple_seq_add_stmt (pre_p, gimple_build_label (lab_over));
 
 
   /* Increment register save count.  */
 
-  u = build2 (PREINCREMENT_EXPR, TREE_TYPE (reg), reg,
-	      fold_convert (TREE_TYPE (reg), size_int (n_reg)));
-  gimplify_and_add (u, pre_p);
+  if (n_reg > 0)
+    {
+      u = build2 (PREINCREMENT_EXPR, TREE_TYPE (reg), reg,
+		  fold_convert (TREE_TYPE (reg), size_int (n_reg)));
+      gimplify_and_add (u, pre_p);
+    }
 
   if (indirect_p)
     {
@@ -9793,7 +11697,14 @@ s390_expand_tbegin (rtx dest, rtx tdb, r
     }
 
   if (clobber_fprs_p)
-    emit_insn (gen_tbegin_1 (gen_rtx_CONST_INT (VOIDmode, TBEGIN_MASK), tdb));
+    {
+      if (TARGET_VX)
+	emit_insn (gen_tbegin_1_z13 (gen_rtx_CONST_INT (VOIDmode, TBEGIN_MASK),
+				     tdb));
+      else
+	emit_insn (gen_tbegin_1 (gen_rtx_CONST_INT (VOIDmode, TBEGIN_MASK),
+				 tdb));
+    }
   else
     emit_insn (gen_tbegin_nofloat_1 (gen_rtx_CONST_INT (VOIDmode, TBEGIN_MASK),
 				     tdb));
@@ -9831,210 +11742,6 @@ s390_expand_tbegin (rtx dest, rtx tdb, r
     }
 }
 
-/* Builtins.  */
-
-enum s390_builtin
-{
-  S390_BUILTIN_TBEGIN,
-  S390_BUILTIN_TBEGIN_NOFLOAT,
-  S390_BUILTIN_TBEGIN_RETRY,
-  S390_BUILTIN_TBEGIN_RETRY_NOFLOAT,
-  S390_BUILTIN_TBEGINC,
-  S390_BUILTIN_TEND,
-  S390_BUILTIN_TABORT,
-  S390_BUILTIN_NON_TX_STORE,
-  S390_BUILTIN_TX_NESTING_DEPTH,
-  S390_BUILTIN_TX_ASSIST,
-
-  S390_BUILTIN_max
-};
-
-static enum insn_code const code_for_builtin[S390_BUILTIN_max] = {
-  CODE_FOR_tbegin,
-  CODE_FOR_tbegin_nofloat,
-  CODE_FOR_tbegin_retry,
-  CODE_FOR_tbegin_retry_nofloat,
-  CODE_FOR_tbeginc,
-  CODE_FOR_tend,
-  CODE_FOR_tabort,
-  CODE_FOR_ntstg,
-  CODE_FOR_etnd,
-  CODE_FOR_tx_assist
-};
-
-static void
-s390_init_builtins (void)
-{
-  tree ftype, uint64_type;
-  tree returns_twice_attr = tree_cons (get_identifier ("returns_twice"),
-				       NULL, NULL);
-  tree noreturn_attr = tree_cons (get_identifier ("noreturn"), NULL, NULL);
-
-  /* void foo (void) */
-  ftype = build_function_type_list (void_type_node, NULL_TREE);
-  add_builtin_function ("__builtin_tbeginc", ftype, S390_BUILTIN_TBEGINC,
-			BUILT_IN_MD, NULL, NULL_TREE);
-
-  /* void foo (int) */
-  ftype = build_function_type_list (void_type_node, integer_type_node,
-				    NULL_TREE);
-  add_builtin_function ("__builtin_tabort", ftype,
-			S390_BUILTIN_TABORT, BUILT_IN_MD, NULL, noreturn_attr);
-  add_builtin_function ("__builtin_tx_assist", ftype,
-			S390_BUILTIN_TX_ASSIST, BUILT_IN_MD, NULL, NULL_TREE);
-
-  /* int foo (void *) */
-  ftype = build_function_type_list (integer_type_node, ptr_type_node, NULL_TREE);
-  add_builtin_function ("__builtin_tbegin", ftype, S390_BUILTIN_TBEGIN,
-			BUILT_IN_MD, NULL, returns_twice_attr);
-  add_builtin_function ("__builtin_tbegin_nofloat", ftype,
-			S390_BUILTIN_TBEGIN_NOFLOAT,
-			BUILT_IN_MD, NULL, returns_twice_attr);
-
-  /* int foo (void *, int) */
-  ftype = build_function_type_list (integer_type_node, ptr_type_node,
-				    integer_type_node, NULL_TREE);
-  add_builtin_function ("__builtin_tbegin_retry", ftype,
-			S390_BUILTIN_TBEGIN_RETRY,
-			BUILT_IN_MD,
-			NULL, returns_twice_attr);
-  add_builtin_function ("__builtin_tbegin_retry_nofloat", ftype,
-			S390_BUILTIN_TBEGIN_RETRY_NOFLOAT,
-			BUILT_IN_MD,
-			NULL, returns_twice_attr);
-
-  /* int foo (void) */
-  ftype = build_function_type_list (integer_type_node, NULL_TREE);
-  add_builtin_function ("__builtin_tx_nesting_depth", ftype,
-			S390_BUILTIN_TX_NESTING_DEPTH,
-			BUILT_IN_MD, NULL, NULL_TREE);
-  add_builtin_function ("__builtin_tend", ftype,
-			S390_BUILTIN_TEND, BUILT_IN_MD,	NULL, NULL_TREE);
-
-  /* void foo (uint64_t *, uint64_t) */
-  if (TARGET_64BIT)
-    uint64_type = long_unsigned_type_node;
-  else
-    uint64_type = long_long_unsigned_type_node;
-
-   ftype = build_function_type_list (void_type_node,
- 				    build_pointer_type (uint64_type),
-				    uint64_type, NULL_TREE);
-  add_builtin_function ("__builtin_non_tx_store", ftype,
-			S390_BUILTIN_NON_TX_STORE,
-			BUILT_IN_MD, NULL, NULL_TREE);
-}
-
-/* Expand an expression EXP that calls a built-in function,
-   with result going to TARGET if that's convenient
-   (and in mode MODE if that's convenient).
-   SUBTARGET may be used as the target for computing one of EXP's operands.
-   IGNORE is nonzero if the value is to be ignored.  */
-
-static rtx
-s390_expand_builtin (tree exp, rtx target, rtx subtarget ATTRIBUTE_UNUSED,
-		     enum machine_mode mode ATTRIBUTE_UNUSED,
-		     int ignore ATTRIBUTE_UNUSED)
-{
-#define MAX_ARGS 2
-
-  tree fndecl = TREE_OPERAND (CALL_EXPR_FN (exp), 0);
-  unsigned int fcode = DECL_FUNCTION_CODE (fndecl);
-  enum insn_code icode;
-  rtx op[MAX_ARGS], pat;
-  int arity;
-  bool nonvoid;
-  tree arg;
-  call_expr_arg_iterator iter;
-
-  if (fcode >= S390_BUILTIN_max)
-    internal_error ("bad builtin fcode");
-  icode = code_for_builtin[fcode];
-  if (icode == 0)
-    internal_error ("bad builtin fcode");
-
-  if (!TARGET_HTM)
-    error ("Transactional execution builtins not enabled (-mhtm)\n");
-
-  /* Set a flag in the machine specific cfun part in order to support
-     saving/restoring of FPRs.  */
-  if (fcode == S390_BUILTIN_TBEGIN || fcode == S390_BUILTIN_TBEGIN_RETRY)
-    cfun->machine->tbegin_p = true;
-
-  nonvoid = TREE_TYPE (TREE_TYPE (fndecl)) != void_type_node;
-
-  arity = 0;
-  FOR_EACH_CALL_EXPR_ARG (arg, iter, exp)
-    {
-      const struct insn_operand_data *insn_op;
-
-      if (arg == error_mark_node)
-	return NULL_RTX;
-      if (arity >= MAX_ARGS)
-	return NULL_RTX;
-
-      insn_op = &insn_data[icode].operand[arity + nonvoid];
-
-      op[arity] = expand_expr (arg, NULL_RTX, insn_op->mode, EXPAND_NORMAL);
-
-      if (!(*insn_op->predicate) (op[arity], insn_op->mode))
-	{
-	  if (insn_op->predicate == memory_operand)
-	    {
-	      /* Don't move a NULL pointer into a register. Otherwise
-		 we have to rely on combine being able to move it back
-		 in order to get an immediate 0 in the instruction.  */
-	      if (op[arity] != const0_rtx)
-		op[arity] = copy_to_mode_reg (Pmode, op[arity]);
-	      op[arity] = gen_rtx_MEM (insn_op->mode, op[arity]);
-	    }
-	  else
-	    op[arity] = copy_to_mode_reg (insn_op->mode, op[arity]);
-	}
-
-      arity++;
-    }
-
-  if (nonvoid)
-    {
-      enum machine_mode tmode = insn_data[icode].operand[0].mode;
-      if (!target
-	  || GET_MODE (target) != tmode
-	  || !(*insn_data[icode].operand[0].predicate) (target, tmode))
-	target = gen_reg_rtx (tmode);
-    }
-
-  switch (arity)
-    {
-    case 0:
-      pat = GEN_FCN (icode) (target);
-      break;
-    case 1:
-      if (nonvoid)
-        pat = GEN_FCN (icode) (target, op[0]);
-      else
-	pat = GEN_FCN (icode) (op[0]);
-      break;
-    case 2:
-      if (nonvoid)
-	pat = GEN_FCN (icode) (target, op[0], op[1]);
-      else
-	pat = GEN_FCN (icode) (op[0], op[1]);
-      break;
-    default:
-      gcc_unreachable ();
-    }
-  if (!pat)
-    return NULL_RTX;
-  emit_insn (pat);
-
-  if (nonvoid)
-    return target;
-  else
-    return const0_rtx;
-}
-
-
 /* Output assembly code for the trampoline template to
    stdio stream FILE.
 
@@ -10496,15 +12203,18 @@ s390_call_saved_register_used (tree call
       mode = TYPE_MODE (type);
       gcc_assert (mode);
 
+      /* We assume that in the target function all parameters are
+	 named.  This only has an impact on vector argument register
+	 usage none of which is call-saved.  */
       if (pass_by_reference (&cum_v, mode, type, true))
  	{
  	  mode = Pmode;
  	  type = build_pointer_type (type);
  	}
 
-       parm_rtx = s390_function_arg (cum, mode, type, 0);
+       parm_rtx = s390_function_arg (cum, mode, type, true);
 
-       s390_function_arg_advance (cum, mode, type, 0);
+       s390_function_arg_advance (cum, mode, type, true);
 
        if (!parm_rtx)
 	 continue;
@@ -10711,6 +12421,13 @@ s390_conditional_register_usage (void)
       for (i = 16; i < 32; i++)
 	call_used_regs[i] = fixed_regs[i] = 1;
     }
+
+  /* Disable v16 - v31 for non-vector target.  */
+  if (!TARGET_VX)
+    {
+      for (i = VR16_REGNUM; i <= VR31_REGNUM; i++)
+	fixed_regs[i] = call_used_regs[i] = call_really_used_regs[i] = 1;
+    }
 }
 
 /* Corresponding function to eh_return expander.  */
@@ -11232,7 +12949,8 @@ s390_reorg (void)
   /* Walk over the insns and do some >=z10 specific changes.  */
   if (s390_tune == PROCESSOR_2097_Z10
       || s390_tune == PROCESSOR_2817_Z196
-      || s390_tune == PROCESSOR_2827_ZEC12)
+      || s390_tune == PROCESSOR_2827_ZEC12
+      || s390_tune == PROCESSOR_2964_Z13)
     {
       rtx insn;
       bool insn_added_p = false;
@@ -11392,27 +13110,66 @@ s390_z10_prevent_earlyload_conflicts (rt
 
 static int s390_sched_state;
 
-#define S390_OOO_SCHED_STATE_NORMAL  3
-#define S390_OOO_SCHED_STATE_CRACKED 4
+#define S390_SCHED_STATE_NORMAL  3
+#define S390_SCHED_STATE_CRACKED 4
 
-#define S390_OOO_SCHED_ATTR_MASK_CRACKED    0x1
-#define S390_OOO_SCHED_ATTR_MASK_EXPANDED   0x2
-#define S390_OOO_SCHED_ATTR_MASK_ENDGROUP   0x4
-#define S390_OOO_SCHED_ATTR_MASK_GROUPALONE 0x8
+#define S390_SCHED_ATTR_MASK_CRACKED    0x1
+#define S390_SCHED_ATTR_MASK_EXPANDED   0x2
+#define S390_SCHED_ATTR_MASK_ENDGROUP   0x4
+#define S390_SCHED_ATTR_MASK_GROUPALONE 0x8
 
 static unsigned int
 s390_get_sched_attrmask (rtx insn)
 {
   unsigned int mask = 0;
 
-  if (get_attr_ooo_cracked (insn))
-    mask |= S390_OOO_SCHED_ATTR_MASK_CRACKED;
-  if (get_attr_ooo_expanded (insn))
-    mask |= S390_OOO_SCHED_ATTR_MASK_EXPANDED;
-  if (get_attr_ooo_endgroup (insn))
-    mask |= S390_OOO_SCHED_ATTR_MASK_ENDGROUP;
-  if (get_attr_ooo_groupalone (insn))
-    mask |= S390_OOO_SCHED_ATTR_MASK_GROUPALONE;
+  switch (s390_tune)
+    {
+    case PROCESSOR_2827_ZEC12:
+      if (get_attr_zEC12_cracked (insn))
+	mask |= S390_SCHED_ATTR_MASK_CRACKED;
+      if (get_attr_zEC12_expanded (insn))
+	mask |= S390_SCHED_ATTR_MASK_EXPANDED;
+      if (get_attr_zEC12_endgroup (insn))
+	mask |= S390_SCHED_ATTR_MASK_ENDGROUP;
+      if (get_attr_zEC12_groupalone (insn))
+	mask |= S390_SCHED_ATTR_MASK_GROUPALONE;
+      break;
+    case PROCESSOR_2964_Z13:
+      if (get_attr_z13_cracked (insn))
+	mask |= S390_SCHED_ATTR_MASK_CRACKED;
+      if (get_attr_z13_expanded (insn))
+	mask |= S390_SCHED_ATTR_MASK_EXPANDED;
+      if (get_attr_z13_endgroup (insn))
+	mask |= S390_SCHED_ATTR_MASK_ENDGROUP;
+      if (get_attr_z13_groupalone (insn))
+	mask |= S390_SCHED_ATTR_MASK_GROUPALONE;
+      break;
+    default:
+      gcc_unreachable ();
+    }
+  return mask;
+}
+
+static unsigned int
+s390_get_unit_mask (rtx insn, int *units)
+{
+  unsigned int mask = 0;
+
+  switch (s390_tune)
+    {
+    case PROCESSOR_2964_Z13:
+      *units = 3;
+      if (get_attr_z13_unit_lsu (insn))
+	mask |= 1 << 0;
+      if (get_attr_z13_unit_fxu (insn))
+	mask |= 1 << 1;
+      if (get_attr_z13_unit_vfu (insn))
+	mask |= 1 << 2;
+      break;
+    default:
+      gcc_unreachable ();
+    }
   return mask;
 }
 
@@ -11430,48 +13187,66 @@ s390_sched_score (rtx insn)
     case 0:
       /* Try to put insns into the first slot which would otherwise
 	 break a group.  */
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_CRACKED) != 0
-	  || (mask & S390_OOO_SCHED_ATTR_MASK_EXPANDED) != 0)
+      if ((mask & S390_SCHED_ATTR_MASK_CRACKED) != 0
+	  || (mask & S390_SCHED_ATTR_MASK_EXPANDED) != 0)
 	score += 5;
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_GROUPALONE) != 0)
+      if ((mask & S390_SCHED_ATTR_MASK_GROUPALONE) != 0)
 	score += 10;
     case 1:
       /* Prefer not cracked insns while trying to put together a
 	 group.  */
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_CRACKED) == 0
-	  && (mask & S390_OOO_SCHED_ATTR_MASK_EXPANDED) == 0
-	  && (mask & S390_OOO_SCHED_ATTR_MASK_GROUPALONE) == 0)
+      if ((mask & S390_SCHED_ATTR_MASK_CRACKED) == 0
+	  && (mask & S390_SCHED_ATTR_MASK_EXPANDED) == 0
+	  && (mask & S390_SCHED_ATTR_MASK_GROUPALONE) == 0)
 	score += 10;
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_ENDGROUP) == 0)
+      if ((mask & S390_SCHED_ATTR_MASK_ENDGROUP) == 0)
 	score += 5;
       break;
     case 2:
       /* Prefer not cracked insns while trying to put together a
 	 group.  */
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_CRACKED) == 0
-	  && (mask & S390_OOO_SCHED_ATTR_MASK_EXPANDED) == 0
-	  && (mask & S390_OOO_SCHED_ATTR_MASK_GROUPALONE) == 0)
+      if ((mask & S390_SCHED_ATTR_MASK_CRACKED) == 0
+	  && (mask & S390_SCHED_ATTR_MASK_EXPANDED) == 0
+	  && (mask & S390_SCHED_ATTR_MASK_GROUPALONE) == 0)
 	score += 10;
       /* Prefer endgroup insns in the last slot.  */
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_ENDGROUP) != 0)
+      if ((mask & S390_SCHED_ATTR_MASK_ENDGROUP) != 0)
 	score += 10;
       break;
-    case S390_OOO_SCHED_STATE_NORMAL:
+    case S390_SCHED_STATE_NORMAL:
       /* Prefer not cracked insns if the last was not cracked.  */
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_CRACKED) == 0
-	  && (mask & S390_OOO_SCHED_ATTR_MASK_EXPANDED) == 0)
+      if ((mask & S390_SCHED_ATTR_MASK_CRACKED) == 0
+	  && (mask & S390_SCHED_ATTR_MASK_EXPANDED) == 0)
 	score += 5;
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_GROUPALONE) != 0)
+      if ((mask & S390_SCHED_ATTR_MASK_GROUPALONE) != 0)
 	score += 10;
       break;
-    case S390_OOO_SCHED_STATE_CRACKED:
+    case S390_SCHED_STATE_CRACKED:
       /* Try to keep cracked insns together to prevent them from
 	 interrupting groups.  */
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_CRACKED) != 0
-	  || (mask & S390_OOO_SCHED_ATTR_MASK_EXPANDED) != 0)
+      if ((mask & S390_SCHED_ATTR_MASK_CRACKED) != 0
+	  || (mask & S390_SCHED_ATTR_MASK_EXPANDED) != 0)
 	score += 5;
       break;
     }
+
+  if (s390_tune == PROCESSOR_2964_Z13)
+    {
+      int units, i;
+      unsigned unit_mask, m = 1;
+
+      unit_mask = s390_get_unit_mask (insn, &units);
+      gcc_assert (units <= MAX_SCHED_UNITS);
+
+      /* Add a score in range 0..MAX_SCHED_MIX_SCORE depending on how long
+	 ago the last insn of this unit type got scheduled.  This is
+	 supposed to help providing a proper instruction mix to the
+	 CPU.  */
+      for (i = 0; i < units; i++, m <<= 1)
+	if (m & unit_mask)
+	  score += (last_scheduled_unit_distance[i] * MAX_SCHED_MIX_SCORE /
+		    MAX_SCHED_MIX_DISTANCE);
+    }
   return score;
 }
 
@@ -11487,7 +13262,8 @@ s390_sched_reorder (FILE *file, int verb
     if (reload_completed && *nreadyp > 1)
       s390_z10_prevent_earlyload_conflicts (ready, nreadyp);
 
-  if (s390_tune == PROCESSOR_2827_ZEC12
+  if ((s390_tune == PROCESSOR_2827_ZEC12
+       || s390_tune == PROCESSOR_2964_Z13)
       && reload_completed
       && *nreadyp > 1)
     {
@@ -11526,12 +13302,12 @@ s390_sched_reorder (FILE *file, int verb
 
 	      if (verbose > 5)
 		fprintf (file,
-			 "move insn %d to the top of list\n",
+			 ";;\t\tBACKEND: move insn %d to the top of list\n",
 			 INSN_UID (ready[last_index]));
 	    }
 	  else if (verbose > 5)
 	    fprintf (file,
-		     "best insn %d already on top\n",
+		     ";;\t\tBACKEND: best insn %d already on top\n",
 		     INSN_UID (ready[last_index]));
 	}
 
@@ -11542,16 +13318,35 @@ s390_sched_reorder (FILE *file, int verb
 
 	  for (i = last_index; i >= 0; i--)
 	    {
-	      if (recog_memoized (ready[i]) < 0)
+	      unsigned int sched_mask;
+	      rtx insn = ready[i];
+
+	      if (recog_memoized (insn) < 0)
 		continue;
-	      fprintf (file, "insn %d score: %d: ", INSN_UID (ready[i]),
-		       s390_sched_score (ready[i]));
-#define PRINT_OOO_ATTR(ATTR) fprintf (file, "%s ", get_attr_##ATTR (ready[i]) ? #ATTR : "!" #ATTR);
-	      PRINT_OOO_ATTR (ooo_cracked);
-	      PRINT_OOO_ATTR (ooo_expanded);
-	      PRINT_OOO_ATTR (ooo_endgroup);
-	      PRINT_OOO_ATTR (ooo_groupalone);
-#undef PRINT_OOO_ATTR
+
+	      sched_mask = s390_get_sched_attrmask (insn);
+	      fprintf (file, ";;\t\tBACKEND: insn %d score: %d: ",
+		       INSN_UID (insn),
+		       s390_sched_score (insn));
+#define PRINT_SCHED_ATTR(M, ATTR) fprintf (file, "%s ",\
+					   ((M) & sched_mask) ? #ATTR : "");
+	      PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_CRACKED, cracked);
+	      PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_EXPANDED, expanded);
+	      PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_ENDGROUP, endgroup);
+	      PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_GROUPALONE, groupalone);
+#undef PRINT_SCHED_ATTR
+	      if (s390_tune == PROCESSOR_2964_Z13)
+		{
+		  unsigned int unit_mask, m = 1;
+		  int units, j;
+
+		  unit_mask  = s390_get_unit_mask (insn, &units);
+		  fprintf (file, "(units:");
+		  for (j = 0; j < units; j++, m <<= 1)
+		    if (m & unit_mask)
+		      fprintf (file, " u%d", j);
+		  fprintf (file, ")");
+		}
 	      fprintf (file, "\n");
 	    }
 	}
@@ -11570,18 +13365,19 @@ s390_sched_variable_issue (FILE *file, i
 {
   last_scheduled_insn = insn;
 
-  if (s390_tune == PROCESSOR_2827_ZEC12
+  if ((s390_tune == PROCESSOR_2827_ZEC12
+       || s390_tune == PROCESSOR_2964_Z13)
       && reload_completed
       && recog_memoized (insn) >= 0)
     {
       unsigned int mask = s390_get_sched_attrmask (insn);
 
-      if ((mask & S390_OOO_SCHED_ATTR_MASK_CRACKED) != 0
-	  || (mask & S390_OOO_SCHED_ATTR_MASK_EXPANDED) != 0)
-	s390_sched_state = S390_OOO_SCHED_STATE_CRACKED;
-      else if ((mask & S390_OOO_SCHED_ATTR_MASK_ENDGROUP) != 0
-	       || (mask & S390_OOO_SCHED_ATTR_MASK_GROUPALONE) != 0)
-	s390_sched_state = S390_OOO_SCHED_STATE_NORMAL;
+      if ((mask & S390_SCHED_ATTR_MASK_CRACKED) != 0
+	  || (mask & S390_SCHED_ATTR_MASK_EXPANDED) != 0)
+	s390_sched_state = S390_SCHED_STATE_CRACKED;
+      else if ((mask & S390_SCHED_ATTR_MASK_ENDGROUP) != 0
+	       || (mask & S390_SCHED_ATTR_MASK_GROUPALONE) != 0)
+	s390_sched_state = S390_SCHED_STATE_NORMAL;
       else
 	{
 	  /* Only normal insns are left (mask == 0).  */
@@ -11590,30 +13386,73 @@ s390_sched_variable_issue (FILE *file, i
 	    case 0:
 	    case 1:
 	    case 2:
-	    case S390_OOO_SCHED_STATE_NORMAL:
-	      if (s390_sched_state == S390_OOO_SCHED_STATE_NORMAL)
+	    case S390_SCHED_STATE_NORMAL:
+	      if (s390_sched_state == S390_SCHED_STATE_NORMAL)
 		s390_sched_state = 1;
 	      else
 		s390_sched_state++;
 
 	      break;
-	    case S390_OOO_SCHED_STATE_CRACKED:
-	      s390_sched_state = S390_OOO_SCHED_STATE_NORMAL;
+	    case S390_SCHED_STATE_CRACKED:
+	      s390_sched_state = S390_SCHED_STATE_NORMAL;
 	      break;
 	    }
 	}
+
+      if (s390_tune == PROCESSOR_2964_Z13)
+	{
+	  int units, i;
+	  unsigned unit_mask, m = 1;
+
+	  unit_mask = s390_get_unit_mask (insn, &units);
+	  gcc_assert (units <= MAX_SCHED_UNITS);
+
+	  for (i = 0; i < units; i++, m <<= 1)
+	    if (m & unit_mask)
+	      last_scheduled_unit_distance[i] = 0;
+	    else if (last_scheduled_unit_distance[i] < MAX_SCHED_MIX_DISTANCE)
+	      last_scheduled_unit_distance[i]++;
+	}
+
       if (verbose > 5)
 	{
-	  fprintf (file, "insn %d: ", INSN_UID (insn));
-#define PRINT_OOO_ATTR(ATTR)						\
-	  fprintf (file, "%s ", get_attr_##ATTR (insn) ? #ATTR : "");
-	  PRINT_OOO_ATTR (ooo_cracked);
-	  PRINT_OOO_ATTR (ooo_expanded);
-	  PRINT_OOO_ATTR (ooo_endgroup);
-	  PRINT_OOO_ATTR (ooo_groupalone);
-#undef PRINT_OOO_ATTR
-	  fprintf (file, "\n");
-	  fprintf (file, "sched state: %d\n", s390_sched_state);
+	  unsigned int sched_mask;
+
+	  sched_mask = s390_get_sched_attrmask (insn);
+
+	  fprintf (file, ";;\t\tBACKEND: insn %d: ", INSN_UID (insn));
+#define PRINT_SCHED_ATTR(M, ATTR) fprintf (file, "%s ", ((M) & sched_mask) ? #ATTR : "");
+	  PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_CRACKED, cracked);
+	  PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_EXPANDED, expanded);
+	  PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_ENDGROUP, endgroup);
+	  PRINT_SCHED_ATTR (S390_SCHED_ATTR_MASK_GROUPALONE, groupalone);
+#undef PRINT_SCHED_ATTR
+
+	  if (s390_tune == PROCESSOR_2964_Z13)
+	    {
+	      unsigned int unit_mask, m = 1;
+	      int units, j;
+
+	      unit_mask  = s390_get_unit_mask (insn, &units);
+	      fprintf (file, "(units:");
+	      for (j = 0; j < units; j++, m <<= 1)
+		if (m & unit_mask)
+		  fprintf (file, " %d", j);
+	      fprintf (file, ")");
+	    }
+	  fprintf (file, " sched state: %d\n", s390_sched_state);
+
+	  if (s390_tune == PROCESSOR_2964_Z13)
+	    {
+	      int units, j;
+
+	      s390_get_unit_mask (insn, &units);
+
+	      fprintf (file, ";;\t\tBACKEND: units unused for: ");
+	      for (j = 0; j < units; j++)
+		fprintf (file, "%d:%d ", j, last_scheduled_unit_distance[j]);
+	      fprintf (file, "\n");
+	    }
 	}
     }
 
@@ -11630,6 +13469,7 @@ s390_sched_init (FILE *file ATTRIBUTE_UN
 		 int max_ready ATTRIBUTE_UNUSED)
 {
   last_scheduled_insn = NULL_RTX;
+  memset (last_scheduled_unit_distance, 0, MAX_SCHED_UNITS * sizeof (int));
   s390_sched_state = 0;
 }
 
@@ -11663,7 +13503,8 @@ s390_loop_unroll_adjust (unsigned nunrol
 
   if (s390_tune != PROCESSOR_2097_Z10
       && s390_tune != PROCESSOR_2817_Z196
-      && s390_tune != PROCESSOR_2827_ZEC12)
+      && s390_tune != PROCESSOR_2827_ZEC12
+      && s390_tune != PROCESSOR_2964_Z13)
     return nunroll;
 
   /* Count the number of memory references within the loop body.  */
@@ -11691,6 +13532,84 @@ s390_loop_unroll_adjust (unsigned nunrol
     }
 }
 
+/* Return the vector mode to be used for inner mode MODE when doing
+   vectorization.  */
+static enum machine_mode
+s390_preferred_simd_mode (enum machine_mode mode)
+{
+  if (TARGET_VX)
+    switch (mode)
+      {
+      case DFmode:
+	return V2DFmode;
+      case DImode:
+	return V2DImode;
+      case SImode:
+	return V4SImode;
+      case HImode:
+	return V8HImode;
+      case QImode:
+	return V16QImode;
+      default:;
+      }
+  return word_mode;
+}
+
+/* Our hardware does not require vectors to be strictly aligned.  */
+static bool
+s390_support_vector_misalignment (enum machine_mode mode ATTRIBUTE_UNUSED,
+				  const_tree type ATTRIBUTE_UNUSED,
+				  int misalignment ATTRIBUTE_UNUSED,
+				  bool is_packed ATTRIBUTE_UNUSED)
+{
+  if (TARGET_VX)
+    return true;
+
+  return default_builtin_support_vector_misalignment (mode, type, misalignment,
+						      is_packed);
+}
+
+/* The vector ABI requires vector types to be aligned on an 8 byte
+   boundary (our stack alignment).  However, we allow this to be
+   overriden by the user, while this definitely breaks the ABI.  */
+static HOST_WIDE_INT
+s390_vector_alignment (const_tree type)
+{
+  if (!TARGET_VX_ABI)
+    return default_vector_alignment (type);
+
+  if (TYPE_USER_ALIGN (type))
+    return TYPE_ALIGN (type);
+
+  return MIN (64, tree_low_cst (TYPE_SIZE (type), 0));
+}
+
+/* Implement TARGET_ASM_FILE_END.  */
+static void
+s390_asm_file_end (void)
+{
+#ifdef HAVE_AS_GNU_ATTRIBUTE
+  varpool_node *vnode;
+  cgraph_node *cnode;
+
+  FOR_EACH_VARIABLE (vnode)
+    if (TREE_PUBLIC (vnode->symbol.decl))
+      s390_check_type_for_vector_abi (TREE_TYPE (vnode->symbol.decl),
+				      false, false);
+
+  FOR_EACH_FUNCTION (cnode)
+    if (TREE_PUBLIC (cnode->symbol.decl))
+      s390_check_type_for_vector_abi (TREE_TYPE (cnode->symbol.decl),
+				      false, false);
+
+
+  if (s390_vector_abi != 0)
+    fprintf (asm_out_file, "\t.gnu_attribute 8, %d\n",
+	     s390_vector_abi);
+#endif
+  file_end_indicate_exec_stack ();
+}
+
 /* Initialize GCC target structure.  */
 
 #undef  TARGET_ASM_ALIGNED_HI_OP
@@ -11797,6 +13716,8 @@ s390_loop_unroll_adjust (unsigned nunrol
 #define TARGET_FUNCTION_VALUE s390_function_value
 #undef TARGET_LIBCALL_VALUE
 #define TARGET_LIBCALL_VALUE s390_libcall_value
+#undef TARGET_STRICT_ARGUMENT_NAMING
+#define TARGET_STRICT_ARGUMENT_NAMING hook_bool_CUMULATIVE_ARGS_true
 
 #undef TARGET_FIXED_CONDITION_CODE_REGS
 #define TARGET_FIXED_CONDITION_CODE_REGS s390_fixed_condition_code_regs
@@ -11812,6 +13733,9 @@ s390_loop_unroll_adjust (unsigned nunrol
 #define TARGET_ASM_OUTPUT_DWARF_DTPREL s390_output_dwarf_dtprel
 #endif
 
+#undef TARGET_DWARF_FRAME_REG_MODE
+#define TARGET_DWARF_FRAME_REG_MODE s390_dwarf_frame_reg_mode
+
 #ifdef TARGET_ALTERNATE_LONG_DOUBLE_MANGLING
 #undef TARGET_MANGLE_TYPE
 #define TARGET_MANGLE_TYPE s390_mangle_type
@@ -11820,6 +13744,9 @@ s390_loop_unroll_adjust (unsigned nunrol
 #undef TARGET_SCALAR_MODE_SUPPORTED_P
 #define TARGET_SCALAR_MODE_SUPPORTED_P s390_scalar_mode_supported_p
 
+#undef TARGET_VECTOR_MODE_SUPPORTED_P
+#define TARGET_VECTOR_MODE_SUPPORTED_P s390_vector_mode_supported_p
+
 #undef  TARGET_PREFERRED_RELOAD_CLASS
 #define TARGET_PREFERRED_RELOAD_CLASS s390_preferred_reload_class
 
@@ -11864,6 +13791,21 @@ s390_loop_unroll_adjust (unsigned nunrol
 #undef TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P
 #define TARGET_FUNCTION_ATTRIBUTE_INLINABLE_P hook_bool_const_tree_true
 
+#undef TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN
+#define TARGET_INVALID_ARG_FOR_UNPROTOTYPED_FN s390_invalid_arg_for_unprototyped_fn
+
+#undef TARGET_VECTORIZE_PREFERRED_SIMD_MODE
+#define TARGET_VECTORIZE_PREFERRED_SIMD_MODE s390_preferred_simd_mode
+
+#undef TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT
+#define TARGET_VECTORIZE_SUPPORT_VECTOR_MISALIGNMENT s390_support_vector_misalignment
+
+#undef TARGET_VECTOR_ALIGNMENT
+#define TARGET_VECTOR_ALIGNMENT s390_vector_alignment
+
+#undef TARGET_ASM_FILE_END
+#define TARGET_ASM_FILE_END s390_asm_file_end
+
 struct gcc_target targetm = TARGET_INITIALIZER;
 
 #include "gt-s390.h"
--- gcc/config/s390/s390-c.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/s390-c.c	2016-05-11 19:20:42.792826040 +0200
@@ -0,0 +1,903 @@
+/* Language specific subroutines used for code generation on IBM S/390
+   and zSeries
+   Copyright (C) 2015 Free Software Foundation, Inc.
+
+   Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com).
+
+   This file is part of GCC.
+
+   GCC is free software; you can redistribute it and/or modify it
+   under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 3, or (at your option)
+   any later version.
+
+   GCC is distributed in the hope that it will be useful, but WITHOUT
+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public
+   License for more details.
+
+   You should have received a copy of the GNU General Public License
+   along with GCC; see the file COPYING3.  If not see
+   <http://www.gnu.org/licenses/>.
+
+   Based on gcc/config/rs6000/rs6000-c.c.
+
+   In GCC terms this file belongs to the frontend.  It will be
+   compiled with -DIN_GCC_FRONTEND.  With that rtl.h cannot be
+   included anymore - a mechanism supposed to avoid adding frontend -
+   backend dependencies.  */
+
+#include "config.h"
+#include "system.h"
+#include "coretypes.h"
+#include "tm.h"
+#include "cpplib.h"
+#include "machmode.h"
+#include "vec.h"
+#include "double-int.h"
+#include "input.h"
+#include "alias.h"
+#include "symtab.h"
+#include "tree.h"
+#include "c-family/c-common.h"
+#include "c-family/c-pragma.h"
+#include "diagnostic-core.h"
+#include "tm_p.h"
+#include "target.h"
+#include "langhooks.h"
+#include "tree-pretty-print.h"
+#include "c/c-tree.h"
+
+#include "s390-builtins.h"
+
+static GTY(()) tree __vector_keyword;
+static GTY(()) tree vector_keyword;
+static GTY(()) tree __bool_keyword;
+static GTY(()) tree bool_keyword;
+static GTY(()) tree _Bool_keyword;
+
+
+/* Generate an array holding all the descriptions of variants of
+   overloaded builtins defined with OB_DEF_VAR in
+   s390-builtins.def.  */
+static enum s390_builtin_ov_type_index
+type_for_overloaded_builtin_var[S390_OVERLOADED_BUILTIN_VAR_MAX + 1] =
+  {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(...)
+#define OB_DEF_VAR(NAME, PATTERN, FLAGS, FNTYPE) FNTYPE,
+#include "s390-builtins.def"
+    BT_OV_MAX
+  };
+
+
+/* Generate an array indexed by an overloaded builtin index returning
+   the first index in desc_for_overloaded_builtin_var where the
+   variants for the builtin can be found.  */
+static enum s390_overloaded_builtin_vars
+desc_start_for_overloaded_builtin[S390_OVERLOADED_BUILTIN_MAX + 1] =
+  {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(NAME, FIRST_VAR_NAME,...)	\
+    S390_OVERLOADED_BUILTIN_VAR_##FIRST_VAR_NAME,
+#define OB_DEF_VAR(...)
+    #include "s390-builtins.def"
+    S390_OVERLOADED_BUILTIN_VAR_MAX
+  };
+
+/* Generate an array indexed by an overloaded builtin index returning
+   the last index in desc_for_overloaded_builtin_var where the
+   variants for the builtin can be found.  */
+static enum s390_overloaded_builtin_vars
+desc_end_for_overloaded_builtin[S390_OVERLOADED_BUILTIN_MAX + 1] =
+  {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(NAME, FIRST_VAR_NAME, LAST_VAR_NAME,...)	\
+    S390_OVERLOADED_BUILTIN_VAR_##LAST_VAR_NAME,
+#define OB_DEF_VAR(...)
+    #include "s390-builtins.def"
+    S390_OVERLOADED_BUILTIN_VAR_MAX
+  };
+
+static enum s390_builtin_type_index
+s390_builtin_ov_types[BT_OV_MAX][MAX_OV_OPERANDS] =
+  {
+#undef DEF_TYPE
+#undef DEF_POINTER_TYPE
+#undef DEF_DISTINCT_TYPE
+#undef DEF_VECTOR_TYPE
+#undef DEF_OPAQUE_VECTOR_TYPE
+#undef DEF_FN_TYPE
+#undef DEF_OV_TYPE
+#define DEF_TYPE(...)
+#define DEF_POINTER_TYPE(...)
+#define DEF_DISTINCT_TYPE(...)
+#define DEF_VECTOR_TYPE(...)
+#define DEF_OPAQUE_VECTOR_TYPE(...)
+#define DEF_FN_TYPE(...)
+#define DEF_OV_TYPE(INDEX, args...) { args },
+#include "s390-builtin-types.def"
+  };
+
+static const enum s390_builtins
+bt_for_overloaded_builtin_var[S390_OVERLOADED_BUILTIN_VAR_MAX] = {
+#undef B_DEF
+#undef OB_DEF
+#undef OB_DEF_VAR
+#define B_DEF(...)
+#define OB_DEF(...)
+#define OB_DEF_VAR(NAME, BT, ...) S390_BUILTIN_##BT,
+
+#include "s390-builtins.def"
+  };
+
+/* In addition to calling fold_convert for EXPR of type TYPE, also
+   call c_fully_fold to remove any C_MAYBE_CONST_EXPRs that could be
+   hiding there (PR47197).  */
+tree
+fully_fold_convert (tree type, tree expr)
+{
+  tree result = fold_convert (type, expr);
+  bool maybe_const = true;
+
+  if (!c_dialect_cxx ())
+    result = c_fully_fold (result, false, &maybe_const);
+
+  return result;
+}
+
+/* Unify the different variants to the same nodes in order to keep the
+   code working with it simple.  */
+static cpp_hashnode *
+s390_categorize_keyword (const cpp_token *tok)
+{
+  if (tok->type == CPP_NAME)
+    {
+      cpp_hashnode *ident = tok->val.node.node;
+
+      if (ident == C_CPP_HASHNODE (vector_keyword))
+	return C_CPP_HASHNODE (__vector_keyword);
+
+      if (ident == C_CPP_HASHNODE (bool_keyword))
+	return C_CPP_HASHNODE (__bool_keyword);
+
+      if (ident == C_CPP_HASHNODE (_Bool_keyword))
+	return C_CPP_HASHNODE (__bool_keyword);
+      return ident;
+    }
+
+  return 0;
+}
+
+
+/* Called to decide whether a conditional macro should be expanded.
+   Since we have exactly one such macro (i.e, 'vector'), we do not
+   need to examine the 'tok' parameter.  */
+
+static cpp_hashnode *
+s390_macro_to_expand (cpp_reader *pfile, const cpp_token *tok)
+{
+  cpp_hashnode *expand_this = tok->val.node.node;
+  cpp_hashnode *ident;
+  static bool expand_bool_p = false;
+  int idx = 0;
+  enum rid rid_code;
+
+  /* The vector keyword is only expanded if the machine actually
+     provides hardware support.  */
+  if (!TARGET_ZVECTOR)
+    return NULL;
+
+  ident = s390_categorize_keyword (tok);
+
+  /* Triggered when we picked a different variant in
+     s390_categorize_keyword.  */
+  if (ident != expand_this)
+    expand_this = NULL;
+
+  /* The vector keyword has been found already and we remembered to
+     expand the next bool.  */
+  if (expand_bool_p && ident == C_CPP_HASHNODE (__bool_keyword))
+    {
+      expand_bool_p = false;
+      return ident;
+    }
+
+  if (ident != C_CPP_HASHNODE (__vector_keyword))
+    return expand_this;
+
+  do
+    tok = cpp_peek_token (pfile, idx++);
+  while (tok->type == CPP_PADDING);
+  ident = s390_categorize_keyword (tok);
+
+  if (!ident)
+    return expand_this;
+
+  /* vector bool - remember to expand the next bool. */
+  if (ident == C_CPP_HASHNODE (__bool_keyword))
+    {
+      expand_bool_p = true;
+      return C_CPP_HASHNODE (__vector_keyword);
+    }
+
+  /* The boost libraries have code with Iterator::vector vector in it.
+     If we allow the normal handling, this module will be called
+     recursively, and the vector will be skipped.; */
+  if (ident == C_CPP_HASHNODE (__vector_keyword))
+    return expand_this;
+
+  rid_code = (enum rid)(ident->rid_code);
+
+  if (ident->type == NT_MACRO)
+    {
+      /* Now actually fetch the tokens we "peeked" before and do a
+	 lookahead for the next.  */
+      do
+	(void) cpp_get_token (pfile);
+      while (--idx > 0);
+      do
+	tok = cpp_peek_token (pfile, idx++);
+      while (tok->type == CPP_PADDING);
+      ident = s390_categorize_keyword (tok);
+
+      if (ident == C_CPP_HASHNODE (__bool_keyword))
+	{
+	  expand_bool_p = true;
+	  return C_CPP_HASHNODE (__vector_keyword);
+	}
+      else if (ident)
+	rid_code = (enum rid)(ident->rid_code);
+    }
+
+  /* vector keyword followed by type identifier: vector unsigned,
+     vector long, ...
+     Types consisting of more than one identifier are not supported by
+     zvector e.g. long long, long double, unsigned long int.  */
+  if (rid_code == RID_UNSIGNED || rid_code == RID_LONG
+      || rid_code == RID_SHORT || rid_code == RID_SIGNED
+      || rid_code == RID_INT || rid_code == RID_CHAR
+      || rid_code == RID_DOUBLE)
+    {
+      expand_this = C_CPP_HASHNODE (__vector_keyword);
+      /* If the next keyword is bool, it will need to be expanded as
+	 well.  */
+      do
+	tok = cpp_peek_token (pfile, idx++);
+      while (tok->type == CPP_PADDING);
+      ident = s390_categorize_keyword (tok);
+
+      /* __vector long __bool a; */
+      if (ident == C_CPP_HASHNODE (__bool_keyword))
+	expand_bool_p = true;
+      else
+	{
+	  /* Triggered with: __vector long long __bool a; */
+	  do
+	    tok = cpp_peek_token (pfile, idx++);
+	  while (tok->type == CPP_PADDING);
+	  ident = s390_categorize_keyword (tok);
+
+	  if (ident == C_CPP_HASHNODE (__bool_keyword))
+	    expand_bool_p = true;
+	}
+    }
+
+  return expand_this;
+}
+
+/* Define platform dependent macros.  */
+void
+s390_cpu_cpp_builtins (cpp_reader *pfile)
+{
+  cpp_assert (pfile, "cpu=s390");
+  cpp_assert (pfile, "machine=s390");
+  cpp_define (pfile, "__s390__");
+  if (TARGET_ZARCH)
+    cpp_define (pfile, "__zarch__");
+  if (TARGET_64BIT)
+    cpp_define (pfile, "__s390x__");
+  if (TARGET_LONG_DOUBLE_128)
+    cpp_define (pfile, "__LONG_DOUBLE_128__");
+  if (TARGET_HTM)
+    cpp_define (pfile, "__HTM__");
+  if (TARGET_ZVECTOR)
+    {
+      cpp_define (pfile, "__VEC__=10301");
+      cpp_define (pfile, "__vector=__attribute__((vector_size(16)))");
+      cpp_define (pfile, "__bool=__attribute__((s390_vector_bool)) unsigned");
+
+      if (!flag_iso)
+	{
+	  cpp_define (pfile, "__VECTOR_KEYWORD_SUPPORTED__");
+	  cpp_define (pfile, "vector=vector");
+	  cpp_define (pfile, "bool=bool");
+
+	  __vector_keyword = get_identifier ("__vector");
+	  C_CPP_HASHNODE (__vector_keyword)->flags |= NODE_CONDITIONAL;
+
+	  vector_keyword = get_identifier ("vector");
+	  C_CPP_HASHNODE (vector_keyword)->flags |= NODE_CONDITIONAL;
+
+	  __bool_keyword = get_identifier ("__bool");
+	  C_CPP_HASHNODE (__bool_keyword)->flags |= NODE_CONDITIONAL;
+
+	  bool_keyword = get_identifier ("bool");
+	  C_CPP_HASHNODE (bool_keyword)->flags |= NODE_CONDITIONAL;
+
+	  _Bool_keyword = get_identifier ("_Bool");
+	  C_CPP_HASHNODE (_Bool_keyword)->flags |= NODE_CONDITIONAL;
+
+	  /* Enable context-sensitive macros.  */
+	  cpp_get_callbacks (pfile)->macro_to_expand = s390_macro_to_expand;
+	}
+    }
+}
+
+/* Expand builtins which can directly be mapped to tree expressions.
+   LOC - location information
+   FCODE - function code of the builtin
+   ARGLIST - value supposed to be passed as arguments
+   RETURN-TYPE - expected return type of the builtin */
+static tree
+s390_expand_overloaded_builtin (location_t loc,
+				unsigned fcode,
+				vec<tree, va_gc> *arglist,
+				tree return_type)
+{
+  switch (fcode)
+    {
+    case S390_OVERLOADED_BUILTIN_s390_vec_step:
+      if (TREE_CODE (TREE_TYPE ((*arglist)[0])) != VECTOR_TYPE)
+	{
+	  error_at (loc, "Builtin vec_step can only be used on vector types.");
+	  return error_mark_node;
+	}
+      return build_int_cst (NULL_TREE,
+			    TYPE_VECTOR_SUBPARTS (TREE_TYPE ((*arglist)[0])));
+    case S390_OVERLOADED_BUILTIN_s390_vec_xld2:
+    case S390_OVERLOADED_BUILTIN_s390_vec_xlw4:
+      return build2 (MEM_REF, return_type,
+		     fold_build_pointer_plus ((*arglist)[1], (*arglist)[0]),
+		     build_int_cst (TREE_TYPE ((*arglist)[1]), 0));
+    case S390_OVERLOADED_BUILTIN_s390_vec_xstd2:
+    case S390_OVERLOADED_BUILTIN_s390_vec_xstw4:
+      return build2 (MODIFY_EXPR, TREE_TYPE((*arglist)[0]),
+		     build1 (INDIRECT_REF, TREE_TYPE((*arglist)[0]),
+			     fold_build_pointer_plus ((*arglist)[2], (*arglist)[1])),
+		     (*arglist)[0]);
+    case S390_OVERLOADED_BUILTIN_s390_vec_load_pair:
+      {
+	vec<constructor_elt, va_gc> *v;
+	constructor_elt elt1 = { NULL_TREE , (*arglist)[0] };
+	constructor_elt elt2 = { NULL_TREE , (*arglist)[1] };
+
+	vec_alloc (v, 2);
+	v->quick_push (elt1);
+	v->quick_push (elt2);
+	return build_constructor (return_type, v);
+      }
+    default:
+      gcc_unreachable ();
+    }
+}
+
+/* invert result */
+#define __VSTRING_FLAG_IN         8
+/* result type */
+#define __VSTRING_FLAG_RT         4
+/* zero search */
+#define __VSTRING_FLAG_ZS         2
+/* set condition code */
+#define __VSTRING_FLAG_CS         1
+
+/* Return the flags value to be used for string low-level builtins
+   when expanded from overloaded builtin OB_FCODE.  */
+static unsigned int
+s390_get_vstring_flags (int ob_fcode)
+{
+  unsigned int flags = 0;
+
+  switch (ob_fcode)
+    {
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_or_0_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_or_0_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_cc:
+      flags |= __VSTRING_FLAG_IN;
+      break;
+    default:
+      break;
+    }
+  switch (ob_fcode)
+    {
+
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmprg:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_cc:
+      flags |= __VSTRING_FLAG_RT;
+      break;
+    default:
+      break;
+    }
+  switch (ob_fcode)
+    {
+
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_or_0_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_or_0_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_or_0_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_or_0_idx:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_or_0_idx_cc:
+      flags |= __VSTRING_FLAG_ZS;
+      break;
+    default:
+      break;
+    }
+  switch (ob_fcode)
+    {
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_or_0_idx_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_cc:
+    case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_cc:
+      flags |= __VSTRING_FLAG_CS;
+      break;
+    default:
+      break;
+    }
+  return flags;
+}
+#undef __VSTRING_FLAG_IN
+#undef __VSTRING_FLAG_RT
+#undef __VSTRING_FLAG_ZS
+#undef __VSTRING_FLAG_CS
+
+/* For several overloaded builtins the argument lists do not match
+   exactly the signature of a low-level builtin.  This function
+   adjusts the argument list ARGLIST for the overloaded builtin
+   OB_FCODE to the signature of the low-level builtin given by
+   DECL.  */
+static void
+s390_adjust_builtin_arglist (unsigned int ob_fcode, tree decl,
+			     vec<tree, va_gc> **arglist)
+{
+  tree arg_chain;
+  int src_arg_index, dest_arg_index;
+  vec<tree, va_gc> *folded_args = NULL;
+
+  /* We at most add one more operand to the list.  */
+  vec_alloc (folded_args, (*arglist)->allocated () + 1);
+  for (arg_chain = TYPE_ARG_TYPES (TREE_TYPE (decl)),
+	 src_arg_index = 0, dest_arg_index = 0;
+       !VOID_TYPE_P (TREE_VALUE (arg_chain));
+       arg_chain = TREE_CHAIN (arg_chain), dest_arg_index++)
+    {
+      bool arg_assigned_p = false;
+      switch (ob_fcode)
+	{
+	  /* For all these the low level builtin needs an additional flags parameter.  */
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_or_0_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_or_0_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_or_0_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_or_0_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_eq_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_find_any_ne_cc:
+	  if (dest_arg_index == 2)
+	    {
+	      folded_args->quick_push (build_int_cst (integer_type_node,
+				       s390_get_vstring_flags (ob_fcode)));
+	      arg_assigned_p = true;
+	    }
+	  break;
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_or_0_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_or_0_idx:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmprg:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_or_0_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_or_0_idx_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmprg_cc:
+	case S390_OVERLOADED_BUILTIN_s390_vec_cmpnrg_cc:
+	  if (dest_arg_index == 3)
+	    {
+	      folded_args->quick_push (build_int_cst (integer_type_node,
+				       s390_get_vstring_flags (ob_fcode)));
+	      arg_assigned_p = true;
+	    }
+	  break;
+	case S390_OVERLOADED_BUILTIN_s390_vec_sel:
+	case S390_OVERLOADED_BUILTIN_s390_vec_insert:
+	case S390_OVERLOADED_BUILTIN_s390_vec_load_len:
+	  /* Swap the first to arguments. It is better to do it here
+	     instead of the header file to avoid operand checking
+	     throwing error messages for a weird operand index.  */
+	  if (dest_arg_index < 2)
+	    {
+	      folded_args->quick_push (fully_fold_convert (TREE_VALUE (arg_chain),
+					 (**arglist)[1 - dest_arg_index]));
+	      src_arg_index++;
+	      arg_assigned_p = true;
+	    }
+	  break;
+	case S390_OVERLOADED_BUILTIN_s390_vec_store_len:
+	  if (dest_arg_index == 1 || dest_arg_index == 2)
+	    {
+	      folded_args->quick_push (fully_fold_convert (TREE_VALUE (arg_chain),
+					 (**arglist)[3 - dest_arg_index]));
+	      src_arg_index++;
+	      arg_assigned_p = true;
+	    }
+	  break;
+
+	case S390_OVERLOADED_BUILTIN_s390_vec_load_bndry:
+	  {
+	    int code;
+
+	    if (dest_arg_index == 1)
+	      {
+		switch (tree_low_cst ((**arglist)[src_arg_index], 1))
+		  {
+		  case 64: code = 0; break;
+		  case 128: code = 1; break;
+		  case 256: code = 2; break;
+		  case 512: code = 3; break;
+		  case 1024: code = 4; break;
+		  case 2048: code = 5; break;
+		  case 4096: code = 6; break;
+		  default:
+		    error ("valid values for builtin %qF argument %d are 64, "
+			   "128, 256, 512, 1024, 2048, and 4096", decl,
+			   src_arg_index + 1);
+		    return;
+		  }
+		folded_args->quick_push (build_int_cst (integer_type_node,
+							code));
+		src_arg_index++;
+		arg_assigned_p = true;
+	      }
+	  }
+	  break;
+	case S390_OVERLOADED_BUILTIN_s390_vec_rl_mask:
+	  /* Duplicate the first src arg.  */
+	  if (dest_arg_index == 0)
+	    {
+	      folded_args->quick_push (fully_fold_convert (TREE_VALUE (arg_chain),
+					   (**arglist)[src_arg_index]));
+	      arg_assigned_p = true;
+	    }
+	  break;
+	default:
+	  break;
+	}
+      if (!arg_assigned_p)
+	{
+	  folded_args->quick_push (fully_fold_convert (TREE_VALUE (arg_chain),
+						 (**arglist)[src_arg_index]));
+	  src_arg_index++;
+	}
+    }
+  *arglist = folded_args;
+}
+
+/* Check whether the arguments in ARGLIST match the function type
+   DEF_TYPE. Return the number of argument types which required
+   conversion/promotion in order to make it match.
+   0 stands for a perfect match - all operand types match without changes
+   INT_MAX stands for a mismatch.  */
+static int
+s390_fn_types_compatible (enum s390_builtin_ov_type_index typeindex,
+			  vec<tree, va_gc> *arglist)
+{
+  unsigned int i;
+  int match_type = 0;
+
+  for (i = 0; i < vec_safe_length (arglist); i++)
+    {
+      tree b_arg_type = s390_builtin_types[s390_builtin_ov_types[typeindex][i + 1]];
+      tree in_arg = (*arglist)[i];
+      tree in_type = TREE_TYPE (in_arg);
+
+      if (TREE_CODE (b_arg_type) == VECTOR_TYPE)
+	{
+	  /* Vector types have to match precisely.  */
+	  if (b_arg_type != in_type
+	      && TYPE_MAIN_VARIANT (b_arg_type) != TYPE_MAIN_VARIANT (in_type))
+	    goto mismatch;
+	}
+
+      if (lang_hooks.types_compatible_p (in_type, b_arg_type))
+	continue;
+
+      if (lang_hooks.types_compatible_p (
+	    lang_hooks.types.type_promotes_to (in_type),
+	    lang_hooks.types.type_promotes_to (b_arg_type)))
+	{
+	  match_type++;
+	  continue;
+	}
+
+      /* In this stage the C++ frontend would go ahead trying to find
+	 implicit conversion chains for the argument to match the
+	 target type.  We will mimic this here only for our limited
+	 subset of argument types.  */
+      if (TREE_CODE (b_arg_type) == INTEGER_TYPE
+	  && TREE_CODE (in_type) == INTEGER_TYPE)
+	{
+	  match_type++;
+	  continue;
+	}
+
+      /* If the incoming pointer argument has more qualifiers than the
+	 argument type it can still be an imperfect match.  */
+      if (POINTER_TYPE_P (b_arg_type) && POINTER_TYPE_P (in_type)
+	  && !(TYPE_QUALS (TREE_TYPE (in_type))
+	       & ~TYPE_QUALS (TREE_TYPE (b_arg_type)))
+	  && (TYPE_QUALS (TREE_TYPE (b_arg_type))
+	      & ~TYPE_QUALS (TREE_TYPE (in_type))))
+	{
+	  tree qual_in_type =
+	    build_qualified_type (TREE_TYPE (in_type),
+				  TYPE_QUALS (TREE_TYPE (b_arg_type)));
+
+	  if (lang_hooks.types_compatible_p (qual_in_type,
+					     TREE_TYPE (b_arg_type)))
+	    {
+	      match_type++;
+	      continue;
+	    }
+	}
+
+    mismatch:
+      if (TARGET_DEBUG_ARG)
+	fprintf (stderr, " mismatch in operand: %d\n", i + 1);
+      return INT_MAX;
+    }
+
+  return match_type;
+}
+
+/* Return the number of elements in the vector arguments of FNDECL in
+   case all it matches for all vector arguments, -1 otherwise.  */
+static int
+s390_vec_n_elem (tree fndecl)
+{
+  tree b_arg_chain;
+  int n_elem = -1;
+
+  if (TREE_CODE (TREE_TYPE (TREE_TYPE (fndecl))) == VECTOR_TYPE)
+    n_elem = TYPE_VECTOR_SUBPARTS (TREE_TYPE (TREE_TYPE ((fndecl))));
+
+  for (b_arg_chain = TYPE_ARG_TYPES (TREE_TYPE (fndecl));
+       !VOID_TYPE_P (TREE_VALUE (b_arg_chain));
+       b_arg_chain = TREE_CHAIN (b_arg_chain))
+    {
+      int tmp_n_elem;
+      if (TREE_CODE (TREE_VALUE (b_arg_chain)) != VECTOR_TYPE)
+	continue;
+      tmp_n_elem = TYPE_VECTOR_SUBPARTS (TREE_VALUE (b_arg_chain));
+      if (n_elem != -1 && n_elem != tmp_n_elem)
+	return -1;
+      n_elem = tmp_n_elem;
+    }
+  return n_elem;
+}
+
+
+/* Return a tree expression for a call to the overloaded builtin
+   function OB_FNDECL at LOC with arguments PASSED_ARGLIST.  */
+tree
+s390_resolve_overloaded_builtin (location_t loc,
+				 tree ob_fndecl,
+				 void *passed_arglist)
+{
+  vec<tree, va_gc> *arglist = static_cast<vec<tree, va_gc> *> (passed_arglist);
+  unsigned int in_args_num = vec_safe_length (arglist);
+  unsigned int ob_args_num = 0;
+  unsigned int ob_fcode = DECL_FUNCTION_CODE (ob_fndecl);
+  enum s390_overloaded_builtin_vars bindex;
+  unsigned int i;
+  int last_match_type = INT_MAX;
+  int last_match_index = -1;
+  unsigned int all_op_flags;
+  int num_matches = 0;
+  tree target_builtin_decl, b_arg_chain, return_type;
+  enum s390_builtin_ov_type_index last_match_fntype_index;
+
+  if (TARGET_DEBUG_ARG)
+    fprintf (stderr,
+      "s390_resolve_overloaded_builtin, code = %4d, %s - %s overloaded\n",
+      (int)ob_fcode, IDENTIFIER_POINTER (DECL_NAME (ob_fndecl)),
+     ob_fcode < S390_BUILTIN_MAX ? "not" : "");
+
+  /* 0...S390_BUILTIN_MAX-1 is for non-overloaded builtins.  */
+  if (ob_fcode < S390_BUILTIN_MAX)
+    {
+      if (bflags_for_builtin(ob_fcode) & B_INT)
+	{
+	  error_at (loc,
+		    "Builtin %qF is for GCC internal use only.",
+		    ob_fndecl);
+	  return error_mark_node;
+	}
+      return NULL_TREE;
+    }
+
+  ob_fcode -= S390_BUILTIN_MAX;
+
+  for (b_arg_chain = TYPE_ARG_TYPES (TREE_TYPE (ob_fndecl));
+       !VOID_TYPE_P (TREE_VALUE (b_arg_chain));
+       b_arg_chain = TREE_CHAIN (b_arg_chain))
+    ob_args_num++;
+
+  if (ob_args_num != in_args_num)
+    {
+      error_at (loc,
+		"Mismatch in number of arguments for builtin %qF. "
+		"Expected: %d got %d", ob_fndecl,
+		ob_args_num, in_args_num);
+      return error_mark_node;
+    }
+
+  for (i = 0; i < in_args_num; i++)
+    if ((*arglist)[i] == error_mark_node)
+      return error_mark_node;
+
+  /* Overloaded builtins without any variants are directly expanded here.  */
+  if (desc_start_for_overloaded_builtin[ob_fcode] ==
+      S390_OVERLOADED_BUILTIN_VAR_MAX)
+    return s390_expand_overloaded_builtin (loc, ob_fcode, arglist, NULL_TREE);
+
+  for (bindex = desc_start_for_overloaded_builtin[ob_fcode];
+       bindex <= desc_end_for_overloaded_builtin[ob_fcode];
+       bindex = (enum s390_overloaded_builtin_vars)((int)bindex + 1))
+  {
+    int match_type;
+    enum s390_builtin_ov_type_index type_index =
+      type_for_overloaded_builtin_var[bindex];
+
+    if (TARGET_DEBUG_ARG)
+      fprintf (stderr, "checking variant number: %d", (int)bindex);
+
+    match_type = s390_fn_types_compatible (type_index, arglist);
+
+    if (match_type == INT_MAX)
+      continue;
+
+    if (TARGET_DEBUG_ARG)
+      fprintf (stderr,
+	       " %s match score: %d\n", match_type == 0 ? "perfect" : "imperfect",
+	       match_type);
+
+    if (match_type < last_match_type)
+      {
+	num_matches = 1;
+	last_match_type = match_type;
+	last_match_fntype_index = type_index;
+	last_match_index = bindex;
+      }
+    else if (match_type == last_match_type)
+      num_matches++;
+  }
+
+  if (last_match_type == INT_MAX)
+    {
+      error_at (loc, "invalid parameter combination for intrinsic %qs",
+		IDENTIFIER_POINTER (DECL_NAME (ob_fndecl)));
+      return error_mark_node;
+    }
+  else if (num_matches > 1)
+    {
+      error_at (loc, "ambiguous overload for intrinsic %qs",
+		IDENTIFIER_POINTER (DECL_NAME (ob_fndecl)));
+      return error_mark_node;
+    }
+
+  if (bt_for_overloaded_builtin_var[last_match_index] == S390_BUILTIN_MAX)
+    target_builtin_decl = ob_fndecl;
+  else
+    target_builtin_decl = s390_builtin_decls[bt_for_overloaded_builtin_var[last_match_index]];
+
+  all_op_flags = opflags_overloaded_builtin_var[last_match_index];
+  return_type = s390_builtin_types[s390_builtin_ov_types[last_match_fntype_index][0]];
+
+  /* Check for the operand flags in the overloaded builtin variant.  */
+  for (i = 0; i < ob_args_num; i++)
+    {
+      unsigned int op_flags = all_op_flags & ((1 << O_SHIFT) - 1);
+      tree arg = (*arglist)[i];
+      tree type = s390_builtin_types[s390_builtin_ov_types[last_match_fntype_index][i + 1]];
+
+      all_op_flags = all_op_flags >> O_SHIFT;
+
+      if (op_flags == O_ELEM)
+	{
+	  int n_elem = s390_vec_n_elem (target_builtin_decl);
+	  gcc_assert (n_elem > 0);
+	  gcc_assert (type == integer_type_node);
+	  (*arglist)[i] = build2 (BIT_AND_EXPR, integer_type_node,
+				  fold_convert (integer_type_node, arg),
+				  build_int_cst (NULL_TREE, n_elem - 1));
+	}
+
+      if (TREE_CODE (arg) != INTEGER_CST || !O_IMM_P (op_flags))
+	continue;
+
+      if ((TYPE_UNSIGNED (type)
+	   && !int_fits_type_p (arg, c_common_unsigned_type (type)))
+	  || (!TYPE_UNSIGNED (type)
+	      && !int_fits_type_p (arg, c_common_signed_type (type))))
+	{
+	  error("constant argument %d for builtin %qF is out "
+		"of range for target type",
+		i + 1, target_builtin_decl);
+	  return error_mark_node;
+	}
+
+      if (TREE_CODE (arg) == INTEGER_CST
+	  && !s390_const_operand_ok (arg, i + 1, op_flags, target_builtin_decl))
+	return error_mark_node;
+    }
+
+  /* Handle builtins we expand directly - without mapping it to a low
+     level builtin.  */
+  if (bt_for_overloaded_builtin_var[last_match_index] == S390_BUILTIN_MAX)
+    return s390_expand_overloaded_builtin (loc, ob_fcode, arglist, return_type);
+
+  s390_adjust_builtin_arglist (ob_fcode, target_builtin_decl, &arglist);
+
+  if (VOID_TYPE_P (return_type))
+    return build_function_call_vec (loc, target_builtin_decl,
+				    arglist, NULL);
+  else
+    return fully_fold_convert (return_type,
+			       build_function_call_vec (loc, target_builtin_decl,
+							arglist, NULL));
+}
+
+/* This is used to define the REGISTER_TARGET_PRAGMAS macro in s390.h.  */
+void
+s390_register_target_pragmas (void)
+{
+  targetm.resolve_overloaded_builtin = s390_resolve_overloaded_builtin;
+}
--- gcc/config/s390/s390.h	2016-05-11 14:46:08.219982746 +0200
+++ gcc/config/s390/s390.h	2016-05-11 17:12:39.000000000 +0200
@@ -35,7 +35,9 @@ enum processor_flags
   PF_Z10 = 32,
   PF_Z196 = 64,
   PF_ZEC12 = 128,
-  PF_TX = 256
+  PF_TX = 256,
+  PF_Z13 = 512,
+  PF_VX = 1024
 };
 
 /* This is necessary to avoid a warning about comparing different enum
@@ -64,6 +66,10 @@ enum processor_flags
  	(s390_arch_flags & PF_ZEC12)
 #define TARGET_CPU_HTM \
  	(s390_arch_flags & PF_TX)
+#define TARGET_CPU_Z13 \
+        (s390_arch_flags & PF_Z13)
+#define TARGET_CPU_VX \
+        (s390_arch_flags & PF_VX)
 
 /* These flags indicate that the generated code should run on a cpu
    providing the respective hardware facility when run in
@@ -82,7 +88,15 @@ enum processor_flags
 #define TARGET_ZEC12 \
        (TARGET_ZARCH && TARGET_CPU_ZEC12)
 #define TARGET_HTM (TARGET_OPT_HTM)
-
+#define TARGET_Z13 \
+       (TARGET_ZARCH && TARGET_CPU_Z13)
+#define TARGET_VX \
+       (TARGET_ZARCH && TARGET_CPU_VX && TARGET_OPT_VX && TARGET_HARD_FLOAT)
+
+/* Use the ABI introduced with IBM z13:
+   - pass vector arguments <= 16 bytes in VRs
+   - align *all* vector types to 8 bytes  */
+#define TARGET_VX_ABI TARGET_VX
 
 #define TARGET_AVOID_CMP_AND_BRANCH (s390_tune == PROCESSOR_2817_Z196)
 
@@ -97,25 +111,11 @@ enum processor_flags
 #define TARGET_TPF 0
 
 /* Target CPU builtins.  */
-#define TARGET_CPU_CPP_BUILTINS()					\
-  do									\
-    {									\
-      builtin_assert ("cpu=s390");					\
-      builtin_assert ("machine=s390");					\
-      builtin_define ("__s390__");					\
-      if (TARGET_ZARCH)							\
-	builtin_define ("__zarch__");					\
-      if (TARGET_64BIT)							\
-        builtin_define ("__s390x__");					\
-      if (TARGET_LONG_DOUBLE_128)					\
-        builtin_define ("__LONG_DOUBLE_128__");				\
-      if (TARGET_HTM)							\
-	builtin_define ("__HTM__");					\
-    }									\
-  while (0)
+#define TARGET_CPU_CPP_BUILTINS() s390_cpu_cpp_builtins (pfile)
 
 #ifdef DEFAULT_TARGET_64BIT
-#define TARGET_DEFAULT             (MASK_64BIT | MASK_ZARCH | MASK_HARD_DFP | MASK_OPT_HTM)
+#define TARGET_DEFAULT     (MASK_64BIT | MASK_ZARCH | MASK_HARD_DFP	\
+                            | MASK_OPT_HTM | MASK_OPT_VX)
 #else
 #define TARGET_DEFAULT             0
 #endif
@@ -184,6 +184,13 @@ enum processor_flags
 
 #define STACK_SIZE_MODE (Pmode)
 
+/* Vector arguments are left-justified when placed on the stack during
+   parameter passing.  */
+#define FUNCTION_ARG_PADDING(MODE, TYPE)			\
+  (s390_function_arg_vector ((MODE), (TYPE))			\
+   ? upward							\
+   : DEFAULT_FUNCTION_ARG_PADDING ((MODE), (TYPE)))
+
 #ifndef IN_LIBGCC2
 
 /* Width of a word, in units (bytes).  */
@@ -289,9 +296,11 @@ enum processor_flags
    Reg 35: Return address pointer
 
    Registers 36 and 37 are mapped to access registers
-   0 and 1, used to implement thread-local storage.  */
+   0 and 1, used to implement thread-local storage.
+
+   Reg 38-53: Vector registers v16-v31  */
 
-#define FIRST_PSEUDO_REGISTER 38
+#define FIRST_PSEUDO_REGISTER 54
 
 /* Standard register usage.  */
 #define GENERAL_REGNO_P(N)	((int)(N) >= 0 && (N) < 16)
@@ -300,6 +309,8 @@ enum processor_flags
 #define CC_REGNO_P(N)		((N) == 33)
 #define FRAME_REGNO_P(N)	((N) == 32 || (N) == 34 || (N) == 35)
 #define ACCESS_REGNO_P(N)	((N) == 36 || (N) == 37)
+#define VECTOR_NOFP_REGNO_P(N)  ((N) >= 38 && (N) <= 53)
+#define VECTOR_REGNO_P(N)       (FP_REGNO_P (N) || VECTOR_NOFP_REGNO_P (N))
 
 #define GENERAL_REG_P(X)	(REG_P (X) && GENERAL_REGNO_P (REGNO (X)))
 #define ADDR_REG_P(X)		(REG_P (X) && ADDR_REGNO_P (REGNO (X)))
@@ -307,6 +318,8 @@ enum processor_flags
 #define CC_REG_P(X)		(REG_P (X) && CC_REGNO_P (REGNO (X)))
 #define FRAME_REG_P(X)		(REG_P (X) && FRAME_REGNO_P (REGNO (X)))
 #define ACCESS_REG_P(X)		(REG_P (X) && ACCESS_REGNO_P (REGNO (X)))
+#define VECTOR_NOFP_REG_P(X)    (REG_P (X) && VECTOR_NOFP_REGNO_P (REGNO (X)))
+#define VECTOR_REG_P(X)         (REG_P (X) && VECTOR_REGNO_P (REGNO (X)))
 
 /* Set up fixed registers and calling convention:
 
@@ -321,7 +334,9 @@ enum processor_flags
 
    On 31-bit, FPRs 18-19 are call-clobbered;
    on 64-bit, FPRs 24-31 are call-clobbered.
-   The remaining FPRs are call-saved.  */
+   The remaining FPRs are call-saved.
+
+   All non-FP vector registers are call-clobbered v16-v31.  */
 
 #define FIXED_REGISTERS				\
 { 0, 0, 0, 0, 					\
@@ -333,7 +348,11 @@ enum processor_flags
   0, 0, 0, 0, 					\
   0, 0, 0, 0, 					\
   1, 1, 1, 1,					\
-  1, 1 }
+  1, 1,						\
+  0, 0, 0, 0, 					\
+  0, 0, 0, 0, 					\
+  0, 0, 0, 0, 					\
+  0, 0, 0, 0 }
 
 #define CALL_USED_REGISTERS			\
 { 1, 1, 1, 1, 					\
@@ -345,26 +364,35 @@ enum processor_flags
   1, 1, 1, 1, 					\
   1, 1, 1, 1, 					\
   1, 1, 1, 1,					\
-  1, 1 }
+  1, 1,					        \
+  1, 1, 1, 1, 					\
+  1, 1, 1, 1,					\
+  1, 1, 1, 1, 					\
+  1, 1, 1, 1 }
 
 #define CALL_REALLY_USED_REGISTERS		\
-{ 1, 1, 1, 1, 					\
+{ 1, 1, 1, 1, 	/* r0 - r15 */			\
   1, 1, 0, 0, 					\
   0, 0, 0, 0, 					\
   0, 0, 0, 0,					\
+  1, 1, 1, 1, 	/* f0 (16) - f15 (31) */	\
   1, 1, 1, 1, 					\
   1, 1, 1, 1, 					\
   1, 1, 1, 1, 					\
-  1, 1, 1, 1, 					\
+  1, 1, 1, 1,	/* arg, cc, fp, ret addr */	\
+  0, 0,		/* a0 (36), a1 (37) */	        \
+  1, 1, 1, 1, 	/* v16 (38) - v23 (45) */	\
   1, 1, 1, 1,					\
-  0, 0 }
+  1, 1, 1, 1, 	/* v24 (46) - v31 (53) */	\
+  1, 1, 1, 1 }
 
 /* Preferred register allocation order.  */
-#define REG_ALLOC_ORDER                                         \
-{  1, 2, 3, 4, 5, 0, 12, 11, 10, 9, 8, 7, 6, 14, 13,            \
-   16, 17, 18, 19, 20, 21, 22, 23,                              \
-   24, 25, 26, 27, 28, 29, 30, 31,                              \
-   15, 32, 33, 34, 35, 36, 37 }
+#define REG_ALLOC_ORDER							\
+  {  1, 2, 3, 4, 5, 0, 12, 11, 10, 9, 8, 7, 6, 14, 13,			\
+     16, 17, 18, 19, 20, 21, 22, 23,					\
+     24, 25, 26, 27, 28, 29, 30, 31,					\
+     38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 	\
+     15, 32, 33, 34, 35, 36, 37 }
 
 
 /* Fitting values into registers.  */
@@ -404,26 +432,22 @@ enum processor_flags
    but conforms to the 31-bit ABI, GPRs can hold 8 bytes;
    the ABI guarantees only that the lower 4 bytes are
    saved across calls, however.  */
-#define HARD_REGNO_CALL_PART_CLOBBERED(REGNO, MODE)		\
-  (!TARGET_64BIT && TARGET_ZARCH				\
-   && GET_MODE_SIZE (MODE) > 4					\
-   && (((REGNO) >= 6 && (REGNO) <= 15) || (REGNO) == 32))
+#define HARD_REGNO_CALL_PART_CLOBBERED(REGNO, MODE)			\
+  ((!TARGET_64BIT && TARGET_ZARCH					\
+    && GET_MODE_SIZE (MODE) > 4						\
+    && (((REGNO) >= 6 && (REGNO) <= 15) || (REGNO) == 32))		\
+   || (TARGET_VX							\
+       && GET_MODE_SIZE (MODE) > 8					\
+       && (((TARGET_64BIT && (REGNO) >= 24 && (REGNO) <= 31))		\
+	   || (!TARGET_64BIT && ((REGNO) == 18 || (REGNO) == 19)))))
 
 /* Maximum number of registers to represent a value of mode MODE
    in a register of class CLASS.  */
 #define CLASS_MAX_NREGS(CLASS, MODE)   					\
   s390_class_max_nregs ((CLASS), (MODE))
 
-/* If a 4-byte value is loaded into a FPR, it is placed into the
-   *upper* half of the register, not the lower.  Therefore, we
-   cannot use SUBREGs to switch between modes in FP registers.
-   Likewise for access registers, since they have only half the
-   word size on 64-bit.  */
 #define CANNOT_CHANGE_MODE_CLASS(FROM, TO, CLASS)		        \
-  (GET_MODE_SIZE (FROM) != GET_MODE_SIZE (TO)			        \
-   ? ((reg_classes_intersect_p (FP_REGS, CLASS)				\
-       && (GET_MODE_SIZE (FROM) < 8 || GET_MODE_SIZE (TO) < 8))		\
-      || reg_classes_intersect_p (ACCESS_REGS, CLASS)) : 0)
+  s390_cannot_change_mode_class ((FROM), (TO), (CLASS))
 
 /* Register classes.  */
 
@@ -451,6 +475,7 @@ enum reg_class
   NO_REGS, CC_REGS, ADDR_REGS, GENERAL_REGS, ACCESS_REGS,
   ADDR_CC_REGS, GENERAL_CC_REGS,
   FP_REGS, ADDR_FP_REGS, GENERAL_FP_REGS,
+  VEC_REGS, ADDR_VEC_REGS, GENERAL_VEC_REGS,
   ALL_REGS, LIM_REG_CLASSES
 };
 #define N_REG_CLASSES (int) LIM_REG_CLASSES
@@ -458,11 +483,13 @@ enum reg_class
 #define REG_CLASS_NAMES							\
 { "NO_REGS", "CC_REGS", "ADDR_REGS", "GENERAL_REGS", "ACCESS_REGS",	\
   "ADDR_CC_REGS", "GENERAL_CC_REGS",					\
-  "FP_REGS", "ADDR_FP_REGS", "GENERAL_FP_REGS", "ALL_REGS" }
+  "FP_REGS", "ADDR_FP_REGS", "GENERAL_FP_REGS",				\
+  "VEC_REGS", "ADDR_VEC_REGS", "GENERAL_VEC_REGS",			\
+  "ALL_REGS" }
 
 /* Class -> register mapping.  */
-#define REG_CLASS_CONTENTS \
-{				       			\
+#define REG_CLASS_CONTENTS				\
+{							\
   { 0x00000000, 0x00000000 },	/* NO_REGS */		\
   { 0x00000000, 0x00000002 },	/* CC_REGS */		\
   { 0x0000fffe, 0x0000000d },	/* ADDR_REGS */		\
@@ -473,7 +500,10 @@ enum reg_class
   { 0xffff0000, 0x00000000 },	/* FP_REGS */		\
   { 0xfffffffe, 0x0000000d },	/* ADDR_FP_REGS */	\
   { 0xffffffff, 0x0000000d },	/* GENERAL_FP_REGS */	\
-  { 0xffffffff, 0x0000003f },	/* ALL_REGS */		\
+  { 0xffff0000, 0x003fffc0 },	/* VEC_REGS */		\
+  { 0xfffffffe, 0x003fffcd },	/* ADDR_VEC_REGS */	\
+  { 0xffffffff, 0x003fffcd },	/* GENERAL_VEC_REGS */	\
+  { 0xffffffff, 0x003fffff },	/* ALL_REGS */		\
 }
 
 /* In some case register allocation order is not enough for IRA to
@@ -504,14 +534,27 @@ extern const enum reg_class regclass_map
 #define REGNO_OK_FOR_BASE_P(REGNO) REGNO_OK_FOR_INDEX_P (REGNO)
 
 
-/* We need secondary memory to move data between GPRs and FPRs.  With
-   DFP the ldgr lgdr instructions are available.  But these
-   instructions do not handle GPR pairs so it is not possible for 31
-   bit.  */
-#define SECONDARY_MEMORY_NEEDED(CLASS1, CLASS2, MODE) \
- ((CLASS1) != (CLASS2)                                \
-  && ((CLASS1) == FP_REGS || (CLASS2) == FP_REGS)     \
-  && (!TARGET_DFP || !TARGET_64BIT || GET_MODE_SIZE (MODE) != 8))
+/* We need secondary memory to move data between GPRs and FPRs.
+
+   - With DFP the ldgr lgdr instructions are available.  Due to the
+     different alignment we cannot use them for SFmode.  For 31 bit a
+     64 bit value in GPR would be a register pair so here we still
+     need to go via memory.
+
+   - With z13 we can do the SF/SImode moves with vlgvf.  Due to the
+     overlapping of FPRs and VRs we still disallow TF/TD modes to be
+     in full VRs so as before also on z13 we do these moves via
+     memory.
+
+     FIXME: Should we try splitting it into two vlgvg's/vlvg's instead?  */
+#define SECONDARY_MEMORY_NEEDED(CLASS1, CLASS2, MODE)			\
+  (((reg_classes_intersect_p (CLASS1, VEC_REGS)				\
+     && reg_classes_intersect_p (CLASS2, GENERAL_REGS))			\
+    || (reg_classes_intersect_p (CLASS1, GENERAL_REGS)			\
+	&& reg_classes_intersect_p (CLASS2, VEC_REGS)))			\
+   && (!TARGET_DFP || !TARGET_64BIT || GET_MODE_SIZE (MODE) != 8)	\
+   && (!TARGET_VX || (SCALAR_FLOAT_MODE_P (MODE)			\
+			  && GET_MODE_SIZE (MODE) > 8)))
 
 /* Get_secondary_mem widens its argument to BITS_PER_WORD which loses on 64bit
    because the movsi and movsf patterns don't handle r/f moves.  */
@@ -605,6 +648,11 @@ extern const enum reg_class regclass_map
 /* Let the assembler generate debug line info.  */
 #define DWARF2_ASM_LINE_DEBUG_INFO 1
 
+/* Define the dwarf register mapping.
+   v16-v31 -> 68-83
+   rX      -> X      otherwise  */
+#define DBX_REGISTER_NUMBER(regno)			\
+  ((regno >= 38 && regno <= 53) ? regno + 30 : regno)
 
 /* Frame registers.  */
 
@@ -652,21 +700,29 @@ typedef struct s390_arg_structure
 {
   int gprs;			/* gpr so far */
   int fprs;			/* fpr so far */
+  int vrs;                      /* vr so far */
 }
 CUMULATIVE_ARGS;
 
 #define INIT_CUMULATIVE_ARGS(CUM, FNTYPE, LIBNAME, NN, N_NAMED_ARGS) \
-  ((CUM).gprs=0, (CUM).fprs=0)
+  ((CUM).gprs=0, (CUM).fprs=0, (CUM).vrs=0)
+
+#define FIRST_VEC_ARG_REGNO 46
+#define LAST_VEC_ARG_REGNO 53
 
 /* Arguments can be placed in general registers 2 to 6, or in floating
    point registers 0 and 2 for 31 bit and fprs 0, 2, 4 and 6 for 64
    bit.  */
-#define FUNCTION_ARG_REGNO_P(N) (((N) >=2 && (N) <7) || \
-  (N) == 16 || (N) == 17 || (TARGET_64BIT && ((N) == 18 || (N) == 19)))
+#define FUNCTION_ARG_REGNO_P(N)						\
+  (((N) >=2 && (N) < 7) || (N) == 16 || (N) == 17			\
+   || (TARGET_64BIT && ((N) == 18 || (N) == 19))			\
+   || (TARGET_VX && ((N) >= FIRST_VEC_ARG_REGNO && (N) <= LAST_VEC_ARG_REGNO)))
 
 
-/* Only gpr 2 and fpr 0 are ever used as return registers.  */
-#define FUNCTION_VALUE_REGNO_P(N) ((N) == 2 || (N) == 16)
+/* Only gpr 2, fpr 0, and v24 are ever used as return registers.  */
+#define FUNCTION_VALUE_REGNO_P(N)		\
+  ((N) == 2 || (N) == 16			\
+   || (TARGET_VX && (N) == FIRST_VEC_ARG_REGNO))
 
 
 /* Function entry and exit.  */
@@ -844,12 +900,20 @@ do {									\
 /* How to refer to registers in assembler output.  This sequence is
    indexed by compiler's hard-register-number (see above).  */
 #define REGISTER_NAMES							\
-{ "%r0",  "%r1",  "%r2",  "%r3",  "%r4",  "%r5",  "%r6",  "%r7",	\
-  "%r8",  "%r9",  "%r10", "%r11", "%r12", "%r13", "%r14", "%r15",	\
-  "%f0",  "%f2",  "%f4",  "%f6",  "%f1",  "%f3",  "%f5",  "%f7",	\
-  "%f8",  "%f10", "%f12", "%f14", "%f9",  "%f11", "%f13", "%f15",	\
-  "%ap",  "%cc",  "%fp",  "%rp",  "%a0",  "%a1"				\
-}
+  { "%r0",  "%r1",  "%r2",  "%r3",  "%r4",  "%r5",  "%r6",  "%r7",	\
+    "%r8",  "%r9",  "%r10", "%r11", "%r12", "%r13", "%r14", "%r15",	\
+    "%f0",  "%f2",  "%f4",  "%f6",  "%f1",  "%f3",  "%f5",  "%f7",	\
+    "%f8",  "%f10", "%f12", "%f14", "%f9",  "%f11", "%f13", "%f15",	\
+    "%ap",  "%cc",  "%fp",  "%rp",  "%a0",  "%a1",			\
+    "%v16", "%v18", "%v20", "%v22", "%v17", "%v19", "%v21", "%v23",	\
+    "%v24", "%v26", "%v28", "%v30", "%v25", "%v27", "%v29", "%v31"	\
+  }
+
+#define ADDITIONAL_REGISTER_NAMES					\
+  { { "v0", 16 }, { "v2",  17 }, { "v4",  18 }, { "v6",  19 },		\
+    { "v1", 20 }, { "v3",  21 }, { "v5",  22 }, { "v7",  23 },          \
+    { "v8", 24 }, { "v10", 25 }, { "v12", 26 }, { "v14", 27 },          \
+    { "v9", 28 }, { "v11", 29 }, { "v13", 30 }, { "v15", 31 } };
 
 /* Print operand X (an rtx) in assembler syntax to file FILE.  */
 #define PRINT_OPERAND(FILE, X, CODE) print_operand (FILE, X, CODE)
@@ -915,13 +979,31 @@ do {									\
 #define SYMBOL_REF_NOT_NATURALLY_ALIGNED_P(X) \
   ((SYMBOL_REF_FLAGS (X) & SYMBOL_FLAG_NOT_NATURALLY_ALIGNED))
 
+/* Check whether integer displacement is in range for a short displacement.  */
+#define SHORT_DISP_IN_RANGE(d) ((d) >= 0 && (d) <= 4095)
+
 /* Check whether integer displacement is in range.  */
 #define DISP_IN_RANGE(d) \
   (TARGET_LONG_DISPLACEMENT? ((d) >= -524288 && (d) <= 524287) \
-                           : ((d) >= 0 && (d) <= 4095))
+                           : SHORT_DISP_IN_RANGE(d))
 
 /* Reads can reuse write prefetches, used by tree-ssa-prefetch-loops.c.  */
 #define READ_CAN_USE_WRITE_PREFETCH 1
 
 extern const int processor_flags_table[];
-#endif
+
+/* The truth element value for vector comparisons.  Our instructions
+   always generate -1 in that case.  */
+#define VECTOR_STORE_FLAG_VALUE(MODE) CONSTM1_RTX (GET_MODE_INNER (MODE))
+
+/* Target pragma.  */
+
+/* resolve_overloaded_builtin can not be defined the normal way since
+   it is defined in code which technically belongs to the
+   front-end.  */
+#define REGISTER_TARGET_PRAGMAS()		\
+  do {						\
+    s390_register_target_pragmas ();		\
+  } while (0)
+
+#endif /* S390_H */
--- gcc/config/s390/s390intrin.h	2013-08-14 13:55:12.000000000 +0200
+++ gcc/config/s390/s390intrin.h	2016-05-11 17:12:39.000000000 +0200
@@ -29,5 +29,8 @@ along with GCC; see the file COPYING3.
 #include <htmintrin.h>
 #endif
 
+#ifdef __VEC__
+#include <vecintrin.h>
+#endif
 
 #endif /* _S390INTRIN_H*/
--- gcc/config/s390/s390.md	2015-06-18 16:33:04.000000000 +0200
+++ gcc/config/s390/s390.md	2016-05-11 19:22:59.245881189 +0200
@@ -125,7 +125,109 @@
    UNSPEC_FPINT_CEIL
    UNSPEC_FPINT_NEARBYINT
    UNSPEC_FPINT_RINT
- ])
+
+   UNSPEC_LCBB
+
+   ; Vector
+   UNSPEC_VEC_SMULT_HI
+   UNSPEC_VEC_UMULT_HI
+   UNSPEC_VEC_SMULT_LO
+   UNSPEC_VEC_SMULT_EVEN
+   UNSPEC_VEC_UMULT_EVEN
+   UNSPEC_VEC_SMULT_ODD
+   UNSPEC_VEC_UMULT_ODD
+
+   UNSPEC_VEC_VMAL
+   UNSPEC_VEC_VMAH
+   UNSPEC_VEC_VMALH
+   UNSPEC_VEC_VMAE
+   UNSPEC_VEC_VMALE
+   UNSPEC_VEC_VMAO
+   UNSPEC_VEC_VMALO
+
+   UNSPEC_VEC_GATHER
+   UNSPEC_VEC_EXTRACT
+   UNSPEC_VEC_INSERT_AND_ZERO
+   UNSPEC_VEC_LOAD_BNDRY
+   UNSPEC_VEC_LOAD_LEN
+   UNSPEC_VEC_MERGEH
+   UNSPEC_VEC_MERGEL
+   UNSPEC_VEC_PACK
+   UNSPEC_VEC_PACK_SATURATE
+   UNSPEC_VEC_PACK_SATURATE_CC
+   UNSPEC_VEC_PACK_SATURATE_GENCC
+   UNSPEC_VEC_PACK_UNSIGNED_SATURATE
+   UNSPEC_VEC_PACK_UNSIGNED_SATURATE_CC
+   UNSPEC_VEC_PACK_UNSIGNED_SATURATE_GENCC
+   UNSPEC_VEC_PERM
+   UNSPEC_VEC_PERMI
+   UNSPEC_VEC_EXTEND
+   UNSPEC_VEC_STORE_LEN
+   UNSPEC_VEC_UNPACKH
+   UNSPEC_VEC_UNPACKH_L
+   UNSPEC_VEC_UNPACKL
+   UNSPEC_VEC_UNPACKL_L
+   UNSPEC_VEC_ADDC
+   UNSPEC_VEC_ADDC_U128
+   UNSPEC_VEC_ADDE_U128
+   UNSPEC_VEC_ADDEC_U128
+   UNSPEC_VEC_AVG
+   UNSPEC_VEC_AVGU
+   UNSPEC_VEC_CHECKSUM
+   UNSPEC_VEC_GFMSUM
+   UNSPEC_VEC_GFMSUM_128
+   UNSPEC_VEC_GFMSUM_ACCUM
+   UNSPEC_VEC_GFMSUM_ACCUM_128
+   UNSPEC_VEC_SET
+
+   UNSPEC_VEC_VSUMG
+   UNSPEC_VEC_VSUMQ
+   UNSPEC_VEC_VSUM
+   UNSPEC_VEC_RL_MASK
+   UNSPEC_VEC_SLL
+   UNSPEC_VEC_SLB
+   UNSPEC_VEC_SLDB
+   UNSPEC_VEC_SRAL
+   UNSPEC_VEC_SRAB
+   UNSPEC_VEC_SRL
+   UNSPEC_VEC_SRLB
+
+   UNSPEC_VEC_SUB_U128
+   UNSPEC_VEC_SUBC
+   UNSPEC_VEC_SUBC_U128
+   UNSPEC_VEC_SUBE_U128
+   UNSPEC_VEC_SUBEC_U128
+
+   UNSPEC_VEC_TEST_MASK
+
+   UNSPEC_VEC_VFAE
+   UNSPEC_VEC_VFAECC
+
+   UNSPEC_VEC_VFEE
+   UNSPEC_VEC_VFEECC
+   UNSPEC_VEC_VFENE
+   UNSPEC_VEC_VFENECC
+
+   UNSPEC_VEC_VISTR
+   UNSPEC_VEC_VISTRCC
+
+   UNSPEC_VEC_VSTRC
+   UNSPEC_VEC_VSTRCCC
+
+   UNSPEC_VEC_VCDGB
+   UNSPEC_VEC_VCDLGB
+
+   UNSPEC_VEC_VCGDB
+   UNSPEC_VEC_VCLGDB
+
+   UNSPEC_VEC_VFIDB
+
+   UNSPEC_VEC_VLDEB
+   UNSPEC_VEC_VLEDB
+
+   UNSPEC_VEC_VFTCIDB
+   UNSPEC_VEC_VFTCIDBCC
+])
 
 ;;
 ;; UNSPEC_VOLATILE usage
@@ -167,6 +269,10 @@
    UNSPECV_ETND
    UNSPECV_NTSTG
    UNSPECV_PPA
+
+   ; Set and get floating point control register
+   UNSPECV_SFPC
+   UNSPECV_EFPC
   ])
 
 ;;
@@ -198,6 +304,11 @@
    ; Floating point registers.
    (FPR0_REGNUM                 16)
    (FPR2_REGNUM                 18)
+   (VR0_REGNUM                  16)
+   (VR16_REGNUM                 38)
+   (VR23_REGNUM                 45)
+   (VR24_REGNUM                 46)
+   (VR31_REGNUM                 53)
   ])
 
 ;;
@@ -228,7 +339,7 @@
 ;; Used to determine defaults for length and other attribute values.
 
 (define_attr "op_type"
-  "NN,E,RR,RRE,RX,RS,RSI,RI,SI,S,SS,SSE,RXE,RSE,RIL,RIE,RXY,RSY,SIY,RRF,RRR,SIL,RRS,RIS"
+  "NN,E,RR,RRE,RX,RS,RSI,RI,SI,S,SS,SSE,RXE,RSE,RIL,RIE,RXY,RSY,SIY,RRF,RRR,SIL,RRS,RIS,VRI,VRR,VRS,VRV,VRX"
   (const_string "NN"))
 
 ;; Instruction type attribute used for scheduling.
@@ -306,10 +417,11 @@
 ;; distinguish between g5 and g6, but there are differences between the two
 ;; CPUs could in theory be modeled.
 
-(define_attr "cpu" "g5,g6,z900,z990,z9_109,z9_ec,z10,z196,zEC12"
+(define_attr "cpu" "g5,g6,z900,z990,z9_109,z9_ec,z10,z196,zEC12,z13"
   (const (symbol_ref "s390_tune_attr")))
 
-(define_attr "cpu_facility" "standard,ieee,zarch,longdisp,extimm,dfp,z10,z196,zEC12"
+(define_attr "cpu_facility"
+  "standard,ieee,zarch,longdisp,extimm,dfp,z10,z196,zEC12,vec"
   (const_string "standard"))
 
 (define_attr "enabled" ""
@@ -346,6 +458,10 @@
 
          (and (eq_attr "cpu_facility" "zEC12")
               (match_test "TARGET_ZEC12"))
+	 (const_int 1)
+
+         (and (eq_attr "cpu_facility" "vec")
+              (match_test "TARGET_VX"))
 	 (const_int 1)]
 	(const_int 0)))
 
@@ -365,6 +481,9 @@
 ;; Pipeline description for zEC12
 (include "2827.md")
 
+;; Pipeline description for z13
+(include "2964.md")
+
 ;; Predicates
 (include "predicates.md")
 
@@ -376,12 +495,13 @@
 
 ;; Iterators
 
+(define_mode_iterator ALL [TI DI SI HI QI TF DF SF TD DD SD V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1SF V2SF V4SF V1TI V1DF V2DF V1TF])
+
 ;; These mode iterators allow floating point patterns to be generated from the
 ;; same template.
 (define_mode_iterator FP_ALL [TF DF SF (TD "TARGET_HARD_DFP") (DD "TARGET_HARD_DFP")
                               (SD "TARGET_HARD_DFP")])
 (define_mode_iterator FP [TF DF SF (TD "TARGET_HARD_DFP") (DD "TARGET_HARD_DFP")])
-(define_mode_iterator FPALL [TF DF SF TD DD SD])
 (define_mode_iterator BFP [TF DF SF])
 (define_mode_iterator DFP [TD DD])
 (define_mode_iterator DFP_ALL [TD DD SD])
@@ -417,7 +537,6 @@
 ;; This mode iterator allows the integer patterns to be defined from the
 ;; same template.
 (define_mode_iterator INT [(DI "TARGET_ZARCH") SI HI QI])
-(define_mode_iterator INTALL [TI DI SI HI QI])
 (define_mode_iterator DINT [(TI "TARGET_ZARCH") DI SI HI QI])
 
 ;; This iterator allows some 'ashift' and 'lshiftrt' pattern to be defined from
@@ -476,6 +595,14 @@
 ;; first and the second operand match for bfp modes.
 (define_mode_attr f0 [(TF "0") (DF "0") (SF "0") (TD "f") (DD "f") (DD "f")])
 
+;; This attribute is used to merge the scalar vector instructions into
+;; the FP patterns.  For non-supported modes (all but DF) it expands
+;; to constraints which are supposed to be matched by an earlier
+;; variant.
+(define_mode_attr v0      [(TF "0") (DF "v") (SF "0") (TD "0") (DD "0") (DD "0") (TI "0") (DI "v") (SI "0")])
+(define_mode_attr vf      [(TF "f") (DF "v") (SF "f") (TD "f") (DD "f") (DD "f") (TI "f") (DI "v") (SI "f")])
+(define_mode_attr vd      [(TF "d") (DF "v") (SF "d") (TD "d") (DD "d") (DD "d") (TI "d") (DI "v") (SI "d")])
+
 ;; This attribute is used in the operand list of the instruction to have an
 ;; additional operand for the dfp instructions.
 (define_mode_attr op1 [(TF "") (DF "") (SF "")
@@ -584,6 +711,19 @@
 ;; In place of GET_MODE_BITSIZE (<MODE>mode)
 (define_mode_attr bitsize [(DI "64") (SI "32") (HI "16") (QI "8")])
 
+
+
+; Condition code modes generated by vector fp comparisons.  These will
+; be used also in single element mode.
+(define_mode_iterator VFCMP [CCVEQ CCVFH CCVFHE])
+; Used with VFCMP to expand part of the mnemonic
+; For fp we have a mismatch: eq in the insn name - e in asm
+(define_mode_attr asm_fcmp [(CCVEQ "e") (CCVFH "h") (CCVFHE "he")])
+(define_mode_attr insn_cmp [(CCVEQ "eq") (CCVH "h") (CCVHU "hl") (CCVFH "h") (CCVFHE "he")])
+
+
+(include "vector.md")
+
 ;;
 ;;- Compare instructions.
 ;;
@@ -1091,6 +1231,15 @@
    [(set_attr "op_type" "RRE,RXE")
     (set_attr "type"  "fsimp<mode>")])
 
+; wfcedbs, wfchdbs, wfchedbs
+(define_insn "*vec_cmp<insn_cmp>df_cconly"
+  [(set (reg:VFCMP CC_REGNUM)
+	(compare:VFCMP (match_operand:DF 0 "register_operand" "v")
+		       (match_operand:DF 1 "register_operand" "v")))
+   (clobber (match_scratch:V2DI 2 "=v"))]
+  "TARGET_VX && TARGET_HARD_FLOAT"
+  "wfc<asm_fcmp>dbs\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
 
 ; Compare and Branch instructions
 
@@ -1216,17 +1365,27 @@
 ; movti instruction pattern(s).
 ;
 
+; FIXME: More constants are possible by enabling jxx, jyy constraints
+; for TImode (use double-int for the calculations)
 (define_insn "movti"
-  [(set (match_operand:TI 0 "nonimmediate_operand" "=d,QS,d,o")
-        (match_operand:TI 1 "general_operand" "QS,d,dPRT,d"))]
+  [(set (match_operand:TI 0 "nonimmediate_operand" "=d,QS,v,  v,  v,v,d, v,QR,   d,o")
+        (match_operand:TI 1 "general_operand"      "QS, d,v,j00,jm1,d,v,QR, v,dPRT,d"))]
   "TARGET_ZARCH"
   "@
    lmg\t%0,%N0,%S1
    stmg\t%1,%N1,%S0
+   vlr\t%v0,%v1
+   vzero\t%v0
+   vone\t%v0
+   vlvgp\t%v0,%1,%N1
+   #
+   vl\t%v0,%1
+   vst\t%v1,%0
    #
    #"
-  [(set_attr "op_type" "RSY,RSY,*,*")
-   (set_attr "type" "lm,stm,*,*")])
+  [(set_attr "op_type" "RSY,RSY,VRR,VRI,VRI,VRR,*,VRX,VRX,*,*")
+   (set_attr "type" "lm,stm,*,*,*,*,*,*,*,*,*")
+   (set_attr "cpu_facility" "*,*,vec,vec,vec,vec,vec,vec,vec,*,*")])
 
 (define_split
   [(set (match_operand:TI 0 "nonimmediate_operand" "")
@@ -1256,10 +1415,14 @@
   operands[5] = operand_subword (operands[1], 0, 0, TImode);
 })
 
+; Use part of the TImode target reg to perform the address
+; calculation.  If the TImode value is supposed to be copied into a VR
+; this splitter is not necessary.
 (define_split
   [(set (match_operand:TI 0 "register_operand" "")
         (match_operand:TI 1 "memory_operand" ""))]
   "TARGET_ZARCH && reload_completed
+   && !VECTOR_REG_P (operands[0])
    && !s_operand (operands[1], VOIDmode)"
   [(set (match_dup 0) (match_dup 1))]
 {
@@ -1270,6 +1433,25 @@
 })
 
 
+; Split a VR -> GPR TImode move into 2 vector load GR from VR element.
+; For the higher order bits we do simply a DImode move while the
+; second part is done via vec extract.  Both will end up as vlgvg.
+(define_split
+  [(set (match_operand:TI 0 "register_operand" "")
+        (match_operand:TI 1 "register_operand" ""))]
+  "TARGET_VX && reload_completed
+   && GENERAL_REG_P (operands[0])
+   && VECTOR_REG_P (operands[1])"
+  [(set (match_dup 2) (match_dup 4))
+   (set (match_dup 3) (unspec:DI [(match_dup 5) (const_int 1)]
+				 UNSPEC_VEC_EXTRACT))]
+{
+  operands[2] = operand_subword (operands[0], 0, 0, TImode);
+  operands[3] = operand_subword (operands[0], 1, 0, TImode);
+  operands[4] = gen_rtx_REG (DImode, REGNO (operands[1]));
+  operands[5] = gen_rtx_REG (V2DImode, REGNO (operands[1]));
+})
+
 ;
 ; Patterns used for secondary reloads
 ;
@@ -1278,40 +1460,20 @@
 ; Unfortunately there is no such variant for QI, TI and FP mode moves.
 ; These patterns are also used for unaligned SI and DI accesses.
 
-(define_expand "reload<INTALL:mode><P:mode>_tomem_z10"
-  [(parallel [(match_operand:INTALL 0 "memory_operand"   "")
-	      (match_operand:INTALL 1 "register_operand" "=d")
-	      (match_operand:P 2 "register_operand" "=&a")])]
-  "TARGET_Z10"
-{
-  s390_reload_symref_address (operands[1], operands[0], operands[2], 1);
-  DONE;
-})
-
-(define_expand "reload<INTALL:mode><P:mode>_toreg_z10"
-  [(parallel [(match_operand:INTALL 0 "register_operand" "=d")
-	      (match_operand:INTALL 1 "memory_operand"   "")
-	      (match_operand:P 2 "register_operand" "=a")])]
-  "TARGET_Z10"
-{
-  s390_reload_symref_address (operands[0], operands[1], operands[2], 0);
-  DONE;
-})
-
-(define_expand "reload<FPALL:mode><P:mode>_tomem_z10"
-  [(parallel [(match_operand:FPALL 0 "memory_operand"   "")
-	      (match_operand:FPALL 1 "register_operand" "=d")
-	      (match_operand:P 2 "register_operand" "=&a")])]
+(define_expand "reload<ALL:mode><P:mode>_tomem_z10"
+  [(parallel [(match_operand:ALL 0 "memory_operand"   "")
+	      (match_operand:ALL 1 "register_operand" "=d")
+	      (match_operand:P   2 "register_operand" "=&a")])]
   "TARGET_Z10"
 {
   s390_reload_symref_address (operands[1], operands[0], operands[2], 1);
   DONE;
 })
 
-(define_expand "reload<FPALL:mode><P:mode>_toreg_z10"
-  [(parallel [(match_operand:FPALL 0 "register_operand" "=d")
-	      (match_operand:FPALL 1 "memory_operand"   "")
-	      (match_operand:P 2 "register_operand" "=a")])]
+(define_expand "reload<ALL:mode><P:mode>_toreg_z10"
+  [(parallel [(match_operand:ALL 0 "register_operand" "=d")
+	      (match_operand:ALL 1 "memory_operand"   "")
+	      (match_operand:P   2 "register_operand" "=a")])]
   "TARGET_Z10"
 {
   s390_reload_symref_address (operands[0], operands[1], operands[2], 0);
@@ -1340,9 +1502,16 @@
   DONE;
 })
 
-; Handles assessing a non-offsetable memory address
+; Not all the indirect memory access instructions support the full
+; format (long disp + index + base).  So whenever a move from/to such
+; an address is required and the instruction cannot deal with it we do
+; a load address into a scratch register first and use this as the new
+; base register.
+; This in particular is used for:
+; - non-offsetable memory accesses for multiword moves
+; - full vector reg moves with long displacements
 
-(define_expand "reload<mode>_nonoffmem_in"
+(define_expand "reload<mode>_la_in"
   [(parallel [(match_operand 0   "register_operand" "")
               (match_operand 1   "" "")
               (match_operand:P 2 "register_operand" "=&a")])]
@@ -1355,7 +1524,7 @@
   DONE;
 })
 
-(define_expand "reload<mode>_nonoffmem_out"
+(define_expand "reload<mode>_la_out"
   [(parallel [(match_operand   0 "" "")
               (match_operand   1 "register_operand" "")
               (match_operand:P 2 "register_operand" "=&a")])]
@@ -1408,11 +1577,9 @@
 
 (define_insn "*movdi_64"
   [(set (match_operand:DI 0 "nonimmediate_operand"
-                            "=d,d,d,d,d,d,d,d,f,d,d,d,d,d,
-                             RT,!*f,!*f,!*f,!R,!T,b,Q,d,t,Q,t")
+         "=d,    d,    d,    d,    d, d,    d,    d,f,d,d,d,d, d,RT,!*f,!*f,!*f,!R,!T,b,Q,d,t,Q,t,v,v,v,d, v,QR")
         (match_operand:DI 1 "general_operand"
-                            "K,N0HD0,N1HD0,N2HD0,N3HD0,Os,N0SD0,N1SD0,d,f,L,b,d,RT,
-                             d,*f,R,T,*f,*f,d,K,t,d,t,Q"))]
+         " K,N0HD0,N1HD0,N2HD0,N3HD0,Os,N0SD0,N1SD0,d,f,L,b,d,RT, d, *f,  R,  T,*f,*f,d,K,t,d,t,Q,K,v,d,v,QR, v"))]
   "TARGET_ZARCH"
   "@
    lghi\t%0,%h1
@@ -1440,15 +1607,21 @@
    #
    #
    stam\t%1,%N1,%S0
-   lam\t%0,%N0,%S1"
+   lam\t%0,%N0,%S1
+   vleig\t%v0,%h1,0
+   vlr\t%v0,%v1
+   vlvgg\t%v0,%1,0
+   vlgvg\t%0,%v1,0
+   vleg\t%v0,%1,0
+   vsteg\t%v1,%0,0"
   [(set_attr "op_type" "RI,RI,RI,RI,RI,RIL,RIL,RIL,RRE,RRE,RXY,RIL,RRE,RXY,
-                        RXY,RR,RX,RXY,RX,RXY,RIL,SIL,*,*,RS,RS")
+                        RXY,RR,RX,RXY,RX,RXY,RIL,SIL,*,*,RS,RS,VRI,VRR,VRS,VRS,VRX,VRX")
    (set_attr "type" "*,*,*,*,*,*,*,*,floaddf,floaddf,la,larl,lr,load,store,
-                     floaddf,floaddf,floaddf,fstoredf,fstoredf,larl,*,*,*,
-                     *,*")
+                     floaddf,floaddf,floaddf,fstoredf,fstoredf,larl,*,*,*,*,
+                     *,*,*,*,*,*,*")
    (set_attr "cpu_facility" "*,*,*,*,*,extimm,extimm,extimm,dfp,dfp,longdisp,
                              z10,*,*,*,*,*,longdisp,*,longdisp,
-                             z10,z10,*,*,*,*")
+                             z10,z10,*,*,*,*,vec,vec,vec,vec,vec,vec")
    (set_attr "z10prop" "z10_fwd_A1,
                         z10_fwd_E1,
                         z10_fwd_E1,
@@ -1474,7 +1647,7 @@
                         *,
                         *,
                         *,
-                        *")
+                        *,*,*,*,*,*,*")
 ])
 
 (define_split
@@ -1666,9 +1839,9 @@
 
 (define_insn "*movsi_zarch"
   [(set (match_operand:SI 0 "nonimmediate_operand"
-			    "=d,d,d,d,d,d,d,d,d,R,T,!*f,!*f,!*f,!R,!T,d,t,Q,b,Q,t")
+	 "=d,    d,    d, d,d,d,d,d,d,R,T,!*f,!*f,!*f,!*f,!*f,!R,!T,d,t,Q,b,Q,t,v,v,v,d, v,QR")
         (match_operand:SI 1 "general_operand"
-			    "K,N0HS0,N1HS0,Os,L,b,d,R,T,d,d,*f,R,T,*f,*f,t,d,t,d,K,Q"))]
+	 " K,N0HS0,N1HS0,Os,L,b,d,R,T,d,d, *f, *f,  R,  R,  T,*f,*f,t,d,t,d,K,Q,K,v,d,v,QR, v"))]
   "TARGET_ZARCH"
   "@
    lhi\t%0,%h1
@@ -1682,7 +1855,9 @@
    ly\t%0,%1
    st\t%1,%0
    sty\t%1,%0
+   lder\t%0,%1
    ler\t%0,%1
+   lde\t%0,%1
    le\t%0,%1
    ley\t%0,%1
    ste\t%1,%0
@@ -1692,9 +1867,15 @@
    stam\t%1,%1,%S0
    strl\t%1,%0
    mvhi\t%0,%1
-   lam\t%0,%0,%S1"
+   lam\t%0,%0,%S1
+   vleif\t%v0,%h1,0
+   vlr\t%v0,%v1
+   vlvgf\t%v0,%1,0
+   vlgvf\t%0,%v1,0
+   vlef\t%v0,%1,0
+   vstef\t%v1,%0,0"
   [(set_attr "op_type" "RI,RI,RI,RIL,RXY,RIL,RR,RX,RXY,RX,RXY,
-                        RR,RX,RXY,RX,RXY,RRE,RRE,RS,RIL,SIL,RS")
+                        RRE,RR,RXE,RX,RXY,RX,RXY,RRE,RRE,RS,RIL,SIL,RS,VRI,VRR,VRS,VRS,VRX,VRX")
    (set_attr "type" "*,
                      *,
                      *,
@@ -1709,6 +1890,8 @@
                      floadsf,
                      floadsf,
                      floadsf,
+                     floadsf,
+                     floadsf,
                      fstoresf,
                      fstoresf,
                      *,
@@ -1716,9 +1899,9 @@
                      *,
                      larl,
                      *,
-                     *")
+                     *,*,*,*,*,*,*")
    (set_attr "cpu_facility" "*,*,*,extimm,longdisp,z10,*,*,longdisp,*,longdisp,
-                             *,*,longdisp,*,longdisp,*,*,*,z10,z10,*")
+                             vec,*,vec,*,longdisp,*,longdisp,*,*,*,z10,z10,*,vec,vec,vec,vec,vec,vec")
    (set_attr "z10prop" "z10_fwd_A1,
                         z10_fwd_E1,
                         z10_fwd_E1,
@@ -1735,42 +1918,38 @@
                         *,
                         *,
                         *,
+                        *,
+                        *,
                         z10_super_E1,
                         z10_super,
                         *,
                         z10_rec,
                         z10_super,
-                        *")])
+                        *,*,*,*,*,*,*")])
 
 (define_insn "*movsi_esa"
-  [(set (match_operand:SI 0 "nonimmediate_operand" "=d,d,d,R,!*f,!*f,!R,d,t,Q,t")
-        (match_operand:SI 1 "general_operand" "K,d,R,d,*f,R,*f,t,d,t,Q"))]
+  [(set (match_operand:SI 0 "nonimmediate_operand" "=d,d,d,R,!*f,!*f,!*f,!*f,!R,d,t,Q,t")
+        (match_operand:SI 1 "general_operand"       "K,d,R,d, *f, *f,  R,  R,*f,t,d,t,Q"))]
   "!TARGET_ZARCH"
   "@
    lhi\t%0,%h1
    lr\t%0,%1
    l\t%0,%1
    st\t%1,%0
+   lder\t%0,%1
    ler\t%0,%1
+   lde\t%0,%1
    le\t%0,%1
    ste\t%1,%0
    ear\t%0,%1
    sar\t%0,%1
    stam\t%1,%1,%S0
    lam\t%0,%0,%S1"
-  [(set_attr "op_type" "RI,RR,RX,RX,RR,RX,RX,RRE,RRE,RS,RS")
-   (set_attr "type" "*,lr,load,store,floadsf,floadsf,fstoresf,*,*,*,*")
-   (set_attr "z10prop" "z10_fwd_A1,
-                        z10_fr_E1,
-                        z10_fwd_A3,
-                        z10_rec,
-                        *,
-                        *,
-                        *,
-                        z10_super_E1,
-                        z10_super,
-                        *,
-                        *")
+  [(set_attr "op_type" "RI,RR,RX,RX,RRE,RR,RXE,RX,RX,RRE,RRE,RS,RS")
+   (set_attr "type" "*,lr,load,store,floadsf,floadsf,floadsf,floadsf,fstoresf,*,*,*,*")
+   (set_attr "z10prop" "z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_rec,*,*,*,*,*,z10_super_E1,
+                        z10_super,*,*")
+   (set_attr "cpu_facility" "*,*,*,*,vec,*,vec,*,*,*,*,*,*")
 ])
 
 (define_peephole2
@@ -1880,8 +2059,8 @@
 })
 
 (define_insn "*movhi"
-  [(set (match_operand:HI 0 "nonimmediate_operand" "=d,d,d,d,d,R,T,b,Q")
-        (match_operand:HI 1 "general_operand"      " d,n,R,T,b,d,d,d,K"))]
+  [(set (match_operand:HI 0 "nonimmediate_operand" "=d,d,d,d,d,R,T,b,Q,v,v,v,d, v,QR")
+        (match_operand:HI 1 "general_operand"      " d,n,R,T,b,d,d,d,K,K,v,d,v,QR, v"))]
   ""
   "@
    lr\t%0,%1
@@ -1892,10 +2071,16 @@
    sth\t%1,%0
    sthy\t%1,%0
    sthrl\t%1,%0
-   mvhhi\t%0,%1"
-  [(set_attr "op_type"      "RR,RI,RX,RXY,RIL,RX,RXY,RIL,SIL")
-   (set_attr "type"         "lr,*,*,*,larl,store,store,store,*")
-   (set_attr "cpu_facility" "*,*,*,*,z10,*,*,z10,z10")
+   mvhhi\t%0,%1
+   vleih\t%v0,%h1,0
+   vlr\t%v0,%v1
+   vlvgh\t%v0,%1,0
+   vlgvh\t%0,%v1,0
+   vleh\t%v0,%1,0
+   vsteh\t%v1,%0,0"
+  [(set_attr "op_type"      "RR,RI,RX,RXY,RIL,RX,RXY,RIL,SIL,VRI,VRR,VRS,VRS,VRX,VRX")
+   (set_attr "type"         "lr,*,*,*,larl,store,store,store,*,*,*,*,*,*,*")
+   (set_attr "cpu_facility" "*,*,*,*,z10,*,*,z10,z10,vec,vec,vec,vec,vec,vec")
    (set_attr "z10prop" "z10_fr_E1,
                        z10_fwd_A1,
                        z10_super_E1,
@@ -1904,7 +2089,7 @@
                        z10_rec,
                        z10_rec,
                        z10_rec,
-                       z10_super")])
+                       z10_super,*,*,*,*,*,*")])
 
 (define_peephole2
   [(set (match_operand:HI 0 "register_operand" "")
@@ -1939,8 +2124,8 @@
 })
 
 (define_insn "*movqi"
-  [(set (match_operand:QI 0 "nonimmediate_operand" "=d,d,d,d,R,T,Q,S,?Q")
-        (match_operand:QI 1 "general_operand"      " d,n,R,T,d,d,n,n,?Q"))]
+  [(set (match_operand:QI 0 "nonimmediate_operand" "=d,d,d,d,R,T,Q,S,?Q,v,v,v,d, v,QR")
+        (match_operand:QI 1 "general_operand"      " d,n,R,T,d,d,n,n,?Q,K,v,d,v,QR, v"))]
   ""
   "@
    lr\t%0,%1
@@ -1951,9 +2136,16 @@
    stcy\t%1,%0
    mvi\t%S0,%b1
    mviy\t%S0,%b1
-   #"
-  [(set_attr "op_type" "RR,RI,RX,RXY,RX,RXY,SI,SIY,SS")
-   (set_attr "type" "lr,*,*,*,store,store,store,store,*")
+   #
+   vleib\t%v0,%b1,0
+   vlr\t%v0,%v1
+   vlvgb\t%v0,%1,0
+   vlgvb\t%0,%v1,0
+   vleb\t%v0,%1,0
+   vsteb\t%v1,%0,0"
+  [(set_attr "op_type" "RR,RI,RX,RXY,RX,RXY,SI,SIY,SS,VRI,VRR,VRS,VRS,VRX,VRX")
+   (set_attr "type" "lr,*,*,*,store,store,store,store,*,*,*,*,*,*,*")
+   (set_attr "cpu_facility" "*,*,*,*,*,*,*,*,*,vec,vec,vec,vec,vec,vec")
    (set_attr "z10prop" "z10_fr_E1,
                         z10_fwd_A1,
                         z10_super_E1,
@@ -1962,7 +2154,7 @@
                         z10_rec,
                         z10_super,
                         z10_super,
-                        *")])
+                        *,*,*,*,*,*,*")])
 
 (define_peephole2
   [(set (match_operand:QI 0 "nonimmediate_operand" "")
@@ -2094,7 +2286,7 @@
   [(set (match_operand:TD_TF 0 "register_operand" "")
         (match_operand:TD_TF 1 "memory_operand"   ""))]
   "TARGET_ZARCH && reload_completed
-   && !FP_REG_P (operands[0])
+   && GENERAL_REG_P (operands[0])
    && !s_operand (operands[1], VOIDmode)"
   [(set (match_dup 0) (match_dup 1))]
 {
@@ -2150,9 +2342,9 @@
 
 (define_insn "*mov<mode>_64dfp"
   [(set (match_operand:DD_DF 0 "nonimmediate_operand"
-			       "=f,f,f,d,f,f,R,T,d,d, d,RT")
+			       "=f,f,f,d,f,f,R,T,d,d,d, d,b,RT,v,v,d,v,QR")
         (match_operand:DD_DF 1 "general_operand"
-			       " G,f,d,f,R,T,f,f,G,d,RT, d"))]
+			       " G,f,d,f,R,T,f,f,G,d,b,RT,d, d,v,d,v,QR,v"))]
   "TARGET_DFP"
   "@
    lzdr\t%0
@@ -2165,17 +2357,24 @@
    stdy\t%1,%0
    lghi\t%0,0
    lgr\t%0,%1
+   lgrl\t%0,%1
    lg\t%0,%1
-   stg\t%1,%0"
-  [(set_attr "op_type" "RRE,RR,RRE,RRE,RX,RXY,RX,RXY,RI,RRE,RXY,RXY")
+   stgrl\t%1,%0
+   stg\t%1,%0
+   vlr\t%v0,%v1
+   vlvgg\t%v0,%1,0
+   vlgvg\t%0,%v1,0
+   vleg\t%0,%1,0
+   vsteg\t%1,%0,0"
+  [(set_attr "op_type" "RRE,RR,RRE,RRE,RX,RXY,RX,RXY,RI,RRE,RIL,RXY,RIL,RXY,VRR,VRS,VRS,VRX,VRX")
    (set_attr "type" "fsimpdf,floaddf,floaddf,floaddf,floaddf,floaddf,
-                     fstoredf,fstoredf,*,lr,load,store")
-   (set_attr "z10prop" "*,*,*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_rec")
-   (set_attr "cpu_facility" "z196,*,*,*,*,*,*,*,*,*,*,*")])
+                     fstoredf,fstoredf,*,lr,load,load,store,store,*,*,*,load,store")
+   (set_attr "z10prop" "*,*,*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec,*,*,*,*,*")
+   (set_attr "cpu_facility" "z196,*,*,*,*,*,*,*,*,*,z10,*,z10,*,vec,vec,vec,vec,vec")])
 
 (define_insn "*mov<mode>_64"
-  [(set (match_operand:DD_DF 0 "nonimmediate_operand" "=f,f,f,f,R,T,d,d, d,RT")
-        (match_operand:DD_DF 1 "general_operand"      " G,f,R,T,f,f,G,d,RT, d"))]
+  [(set (match_operand:DD_DF 0 "nonimmediate_operand" "=f,f,f,f,R,T,d,d,d, d,b,RT,v,v,QR")
+        (match_operand:DD_DF 1 "general_operand"      " G,f,R,T,f,f,G,d,b,RT,d, d,v,QR,v"))]
   "TARGET_ZARCH"
   "@
    lzdr\t%0
@@ -2186,13 +2385,18 @@
    stdy\t%1,%0
    lghi\t%0,0
    lgr\t%0,%1
+   lgrl\t%0,%1
    lg\t%0,%1
-   stg\t%1,%0"
-  [(set_attr "op_type" "RRE,RR,RX,RXY,RX,RXY,RI,RRE,RXY,RXY")
+   stgrl\t%1,%0
+   stg\t%1,%0
+   vlr\t%v0,%v1
+   vleg\t%v0,%1,0
+   vsteg\t%v1,%0,0"
+  [(set_attr "op_type" "RRE,RR,RX,RXY,RX,RXY,RI,RRE,RIL,RXY,RIL,RXY,VRR,VRX,VRX")
    (set_attr "type"    "fsimpdf,fload<mode>,fload<mode>,fload<mode>,
-                        fstore<mode>,fstore<mode>,*,lr,load,store")
-   (set_attr "z10prop" "*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_rec")
-   (set_attr "cpu_facility" "z196,*,*,*,*,*,*,*,*,*")])
+                        fstore<mode>,fstore<mode>,*,lr,load,load,store,store,*,load,store")
+   (set_attr "z10prop" "*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec,*,*,*")
+   (set_attr "cpu_facility" "z196,*,*,*,*,*,*,*,z10,*,z10,*,vec,vec,vec")])
 
 (define_insn "*mov<mode>_31"
   [(set (match_operand:DD_DF 0 "nonimmediate_operand"
@@ -2265,28 +2469,38 @@
 
 (define_insn "mov<mode>"
   [(set (match_operand:SD_SF 0 "nonimmediate_operand"
-			       "=f,f,f,f,R,T,d,d,d,d,R,T")
+			       "=f,f,f,f,f,f,R,T,d,d,d,d,d,b,R,T,v,v,v,d,v,QR")
         (match_operand:SD_SF 1 "general_operand"
-			       " G,f,R,T,f,f,G,d,R,T,d,d"))]
+			       " G,f,f,R,R,T,f,f,G,d,b,R,T,d,d,d,v,G,d,v,QR,v"))]
   ""
   "@
    lzer\t%0
+   lder\t%0,%1
    ler\t%0,%1
+   lde\t%0,%1
    le\t%0,%1
    ley\t%0,%1
    ste\t%1,%0
    stey\t%1,%0
    lhi\t%0,0
    lr\t%0,%1
+   lrl\t%0,%1
    l\t%0,%1
    ly\t%0,%1
+   strl\t%1,%0
    st\t%1,%0
-   sty\t%1,%0"
-  [(set_attr "op_type" "RRE,RR,RX,RXY,RX,RXY,RI,RR,RX,RXY,RX,RXY")
-   (set_attr "type"    "fsimpsf,fload<mode>,fload<mode>,fload<mode>,
-                        fstore<mode>,fstore<mode>,*,lr,load,load,store,store")
-   (set_attr "z10prop" "*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec")
-   (set_attr "cpu_facility" "z196,*,*,*,*,*,*,*,*,*,*,*")])
+   sty\t%1,%0
+   vlr\t%v0,%v1
+   vleif\t%v0,0
+   vlvgf\t%v0,%1,0
+   vlgvf\t%0,%v1,0
+   vleg\t%0,%1,0
+   vsteg\t%1,%0,0"
+  [(set_attr "op_type" "RRE,RRE,RR,RXE,RX,RXY,RX,RXY,RI,RR,RIL,RX,RXY,RIL,RX,RXY,VRR,VRI,VRS,VRS,VRX,VRX")
+   (set_attr "type"    "fsimpsf,fsimpsf,fload<mode>,fload<mode>,fload<mode>,fload<mode>,
+                        fstore<mode>,fstore<mode>,*,lr,load,load,load,store,store,store,*,*,*,*,load,store")
+   (set_attr "z10prop" "*,*,*,*,*,*,*,*,z10_fwd_A1,z10_fr_E1,z10_fr_E1,z10_fwd_A3,z10_fwd_A3,z10_rec,z10_rec,z10_rec,*,*,*,*,*,*")
+   (set_attr "cpu_facility" "z196,vec,*,vec,*,*,*,*,*,*,z10,*,*,z10,*,*,vec,vec,vec,vec,vec,vec")])
 
 ;
 ; movcc instruction pattern
@@ -2577,6 +2791,22 @@
 ;
 
 (define_expand "strlen<mode>"
+  [(match_operand:P   0 "register_operand" "")  ; result
+   (match_operand:BLK 1 "memory_operand" "")    ; input string
+   (match_operand:SI  2 "immediate_operand" "") ; search character
+   (match_operand:SI  3 "immediate_operand" "")] ; known alignment
+  ""
+{
+  if (!TARGET_VX || operands[2] != const0_rtx)
+    emit_insn (gen_strlen_srst<mode> (operands[0], operands[1],
+				      operands[2], operands[3]));
+  else
+    s390_expand_vec_strlen (operands[0], operands[1], operands[3]);
+
+  DONE;
+})
+
+(define_expand "strlen_srst<mode>"
   [(set (reg:SI 0) (match_operand:SI 2 "immediate_operand" ""))
    (parallel
     [(set (match_dup 4)
@@ -2674,8 +2904,16 @@
      (clobber (reg:CC CC_REGNUM))])]
   ""
 {
-  rtx addr1 = gen_reg_rtx (Pmode);
-  rtx addr2 = gen_reg_rtx (Pmode);
+  rtx addr1, addr2;
+
+  if (TARGET_VX && optimize_function_for_speed_p (cfun))
+    {
+      s390_expand_vec_movstr (operands[0], operands[1], operands[2]);
+      DONE;
+    }
+
+  addr1 = gen_reg_rtx (Pmode);
+  addr2 = gen_reg_rtx (Pmode);
 
   emit_move_insn (addr1, force_operand (XEXP (operands[1], 0), NULL_RTX));
   emit_move_insn (addr2, force_operand (XEXP (operands[2], 0), NULL_RTX));
@@ -2886,8 +3124,12 @@
   operands[2] = GEN_INT (S390_TDC_INFINITY);
 })
 
+; This extracts CC into a GPR properly shifted.  The actual IPM
+; instruction will be issued by reload.  The constraint of operand 1
+; forces reload to use a GPR.  So reload will issue a movcc insn for
+; copying CC into a GPR first.
 (define_insn_and_split "*cc_to_int"
-  [(set (match_operand:SI 0 "register_operand" "=d")
+  [(set (match_operand:SI 0 "nonimmediate_operand"     "=d")
         (unspec:SI [(match_operand 1 "register_operand" "0")]
                    UNSPEC_CC_TO_INT))]
   "operands != NULL"
@@ -4223,14 +4465,27 @@
 
 ; fixuns_trunc(tf|df|sf|td|dd)(di|si)2 instruction patterns.
 
+(define_insn "*fixuns_truncdfdi2_z13"
+  [(set (match_operand:DI                  0 "register_operand" "=d,v")
+	(unsigned_fix:DI (match_operand:DF 1 "register_operand"  "f,v")))
+   (unspec:DI [(match_operand:DI           2 "immediate_operand" "K,K")] UNSPEC_ROUND)
+   (clobber (reg:CC CC_REGNUM))]
+   "TARGET_VX && TARGET_HARD_FLOAT"
+   "@
+    clgdbr\t%0,%h2,%1,0
+    wclgdb\t%v0,%v1,0,%h2"
+   [(set_attr "op_type" "RRF,VRR")
+    (set_attr "type"    "ftoi")])
+
 ; clfebr, clfdbr, clfxbr, clgebr, clgdbr, clgxbr
 ;         clfdtr, clfxtr,         clgdtr, clgxtr
 (define_insn "*fixuns_trunc<FP:mode><GPR:mode>2_z196"
-  [(set (match_operand:GPR 0 "register_operand" "=r")
-	(unsigned_fix:GPR (match_operand:FP 1 "register_operand" "f")))
-   (unspec:GPR [(match_operand:GPR 2 "immediate_operand" "K")] UNSPEC_ROUND)
+  [(set (match_operand:GPR                  0 "register_operand" "=d")
+	(unsigned_fix:GPR (match_operand:FP 1 "register_operand"  "f")))
+   (unspec:GPR [(match_operand:GPR          2 "immediate_operand" "K")] UNSPEC_ROUND)
    (clobber (reg:CC CC_REGNUM))]
-   "TARGET_Z196"
+   "TARGET_Z196 && TARGET_HARD_FLOAT
+    && (!TARGET_VX || <GPR:MODE>mode != DImode || <FP:MODE>mode != DFmode)"
    "cl<GPR:gf><FP:xde><FP:bt>r\t%0,%h2,%1,0"
    [(set_attr "op_type" "RRF")
     (set_attr "type"    "ftoi")])
@@ -4245,18 +4500,37 @@
   DONE;
 })
 
+(define_insn "*fix_truncdfdi2_bfp_z13"
+  [(set (match_operand:DI         0 "register_operand" "=d,v")
+        (fix:DI (match_operand:DF 1 "register_operand"  "f,v")))
+   (unspec:DI [(match_operand:DI  2 "immediate_operand" "K,K")] UNSPEC_ROUND)
+   (clobber (reg:CC CC_REGNUM))]
+  "TARGET_VX && TARGET_HARD_FLOAT"
+  "@
+   cgdbr\t%0,%h2,%1
+   wcgdb\t%v0,%v1,0,%h2"
+  [(set_attr "op_type" "RRE,VRR")
+   (set_attr "type"    "ftoi")])
+
 ; cgxbr, cgdbr, cgebr, cfxbr, cfdbr, cfebr
-(define_insn "fix_trunc<BFP:mode><GPR:mode>2_bfp"
-  [(set (match_operand:GPR 0 "register_operand" "=d")
-        (fix:GPR (match_operand:BFP 1 "register_operand" "f")))
-   (unspec:GPR [(match_operand:GPR 2 "immediate_operand" "K")] UNSPEC_ROUND)
+(define_insn "*fix_trunc<BFP:mode><GPR:mode>2_bfp"
+  [(set (match_operand:GPR          0 "register_operand" "=d")
+        (fix:GPR (match_operand:BFP 1 "register_operand"  "f")))
+   (unspec:GPR [(match_operand:GPR  2 "immediate_operand" "K")] UNSPEC_ROUND)
    (clobber (reg:CC CC_REGNUM))]
-  "TARGET_HARD_FLOAT"
+  "TARGET_HARD_FLOAT
+    && (!TARGET_VX || <GPR:MODE>mode != DImode || <BFP:MODE>mode != DFmode)"
   "c<GPR:gf><BFP:xde>br\t%0,%h2,%1"
   [(set_attr "op_type" "RRE")
    (set_attr "type"    "ftoi")])
 
-
+(define_expand "fix_trunc<BFP:mode><GPR:mode>2_bfp"
+  [(parallel
+    [(set (match_operand:GPR          0 "register_operand" "=d")
+	  (fix:GPR (match_operand:BFP 1 "register_operand"  "f")))
+     (unspec:GPR [(match_operand:GPR  2 "immediate_operand" "K")] UNSPEC_ROUND)
+     (clobber (reg:CC CC_REGNUM))])]
+  "TARGET_HARD_FLOAT")
 ;
 ; fix_trunc(td|dd)di2 instruction pattern(s).
 ;
@@ -4303,12 +4577,15 @@
 
 ; cxgbr, cdgbr, cegbr, cxgtr, cdgtr
 (define_insn "floatdi<mode>2"
-  [(set (match_operand:FP 0 "register_operand" "=f")
-        (float:FP (match_operand:DI 1 "register_operand" "d")))]
+  [(set (match_operand:FP           0 "register_operand" "=f,<vf>")
+        (float:FP (match_operand:DI 1 "register_operand"  "d,<vd>")))]
   "TARGET_ZARCH && TARGET_HARD_FLOAT"
-  "c<xde>g<bt>r\t%0,%1"
-  [(set_attr "op_type" "RRE")
-   (set_attr "type"    "itof<mode>" )])
+  "@
+   c<xde>g<bt>r\t%0,%1
+   wcdgb\t%v0,%v1,0,0"
+  [(set_attr "op_type"      "RRE,VRR")
+   (set_attr "type"         "itof<mode>" )
+   (set_attr "cpu_facility" "*,vec")])
 
 ; cxfbr, cdfbr, cefbr
 (define_insn "floatsi<mode>2"
@@ -4332,27 +4609,47 @@
 ; floatuns(si|di)(tf|df|sf|td|dd)2 instruction pattern(s).
 ;
 
+(define_insn "*floatunsdidf2_z13"
+  [(set (match_operand:DF                    0 "register_operand" "=f,v")
+        (unsigned_float:DF (match_operand:DI 1 "register_operand"  "d,v")))]
+  "TARGET_VX && TARGET_HARD_FLOAT"
+  "@
+   cdlgbr\t%0,0,%1,0
+   wcdlgb\t%v0,%v1,0,0"
+  [(set_attr "op_type" "RRE,VRR")
+   (set_attr "type"    "itofdf")])
+
 ; cxlgbr, cdlgbr, celgbr, cxlgtr, cdlgtr
 ; cxlfbr, cdlfbr, celfbr, cxlftr, cdlftr
-(define_insn "floatuns<GPR:mode><FP:mode>2"
-  [(set (match_operand:FP 0 "register_operand" "=f")
-        (unsigned_float:FP (match_operand:GPR 1 "register_operand" "d")))]
-  "TARGET_Z196 && TARGET_HARD_FLOAT"
+(define_insn "*floatuns<GPR:mode><FP:mode>2"
+  [(set (match_operand:FP                     0 "register_operand" "=f")
+        (unsigned_float:FP (match_operand:GPR 1 "register_operand"  "d")))]
+  "TARGET_Z196 && TARGET_HARD_FLOAT
+   && (!TARGET_VX || <FP:MODE>mode != DFmode || <GPR:MODE>mode != DImode)"
   "c<FP:xde>l<GPR:gf><FP:bt>r\t%0,0,%1,0"
   [(set_attr "op_type" "RRE")
-   (set_attr "type"    "itof<FP:mode>" )])
+   (set_attr "type"    "itof<FP:mode>")])
+
+(define_expand "floatuns<GPR:mode><FP:mode>2"
+  [(set (match_operand:FP                     0 "register_operand" "")
+        (unsigned_float:FP (match_operand:GPR 1 "register_operand" "")))]
+  "TARGET_Z196 && TARGET_HARD_FLOAT")
 
 ;
 ; truncdfsf2 instruction pattern(s).
 ;
 
 (define_insn "truncdfsf2"
-  [(set (match_operand:SF 0 "register_operand" "=f")
-        (float_truncate:SF (match_operand:DF 1 "register_operand" "f")))]
+  [(set (match_operand:SF                    0 "register_operand" "=f,v")
+        (float_truncate:SF (match_operand:DF 1 "register_operand"  "f,v")))]
   "TARGET_HARD_FLOAT"
-  "ledbr\t%0,%1"
-  [(set_attr "op_type"  "RRE")
-   (set_attr "type"   "ftruncdf")])
+  "@
+   ledbr\t%0,%1
+   wledb\t%v0,%v1,0,0" ; IEEE inexact exception not suppressed
+                       ; According to BFP rounding mode
+  [(set_attr "op_type"      "RRE,VRR")
+   (set_attr "type"         "ftruncdf")
+   (set_attr "cpu_facility" "*,vec")])
 
 ;
 ; trunctf(df|sf)2 instruction pattern(s).
@@ -4393,17 +4690,35 @@
 ; extend(sf|df)(df|tf)2 instruction pattern(s).
 ;
 
+(define_insn "*extendsfdf2_z13"
+  [(set (match_operand:DF                  0 "register_operand"     "=f,f,v")
+        (float_extend:DF (match_operand:SF 1 "nonimmediate_operand"  "f,R,v")))]
+  "TARGET_VX && TARGET_HARD_FLOAT"
+  "@
+   ldebr\t%0,%1
+   ldeb\t%0,%1
+   wldeb\t%v0,%v1"
+  [(set_attr "op_type" "RRE,RXE,VRR")
+   (set_attr "type"    "fsimpdf, floaddf,fsimpdf")])
+
 ; ldebr, ldeb, lxdbr, lxdb, lxebr, lxeb
-(define_insn "extend<DSF:mode><BFP:mode>2"
-  [(set (match_operand:BFP 0 "register_operand" "=f,f")
+(define_insn "*extend<DSF:mode><BFP:mode>2"
+  [(set (match_operand:BFP                   0 "register_operand"     "=f,f")
         (float_extend:BFP (match_operand:DSF 1 "nonimmediate_operand"  "f,R")))]
   "TARGET_HARD_FLOAT
-   && GET_MODE_SIZE (<BFP:MODE>mode) > GET_MODE_SIZE (<DSF:MODE>mode)"
+   && GET_MODE_SIZE (<BFP:MODE>mode) > GET_MODE_SIZE (<DSF:MODE>mode)
+   && (!TARGET_VX || <BFP:MODE>mode != DFmode || <DSF:MODE>mode != SFmode)"
   "@
    l<BFP:xde><DSF:xde>br\t%0,%1
    l<BFP:xde><DSF:xde>b\t%0,%1"
-  [(set_attr "op_type"  "RRE,RXE")
-   (set_attr "type"   "fsimp<BFP:mode>, fload<BFP:mode>")])
+  [(set_attr "op_type" "RRE,RXE")
+   (set_attr "type"    "fsimp<BFP:mode>, fload<BFP:mode>")])
+
+(define_expand "extend<DSF:mode><BFP:mode>2"
+  [(set (match_operand:BFP                   0 "register_operand"     "")
+        (float_extend:BFP (match_operand:DSF 1 "nonimmediate_operand" "")))]
+  "TARGET_HARD_FLOAT
+   && GET_MODE_SIZE (<BFP:MODE>mode) > GET_MODE_SIZE (<DSF:MODE>mode)")
 
 ;
 ; extendddtd2 and extendsddd2 instruction pattern(s).
@@ -4616,10 +4931,29 @@
 ; addti3 instruction pattern(s).
 ;
 
-(define_insn_and_split "addti3"
-  [(set (match_operand:TI 0 "register_operand" "=&d")
+(define_expand "addti3"
+  [(parallel
+    [(set (match_operand:TI          0 "register_operand"     "")
+	  (plus:TI (match_operand:TI 1 "nonimmediate_operand" "")
+		   (match_operand:TI 2 "general_operand"      "") ) )
+     (clobber (reg:CC CC_REGNUM))])]
+  "TARGET_ZARCH"
+{
+  /* For z13 we have vaq which doesn't set CC.  */
+  if (TARGET_VX)
+    {
+      emit_insn (gen_rtx_SET (VOIDmode, operands[0],
+			      gen_rtx_PLUS (TImode,
+                                            copy_to_mode_reg (TImode, operands[1]),
+                                            copy_to_mode_reg (TImode, operands[2]))));
+      DONE;
+    }
+})
+
+(define_insn_and_split "*addti3"
+  [(set (match_operand:TI          0 "register_operand"    "=&d")
         (plus:TI (match_operand:TI 1 "nonimmediate_operand" "%0")
-                 (match_operand:TI 2 "general_operand" "do") ) )
+                 (match_operand:TI 2 "general_operand"      "do") ) )
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ZARCH"
   "#"
@@ -4639,7 +4973,9 @@
    operands[5] = operand_subword (operands[2], 0, 0, TImode);
    operands[6] = operand_subword (operands[0], 1, 0, TImode);
    operands[7] = operand_subword (operands[1], 1, 0, TImode);
-   operands[8] = operand_subword (operands[2], 1, 0, TImode);")
+   operands[8] = operand_subword (operands[2], 1, 0, TImode);"
+  [(set_attr "op_type"  "*")
+   (set_attr "cpu_facility" "*")])
 
 ;
 ; adddi3 instruction pattern(s).
@@ -4976,17 +5312,20 @@
 ;
 
 ; axbr, adbr, aebr, axb, adb, aeb, adtr, axtr
+; FIXME: wfadb does not clobber cc
 (define_insn "add<mode>3"
-  [(set (match_operand:FP 0 "register_operand"              "=f,   f")
-        (plus:FP (match_operand:FP 1 "nonimmediate_operand" "%<f0>,0")
-		 (match_operand:FP 2 "general_operand"      " f,<Rf>")))
+  [(set (match_operand:FP 0 "register_operand"                 "=f,   f,<vf>")
+        (plus:FP (match_operand:FP 1 "nonimmediate_operand" "%<f0>,   0,<v0>")
+		 (match_operand:FP 2 "general_operand"          "f,<Rf>,<vf>")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_HARD_FLOAT"
   "@
    a<xde><bt>r\t%0,<op1>%2
-   a<xde>b\t%0,%2"
-  [(set_attr "op_type"  "<RRer>,RXE")
-   (set_attr "type"     "fsimp<mode>")])
+   a<xde>b\t%0,%2
+   wfadb\t%v0,%v1,%v2"
+  [(set_attr "op_type"      "<RRer>,RXE,VRR")
+   (set_attr "type"         "fsimp<mode>")
+   (set_attr "cpu_facility" "*,*,vec")])
 
 ; axbr, adbr, aebr, axb, adb, aeb, adtr, axtr
 (define_insn "*add<mode>3_cc"
@@ -5026,10 +5365,29 @@
 ; subti3 instruction pattern(s).
 ;
 
-(define_insn_and_split "subti3"
-  [(set (match_operand:TI 0 "register_operand" "=&d")
-        (minus:TI (match_operand:TI 1 "register_operand" "0")
-                  (match_operand:TI 2 "general_operand" "do") ) )
+(define_expand "subti3"
+  [(parallel
+    [(set (match_operand:TI           0 "register_operand" "")
+	  (minus:TI (match_operand:TI 1 "register_operand" "")
+		    (match_operand:TI 2 "general_operand"  "") ) )
+     (clobber (reg:CC CC_REGNUM))])]
+  "TARGET_ZARCH"
+{
+  /* For z13 we have vaq which doesn't set CC.  */
+  if (TARGET_VX)
+    {
+      emit_insn (gen_rtx_SET (VOIDmode, operands[0],
+			      gen_rtx_MINUS (TImode,
+                                            operands[1],
+                                            copy_to_mode_reg (TImode, operands[2]))));
+      DONE;
+    }
+})
+
+(define_insn_and_split "*subti3"
+  [(set (match_operand:TI           0 "register_operand" "=&d")
+        (minus:TI (match_operand:TI 1 "register_operand"   "0")
+                  (match_operand:TI 2 "general_operand"   "do") ) )
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_ZARCH"
   "#"
@@ -5048,7 +5406,9 @@
    operands[5] = operand_subword (operands[2], 0, 0, TImode);
    operands[6] = operand_subword (operands[0], 1, 0, TImode);
    operands[7] = operand_subword (operands[1], 1, 0, TImode);
-   operands[8] = operand_subword (operands[2], 1, 0, TImode);")
+   operands[8] = operand_subword (operands[2], 1, 0, TImode);"
+  [(set_attr "op_type"      "*")
+   (set_attr "cpu_facility" "*")])
 
 ;
 ; subdi3 instruction pattern(s).
@@ -5327,16 +5687,18 @@
 
 ; sxbr, sdbr, sebr, sdb, seb, sxtr, sdtr
 (define_insn "sub<mode>3"
-  [(set (match_operand:FP 0 "register_operand"            "=f,  f")
-        (minus:FP (match_operand:FP 1 "register_operand" "<f0>,0")
-                  (match_operand:FP 2 "general_operand"  "f,<Rf>")))
+  [(set (match_operand:FP           0 "register_operand"   "=f,   f,<vf>")
+        (minus:FP (match_operand:FP 1 "register_operand" "<f0>,   0,<v0>")
+                  (match_operand:FP 2 "general_operand"     "f,<Rf>,<vf>")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_HARD_FLOAT"
   "@
    s<xde><bt>r\t%0,<op1>%2
-   s<xde>b\t%0,%2"
-  [(set_attr "op_type"  "<RRer>,RXE")
-   (set_attr "type"     "fsimp<mode>")])
+   s<xde>b\t%0,%2
+   wfsdb\t%v0,%v1,%v2"
+  [(set_attr "op_type"      "<RRer>,RXE,VRR")
+   (set_attr "type"         "fsimp<mode>")
+   (set_attr "cpu_facility" "*,*,vec")])
 
 ; sxbr, sdbr, sebr, sdb, seb, sxtr, sdtr
 (define_insn "*sub<mode>3_cc"
@@ -5742,41 +6104,47 @@
 
 ; mxbr, mdbr, meebr, mxb, mxb, meeb, mdtr, mxtr
 (define_insn "mul<mode>3"
-  [(set (match_operand:FP 0 "register_operand"              "=f,f")
-        (mult:FP (match_operand:FP 1 "nonimmediate_operand" "%<f0>,0")
-                 (match_operand:FP 2 "general_operand"      "f,<Rf>")))]
+  [(set (match_operand:FP          0 "register_operand"        "=f,   f,<vf>")
+        (mult:FP (match_operand:FP 1 "nonimmediate_operand" "%<f0>,   0,<v0>")
+                 (match_operand:FP 2 "general_operand"          "f,<Rf>,<vf>")))]
   "TARGET_HARD_FLOAT"
   "@
    m<xdee><bt>r\t%0,<op1>%2
-   m<xdee>b\t%0,%2"
-  [(set_attr "op_type"  "<RRer>,RXE")
-   (set_attr "type"     "fmul<mode>")])
+   m<xdee>b\t%0,%2
+   wfmdb\t%v0,%v1,%v2"
+  [(set_attr "op_type"      "<RRer>,RXE,VRR")
+   (set_attr "type"         "fmul<mode>")
+   (set_attr "cpu_facility" "*,*,vec")])
 
 ; madbr, maebr, maxb, madb, maeb
 (define_insn "fma<mode>4"
-  [(set (match_operand:DSF 0 "register_operand" "=f,f")
-	(fma:DSF (match_operand:DSF 1 "nonimmediate_operand" "%f,f")
-		 (match_operand:DSF 2 "nonimmediate_operand" "f,R")
-		 (match_operand:DSF 3 "register_operand" "0,0")))]
+  [(set (match_operand:DSF          0 "register_operand"     "=f,f,<vf>")
+	(fma:DSF (match_operand:DSF 1 "nonimmediate_operand" "%f,f,<vf>")
+		 (match_operand:DSF 2 "nonimmediate_operand"  "f,R,<vf>")
+		 (match_operand:DSF 3 "register_operand"      "0,0,<v0>")))]
   "TARGET_HARD_FLOAT"
   "@
    ma<xde>br\t%0,%1,%2
-   ma<xde>b\t%0,%1,%2"
-  [(set_attr "op_type"  "RRE,RXE")
-   (set_attr "type"     "fmadd<mode>")])
+   ma<xde>b\t%0,%1,%2
+   wfmadb\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type"      "RRE,RXE,VRR")
+   (set_attr "type"         "fmadd<mode>")
+   (set_attr "cpu_facility" "*,*,vec")])
 
 ; msxbr, msdbr, msebr, msxb, msdb, mseb
 (define_insn "fms<mode>4"
-  [(set (match_operand:DSF 0 "register_operand" "=f,f")
-	(fma:DSF (match_operand:DSF 1 "nonimmediate_operand" "%f,f")
-		 (match_operand:DSF 2 "nonimmediate_operand" "f,R")
-		 (neg:DSF (match_operand:DSF 3 "register_operand" "0,0"))))]
+  [(set (match_operand:DSF                   0 "register_operand"     "=f,f,<vf>")
+	(fma:DSF (match_operand:DSF          1 "nonimmediate_operand" "%f,f,<vf>")
+		 (match_operand:DSF          2 "nonimmediate_operand"  "f,R,<vf>")
+		 (neg:DSF (match_operand:DSF 3 "register_operand"      "0,0,<v0>"))))]
   "TARGET_HARD_FLOAT"
   "@
    ms<xde>br\t%0,%1,%2
-   ms<xde>b\t%0,%1,%2"
-  [(set_attr "op_type"  "RRE,RXE")
-   (set_attr "type"     "fmadd<mode>")])
+   ms<xde>b\t%0,%1,%2
+   wfmsdb\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type"      "RRE,RXE,VRR")
+   (set_attr "type"         "fmadd<mode>")
+   (set_attr "cpu_facility" "*,*,vec")])
 
 ;;
 ;;- Divide and modulo instructions.
@@ -6202,15 +6570,17 @@
 
 ; dxbr, ddbr, debr, dxb, ddb, deb, ddtr, dxtr
 (define_insn "div<mode>3"
-  [(set (match_operand:FP 0 "register_operand"          "=f,f")
-        (div:FP (match_operand:FP 1 "register_operand" "<f0>,0")
-                 (match_operand:FP 2 "general_operand"  "f,<Rf>")))]
+  [(set (match_operand:FP         0 "register_operand"   "=f,   f,<vf>")
+        (div:FP (match_operand:FP 1 "register_operand" "<f0>,   0,<v0>")
+		(match_operand:FP 2 "general_operand"     "f,<Rf>,<vf>")))]
   "TARGET_HARD_FLOAT"
   "@
    d<xde><bt>r\t%0,<op1>%2
-   d<xde>b\t%0,%2"
-  [(set_attr "op_type"  "<RRer>,RXE")
-   (set_attr "type"     "fdiv<mode>")])
+   d<xde>b\t%0,%2
+   wfddb\t%v0,%v1,%v2"
+  [(set_attr "op_type"      "<RRer>,RXE,VRR")
+   (set_attr "type"         "fdiv<mode>")
+   (set_attr "cpu_facility" "*,*,vec")])
 
 
 ;;
@@ -7356,14 +7726,18 @@
    (set_attr "type"     "fsimp<mode>")])
 
 ; lcxbr, lcdbr, lcebr
+; FIXME: wflcdb does not clobber cc
 (define_insn "*neg<mode>2"
-  [(set (match_operand:BFP 0 "register_operand" "=f")
-        (neg:BFP (match_operand:BFP 1 "register_operand" "f")))
+  [(set (match_operand:BFP          0 "register_operand" "=f,<vf>")
+        (neg:BFP (match_operand:BFP 1 "register_operand"  "f,<vf>")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_HARD_FLOAT"
-  "lc<xde>br\t%0,%1"
-  [(set_attr "op_type"  "RRE")
-   (set_attr "type"     "fsimp<mode>")])
+  "@
+   lc<xde>br\t%0,%1
+   wflcdb\t%0,%1"
+  [(set_attr "op_type"      "RRE,VRR")
+   (set_attr "cpu_facility" "*,vec")
+   (set_attr "type"         "fsimp<mode>,*")])
 
 
 ;;
@@ -7474,14 +7848,18 @@
    (set_attr "type"     "fsimp<mode>")])
 
 ; lpxbr, lpdbr, lpebr
+; FIXME: wflpdb does not clobber cc
 (define_insn "*abs<mode>2"
-  [(set (match_operand:BFP 0 "register_operand" "=f")
-        (abs:BFP (match_operand:BFP 1 "register_operand" "f")))
+  [(set (match_operand:BFP          0 "register_operand" "=f,<vf>")
+        (abs:BFP (match_operand:BFP 1 "register_operand"  "f,<vf>")))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_HARD_FLOAT"
-  "lp<xde>br\t%0,%1"
-  [(set_attr "op_type"  "RRE")
-   (set_attr "type"     "fsimp<mode>")])
+  "@
+    lp<xde>br\t%0,%1
+    wflpdb\t%0,%1"
+  [(set_attr "op_type"      "RRE,VRR")
+   (set_attr "cpu_facility" "*,vec")
+   (set_attr "type"         "fsimp<mode>,*")])
 
 
 ;;
@@ -7585,14 +7963,18 @@
    (set_attr "type"     "fsimp<mode>")])
 
 ; lnxbr, lndbr, lnebr
+; FIXME: wflndb does not clobber cc
 (define_insn "*negabs<mode>2"
-  [(set (match_operand:BFP 0 "register_operand" "=f")
-        (neg:BFP (abs:BFP (match_operand:BFP 1 "register_operand" "f"))))
+  [(set (match_operand:BFP                   0 "register_operand" "=f,<vf>")
+        (neg:BFP (abs:BFP (match_operand:BFP 1 "register_operand"  "f,<vf>"))))
    (clobber (reg:CC CC_REGNUM))]
   "TARGET_HARD_FLOAT"
-  "ln<xde>br\t%0,%1"
-  [(set_attr "op_type"  "RRE")
-   (set_attr "type"     "fsimp<mode>")])
+  "@
+   ln<xde>br\t%0,%1
+   wflndb\t%0,%1"
+  [(set_attr "op_type"      "RRE,VRR")
+   (set_attr "cpu_facility" "*,vec")
+   (set_attr "type"         "fsimp<mode>,*")])
 
 ;;
 ;;- Square root instructions.
@@ -7604,14 +7986,16 @@
 
 ; sqxbr, sqdbr, sqebr, sqdb, sqeb
 (define_insn "sqrt<mode>2"
-  [(set (match_operand:BFP 0 "register_operand" "=f,f")
-	(sqrt:BFP (match_operand:BFP 1 "general_operand" "f,<Rf>")))]
+  [(set (match_operand:BFP           0 "register_operand" "=f,   f,<vf>")
+	(sqrt:BFP (match_operand:BFP 1 "general_operand"   "f,<Rf>,<vf>")))]
   "TARGET_HARD_FLOAT"
   "@
    sq<xde>br\t%0,%1
-   sq<xde>b\t%0,%1"
-  [(set_attr "op_type" "RRE,RXE")
-   (set_attr "type" "fsqrt<mode>")])
+   sq<xde>b\t%0,%1
+   wfsqdb\t%v0,%v1"
+  [(set_attr "op_type"      "RRE,RXE,VRR")
+   (set_attr "type"         "fsqrt<mode>")
+   (set_attr "cpu_facility" "*,*,vec")])
 
 
 ;;
@@ -10006,6 +10390,35 @@
   DONE;
 })
 
+; Clobber VRs since they don't get restored
+(define_insn "tbegin_1_z13"
+  [(set (reg:CCRAW CC_REGNUM)
+	(unspec_volatile:CCRAW [(match_operand 0 "const_int_operand" "D")]
+			       UNSPECV_TBEGIN))
+   (set (match_operand:BLK 1 "memory_operand" "=Q")
+	(unspec_volatile:BLK [(match_dup 0)] UNSPECV_TBEGIN_TDB))
+   (clobber (reg:TI 16)) (clobber (reg:TI 38))
+   (clobber (reg:TI 17)) (clobber (reg:TI 39))
+   (clobber (reg:TI 18)) (clobber (reg:TI 40))
+   (clobber (reg:TI 19)) (clobber (reg:TI 41))
+   (clobber (reg:TI 20)) (clobber (reg:TI 42))
+   (clobber (reg:TI 21)) (clobber (reg:TI 43))
+   (clobber (reg:TI 22)) (clobber (reg:TI 44))
+   (clobber (reg:TI 23)) (clobber (reg:TI 45))
+   (clobber (reg:TI 24)) (clobber (reg:TI 46))
+   (clobber (reg:TI 25)) (clobber (reg:TI 47))
+   (clobber (reg:TI 26)) (clobber (reg:TI 48))
+   (clobber (reg:TI 27)) (clobber (reg:TI 49))
+   (clobber (reg:TI 28)) (clobber (reg:TI 50))
+   (clobber (reg:TI 29)) (clobber (reg:TI 51))
+   (clobber (reg:TI 30)) (clobber (reg:TI 52))
+   (clobber (reg:TI 31)) (clobber (reg:TI 53))]
+; CONST_OK_FOR_CONSTRAINT_P does not work with D constraint since D is
+; not supposed to be used for immediates (see genpreds.c).
+  "TARGET_VX && INTVAL (operands[0]) >= 0 && INTVAL (operands[0]) <= 0xffff"
+  "tbegin\t%1,%x0"
+  [(set_attr "op_type" "SIL")])
+
 (define_insn "tbegin_1"
   [(set (reg:CCRAW CC_REGNUM)
 	(unspec_volatile:CCRAW [(match_operand 0 "const_int_operand" "D")]
@@ -10141,3 +10554,30 @@
   "TARGET_HTM && INTVAL (operands[2]) < 16"
   "ppa\t%0,%1,%2"
   [(set_attr "op_type" "RRF")])
+
+
+; Set and get floating point control register
+
+(define_insn "sfpc"
+  [(unspec_volatile [(match_operand:SI 0 "register_operand" "d")]
+		    UNSPECV_SFPC)]
+  "TARGET_HARD_FLOAT"
+  "sfpc\t%0")
+
+(define_insn "efpc"
+  [(set (match_operand:SI 0 "register_operand" "=d")
+	(unspec_volatile:SI [(const_int 0)] UNSPECV_EFPC))]
+  "TARGET_HARD_FLOAT"
+  "efpc\t%0")
+
+
+; Load count to block boundary
+
+(define_insn "lcbb"
+  [(set (match_operand:SI             0 "register_operand"  "=d")
+	(unspec:SI [(match_operand    1 "address_operand" "ZQZR")
+		    (match_operand:SI 2 "immediate_operand"  "C")] UNSPEC_LCBB))
+   (clobber (reg:CC CC_REGNUM))]
+  "TARGET_Z13"
+  "lcbb\t%0,%a1,%b2"
+  [(set_attr "op_type" "VRX")])
--- gcc/config/s390/s390-modes.def	2013-08-14 13:55:12.000000000 +0200
+++ gcc/config/s390/s390-modes.def	2016-05-11 17:12:39.000000000 +0200
@@ -84,6 +84,23 @@ Requested mode            -> Destination
 CCS, CCU, CCT, CCSR, CCUR -> CCZ
 CCA                       -> CCAP, CCAN
 
+Vector comparison modes
+
+CCVEQ  	  EQ	  - 	       - 	   NE	      (VCEQ)
+CCVEQANY  EQ	  EQ	       - 	   NE	      (VCEQ)
+
+CCVH	  GT	  - 	       - 	   LE	      (VCH)
+CCVHANY	  GT	  GT	       - 	   LE	      (VCH)
+CCVHU	  GTU	  -  	       -  	   LEU	      (VCHL)
+CCVHUANY  GTU	  GTU	       -  	   LEU	      (VCHL)
+
+CCVFH	  GT	  -   	       -   	   UNLE	      (VFCH)
+CCVFHANY  GT	  GT	       -   	   UNLE	      (VFCH)
+CCVFHE	  GE	  -   	       -   	   UNLT	      (VFCHE)
+CCVFHEANY GE	  GE	       -   	   UNLT	      (VFCHE)
+
+
+
 
 *** Comments ***
 
@@ -152,6 +169,15 @@ The compare and swap instructions sets t
 operands were equal/unequal. The CCZ1 mode ensures the result can be
 effectively placed into a register.
 
+
+CCV*
+
+The variants with and without ANY are generated by the same
+instructions and therefore are holding the same information.  However,
+when generating a condition code mask they require checking different
+bits of CC.  In that case the variants without ANY represent the
+results for *all* elements.
+
 CCRAW
 
 The cc mode generated by a non-compare instruction.  The condition
@@ -181,3 +207,38 @@ CC_MODE (CCT1);
 CC_MODE (CCT2);
 CC_MODE (CCT3);
 CC_MODE (CCRAW);
+
+CC_MODE (CCVEQ);
+CC_MODE (CCVEQANY);
+
+CC_MODE (CCVH);
+CC_MODE (CCVHANY);
+CC_MODE (CCVHU);
+CC_MODE (CCVHUANY);
+
+CC_MODE (CCVFH);
+CC_MODE (CCVFHANY);
+CC_MODE (CCVFHE);
+CC_MODE (CCVFHEANY);
+
+
+/* Vector modes.  */
+
+VECTOR_MODES (INT, 2);        /*                 V2QI */
+VECTOR_MODES (INT, 4);        /*            V4QI V2HI */
+VECTOR_MODES (INT, 8);        /*       V8QI V4HI V2SI */
+VECTOR_MODES (INT, 16);       /* V16QI V8HI V4SI V2DI */
+
+VECTOR_MODE (FLOAT, SF, 2);   /* V2SF */
+VECTOR_MODE (FLOAT, SF, 4);   /* V4SF */
+VECTOR_MODE (FLOAT, DF, 2);   /* V2DF */
+
+VECTOR_MODE (INT, QI, 1);     /* V1QI */
+VECTOR_MODE (INT, HI, 1);     /* V1HI */
+VECTOR_MODE (INT, SI, 1);     /* V1SI */
+VECTOR_MODE (INT, DI, 1);     /* V1DI */
+VECTOR_MODE (INT, TI, 1);     /* V1TI */
+
+VECTOR_MODE (FLOAT, SF, 1);   /* V1SF */
+VECTOR_MODE (FLOAT, DF, 1);   /* V1DF */
+VECTOR_MODE (FLOAT, TF, 1);   /* V1TF */
--- gcc/config/s390/s390.opt	2015-06-18 16:33:05.000000000 +0200
+++ gcc/config/s390/s390.opt	2016-05-11 17:33:59.000000000 +0200
@@ -76,6 +76,9 @@ Enum(processor_type) String(z196) Value(
 EnumValue
 Enum(processor_type) String(zEC12) Value(PROCESSOR_2827_ZEC12)
 
+EnumValue
+Enum(processor_type) String(z13) Value(PROCESSOR_2964_Z13)
+
 mbackchain
 Target Report Mask(BACKCHAIN)
 Maintain backchain pointer
@@ -118,6 +121,10 @@ mhtm
 Target Report Mask(OPT_HTM)
 Use hardware transactional execution instructions
 
+mvx
+Target Report Mask(OPT_VX)
+Use hardware vector facility instructions and enable the vector ABI
+
 mpacked-stack
 Target Report Mask(PACKED_STACK)
 Use packed stack layout
@@ -146,6 +153,11 @@ mmvcle
 Target Report Mask(MVCLE)
 mvcle use
 
+mzvector
+Target Report Mask(ZVECTOR)
+Enable the z vector language extension providing the context-sensitive
+vector macro and enable the Altivec-style builtins in vecintrin.h
+
 mwarn-dynamicstack
 Target RejectNegative Var(s390_warn_dynamicstack_p)
 Warn if a function uses alloca or creates an array with dynamic size
--- gcc/config/s390/s390-opts.h	2013-01-21 16:11:50.000000000 +0100
+++ gcc/config/s390/s390-opts.h	2016-05-11 15:53:24.000000000 +0200
@@ -35,6 +35,7 @@ enum processor_type
   PROCESSOR_2097_Z10,
   PROCESSOR_2817_Z196,
   PROCESSOR_2827_ZEC12,
+  PROCESSOR_2964_Z13,
   PROCESSOR_max
 };
 
--- gcc/config/s390/s390-protos.h	2014-01-14 16:37:04.000000000 +0100
+++ gcc/config/s390/s390-protos.h	2016-05-11 19:28:17.220349132 +0200
@@ -41,6 +41,9 @@ extern void s390_set_has_landing_pad_p (
 extern bool s390_hard_regno_mode_ok (unsigned int, enum machine_mode);
 extern bool s390_hard_regno_rename_ok (unsigned int, unsigned int);
 extern int s390_class_max_nregs (enum reg_class, enum machine_mode);
+extern int s390_cannot_change_mode_class (enum machine_mode, enum machine_mode,
+					  enum reg_class);
+extern bool s390_function_arg_vector (enum machine_mode, const_tree);
 
 #ifdef RTX_CODE
 extern int s390_extra_constraint_str (rtx, int, const char *);
@@ -49,6 +52,9 @@ extern int s390_const_double_ok_for_cons
 extern int s390_single_part (rtx, enum machine_mode, enum machine_mode, int);
 extern unsigned HOST_WIDE_INT s390_extract_part (rtx, enum machine_mode, int);
 extern bool s390_contiguous_bitmask_p (unsigned HOST_WIDE_INT, int, int *, int *);
+extern bool s390_const_vec_duplicate_p (rtx);
+extern bool s390_contiguous_bitmask_vector_p (rtx, int *, int *);
+extern bool s390_bytemask_vector_p (rtx, unsigned *);
 extern bool s390_split_ok_p (rtx, rtx, enum machine_mode, int);
 extern bool s390_overlap_p (rtx, rtx, HOST_WIDE_INT);
 extern bool s390_offset_p (rtx, rtx, rtx);
@@ -81,6 +87,8 @@ extern void s390_load_address (rtx, rtx)
 extern bool s390_expand_movmem (rtx, rtx, rtx);
 extern void s390_expand_setmem (rtx, rtx, rtx);
 extern bool s390_expand_cmpmem (rtx, rtx, rtx, rtx);
+extern void s390_expand_vec_strlen (rtx, rtx, rtx);
+extern void s390_expand_vec_movstr (rtx, rtx, rtx);
 extern bool s390_expand_addcc (enum rtx_code, rtx, rtx, rtx, rtx, rtx);
 extern bool s390_expand_insv (rtx, rtx, rtx, rtx);
 extern void s390_expand_cs_hqi (enum machine_mode, rtx, rtx, rtx,
@@ -88,6 +96,10 @@ extern void s390_expand_cs_hqi (enum mac
 extern void s390_expand_atomic (enum machine_mode, enum rtx_code,
 				rtx, rtx, rtx, bool);
 extern void s390_expand_tbegin (rtx, rtx, rtx, bool);
+extern void s390_expand_vec_compare (rtx, enum rtx_code, rtx, rtx);
+extern void s390_expand_vec_compare_cc (rtx, enum rtx_code, rtx, rtx, bool);
+extern void s390_expand_vcond (rtx, rtx, rtx, enum rtx_code, rtx, rtx);
+extern void s390_expand_vec_init (rtx, rtx);
 extern rtx s390_return_addr_rtx (int, rtx);
 extern rtx s390_back_chain_rtx (void);
 extern rtx s390_emit_call (rtx, rtx, rtx, rtx);
@@ -113,3 +125,10 @@ extern bool s390_extzv_shift_ok (int, in
 extern void s390_asm_output_function_label (FILE *, const char *, tree);
 
 #endif /* RTX_CODE */
+
+/* s390-c.c routines */
+extern void s390_cpu_cpp_builtins (struct cpp_reader *);
+extern void s390_register_target_pragmas (void);
+
+/* Routines for s390-c.c */
+extern bool s390_const_operand_ok (tree, int, int, tree);
--- gcc/config/s390/t-s390	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/t-s390	2016-05-11 17:12:39.000000000 +0200
@@ -0,0 +1,27 @@
+# Copyright (C) 2015 Free Software Foundation, Inc.
+#
+# This file is part of GCC.
+#
+# GCC is free software; you can redistribute it and/or modify
+# it under the terms of the GNU General Public License as published by
+# the Free Software Foundation; either version 3, or (at your option)
+# any later version.
+#
+# GCC is distributed in the hope that it will be useful,
+# but WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+# GNU General Public License for more details.
+#
+# You should have received a copy of the GNU General Public License
+# along with GCC; see the file COPYING3.  If not see
+# <http://www.gnu.org/licenses/>.
+
+TM_H += $(srcdir)/config/s390/s390-builtins.def
+TM_H += $(srcdir)/config/s390/s390-builtin-types.def
+
+s390-c.o: $(srcdir)/config/s390/s390-c.c \
+  $(srcdir)/config/s390/s390-protos.h $(CONFIG_H) $(SYSTEM_H) coretypes.h \
+  $(TM_H) $(TREE_H) $(TM_P_H) $(FLAGS_H) $(C_COMMON_H) $(GGC_H) \
+  $(TARGET_H) $(TARGET_DEF_H) $(CPPLIB_H) $(C_PRAGMA_H)
+	$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \
+		$(srcdir)/config/s390/s390-c.c
--- gcc/config/s390/vecintrin.h	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/vecintrin.h	2016-05-11 18:10:53.000000000 +0200
@@ -0,0 +1,277 @@
+/* GNU compiler hardware transactional execution intrinsics
+   Copyright (C) 2015 Free Software Foundation, Inc.
+   Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com)
+
+This file is part of GCC.
+
+GCC is free software; you can redistribute it and/or modify it under
+the terms of the GNU General Public License as published by the Free
+Software Foundation; either version 3, or (at your option) any later
+version.
+
+GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+WARRANTY; without even the implied warranty of MERCHANTABILITY or
+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+for more details.
+
+You should have received a copy of the GNU General Public License
+along with GCC; see the file COPYING3.  If not see
+<http://www.gnu.org/licenses/>.  */
+
+#ifndef _VECINTRIN_H
+#define _VECINTRIN_H
+
+#ifdef __VEC__
+
+#define __VFTCI_ZERO           1<<11
+#define __VFTCI_ZERO_N         1<<10
+#define __VFTCI_NORMAL          1<<9
+#define __VFTCI_NORMAL_N        1<<8
+#define __VFTCI_SUBNORMAL       1<<7
+#define __VFTCI_SUBNORMAL_N     1<<6
+#define __VFTCI_INF             1<<5
+#define __VFTCI_INF_N           1<<4
+#define __VFTCI_QNAN            1<<3
+#define __VFTCI_QNAN_N          1<<2
+#define __VFTCI_SNAN            1<<1
+#define __VFTCI_SNAN_N          1<<0
+
+/* This also accepts a type for its parameter, so it is not enough
+   to #define vec_step to __builtin_vec_step.  */
+#define vec_step(x) __builtin_vec_step (* (__typeof__ (x) *) 0)
+
+static inline int
+__lcbb(const void *ptr, int bndry)
+{
+  int code;
+  switch (bndry)
+    {
+    case 64: code = 0; break;
+    case 128: code = 1; break;
+    case 256: code = 2; break;
+    case 512: code = 3; break;
+    case 1024: code = 4; break;
+    case 2048: code = 5; break;
+    case 4096: code = 6; break;
+    default: return 0;
+    }
+  return __builtin_s390_lcbb (ptr, code);
+}
+
+#define vec_all_nle(X, Y) vec_all_nge ((Y), (X))
+#define vec_all_nlt(X, Y) vec_all_ngt ((Y), (X))
+#define vec_any_nle(X, Y) vec_any_nge ((Y), (X))
+#define vec_any_nlt(X, Y) vec_any_ngt ((Y), (X))
+#define vec_genmask __builtin_s390_vgbm
+#define vec_genmasks_8 __builtin_s390_vgmb
+#define vec_genmasks_16 __builtin_s390_vgmh
+#define vec_genmasks_32 __builtin_s390_vgmf
+#define vec_genmasks_64 __builtin_s390_vgmg
+#define vec_splat_u8 __builtin_s390_vec_splat_u8
+#define vec_splat_s8 __builtin_s390_vec_splat_s8
+#define vec_splat_u16 __builtin_s390_vec_splat_u16
+#define vec_splat_s16 __builtin_s390_vec_splat_s16
+#define vec_splat_u32 __builtin_s390_vec_splat_u32
+#define vec_splat_s32 __builtin_s390_vec_splat_s32
+#define vec_splat_u64 __builtin_s390_vec_splat_u64
+#define vec_splat_s64 __builtin_s390_vec_splat_s64
+#define vec_add_u128 __builtin_s390_vaq
+#define vec_addc_u128 __builtin_s390_vaccq
+#define vec_adde_u128 __builtin_s390_vacq
+#define vec_addec_u128 __builtin_s390_vacccq
+#define vec_checksum __builtin_s390_vcksm
+#define vec_gfmsum_128 __builtin_s390_vgfmg
+#define vec_gfmsum_accum_128 __builtin_s390_vgfmag
+#define vec_sub_u128 __builtin_s390_vsq
+#define vec_subc_u128 __builtin_s390_vscbiq
+#define vec_sube_u128 __builtin_s390_vsbiq
+#define vec_subec_u128 __builtin_s390_vsbcbiq
+#define vec_ceil(X) __builtin_s390_vfidb((X), 4, 6)
+#define vec_roundp(X) __builtin_s390_vfidb((X), 4, 6)
+#define vec_floor(X) __builtin_s390_vfidb((X), 4, 7)
+#define vec_roundm(X) __builtin_s390_vfidb((X), 4, 7)
+#define vec_trunc(X) __builtin_s390_vfidb((X), 4, 5)
+#define vec_roundz(X) __builtin_s390_vfidb((X), 4, 5)
+#define vec_roundc(X) __builtin_s390_vfidb((X), 4, 0)
+#define vec_round(X) __builtin_s390_vfidb((X), 4, 4)
+#define vec_madd __builtin_s390_vfmadb
+#define vec_msub __builtin_s390_vfmsdb
+
+static inline int
+vec_all_nan (__vector double a)
+{
+  int cc;
+  __builtin_s390_vftcidb (a,
+			  __VFTCI_QNAN
+			  | __VFTCI_QNAN_N
+			  | __VFTCI_SNAN
+			  | __VFTCI_SNAN_N, &cc);
+  return cc == 0 ? 1 : 0;
+}
+
+static inline int
+vec_all_numeric (__vector double a)
+{
+  int cc;
+  __builtin_s390_vftcidb (a,
+			  __VFTCI_NORMAL
+			  | __VFTCI_NORMAL_N
+			  | __VFTCI_SUBNORMAL
+			  | __VFTCI_SUBNORMAL_N, &cc);
+  return cc == 0 ? 1 : 0;
+}
+
+static inline int
+vec_any_nan (__vector double a)
+{
+  int cc;
+  __builtin_s390_vftcidb (a,
+			  __VFTCI_QNAN
+			  | __VFTCI_QNAN_N
+			  | __VFTCI_SNAN
+			  | __VFTCI_SNAN_N, &cc);
+  return cc != 3 ? 1 : 0;
+}
+
+static inline int
+vec_any_numeric (__vector double a)
+{
+  int cc;
+  __builtin_s390_vftcidb (a,
+			  __VFTCI_NORMAL
+			  | __VFTCI_NORMAL_N
+			  | __VFTCI_SUBNORMAL
+			  | __VFTCI_SUBNORMAL_N, &cc);
+  return cc != 3 ? 1 : 0;
+}
+#define vec_gather_element __builtin_s390_vec_gather_element
+#define vec_xld2 __builtin_s390_vec_xld2
+#define vec_xlw4 __builtin_s390_vec_xlw4
+#define vec_splats __builtin_s390_vec_splats
+#define vec_insert __builtin_s390_vec_insert
+#define vec_promote __builtin_s390_vec_promote
+#define vec_extract __builtin_s390_vec_extract
+#define vec_insert_and_zero __builtin_s390_vec_insert_and_zero
+#define vec_load_bndry __builtin_s390_vec_load_bndry
+#define vec_load_pair __builtin_s390_vec_load_pair
+#define vec_load_len __builtin_s390_vec_load_len
+#define vec_mergeh __builtin_s390_vec_mergeh
+#define vec_mergel __builtin_s390_vec_mergel
+#define vec_pack __builtin_s390_vec_pack
+#define vec_packs __builtin_s390_vec_packs
+#define vec_packs_cc __builtin_s390_vec_packs_cc
+#define vec_packsu __builtin_s390_vec_packsu
+#define vec_packsu_cc __builtin_s390_vec_packsu_cc
+#define vec_perm __builtin_s390_vec_perm
+#define vec_permi __builtin_s390_vec_permi
+#define vec_splat __builtin_s390_vec_splat
+#define vec_scatter_element __builtin_s390_vec_scatter_element
+#define vec_sel __builtin_s390_vec_sel
+#define vec_extend_s64 __builtin_s390_vec_extend_s64
+#define vec_xstd2 __builtin_s390_vec_xstd2
+#define vec_xstw4 __builtin_s390_vec_xstw4
+#define vec_store_len __builtin_s390_vec_store_len
+#define vec_unpackh __builtin_s390_vec_unpackh
+#define vec_unpackl __builtin_s390_vec_unpackl
+#define vec_addc __builtin_s390_vec_addc
+#define vec_and __builtin_s390_vec_and
+#define vec_andc __builtin_s390_vec_andc
+#define vec_avg __builtin_s390_vec_avg
+#define vec_all_eq __builtin_s390_vec_all_eq
+#define vec_all_ne __builtin_s390_vec_all_ne
+#define vec_all_ge __builtin_s390_vec_all_ge
+#define vec_all_gt __builtin_s390_vec_all_gt
+#define vec_all_le __builtin_s390_vec_all_le
+#define vec_all_lt __builtin_s390_vec_all_lt
+#define vec_any_eq __builtin_s390_vec_any_eq
+#define vec_any_ne __builtin_s390_vec_any_ne
+#define vec_any_ge __builtin_s390_vec_any_ge
+#define vec_any_gt __builtin_s390_vec_any_gt
+#define vec_any_le __builtin_s390_vec_any_le
+#define vec_any_lt __builtin_s390_vec_any_lt
+#define vec_cmpeq __builtin_s390_vec_cmpeq
+#define vec_cmpge __builtin_s390_vec_cmpge
+#define vec_cmpgt __builtin_s390_vec_cmpgt
+#define vec_cmple __builtin_s390_vec_cmple
+#define vec_cmplt __builtin_s390_vec_cmplt
+#define vec_cntlz __builtin_s390_vec_cntlz
+#define vec_cnttz __builtin_s390_vec_cnttz
+#define vec_xor __builtin_s390_vec_xor
+#define vec_gfmsum __builtin_s390_vec_gfmsum
+#define vec_gfmsum_accum __builtin_s390_vec_gfmsum_accum
+#define vec_abs __builtin_s390_vec_abs
+#define vec_max __builtin_s390_vec_max
+#define vec_min __builtin_s390_vec_min
+#define vec_mladd __builtin_s390_vec_mladd
+#define vec_mhadd __builtin_s390_vec_mhadd
+#define vec_meadd __builtin_s390_vec_meadd
+#define vec_moadd __builtin_s390_vec_moadd
+#define vec_mulh __builtin_s390_vec_mulh
+#define vec_mule __builtin_s390_vec_mule
+#define vec_mulo __builtin_s390_vec_mulo
+#define vec_nor __builtin_s390_vec_nor
+#define vec_or __builtin_s390_vec_or
+#define vec_popcnt __builtin_s390_vec_popcnt
+#define vec_rl __builtin_s390_vec_rl
+#define vec_rli __builtin_s390_vec_rli
+#define vec_rl_mask __builtin_s390_vec_rl_mask
+#define vec_sll __builtin_s390_vec_sll
+#define vec_slb __builtin_s390_vec_slb
+#define vec_sld __builtin_s390_vec_sld
+#define vec_sldw __builtin_s390_vec_sldw
+#define vec_sral __builtin_s390_vec_sral
+#define vec_srab __builtin_s390_vec_srab
+#define vec_srl __builtin_s390_vec_srl
+#define vec_srb __builtin_s390_vec_srb
+#define vec_subc __builtin_s390_vec_subc
+#define vec_sum2 __builtin_s390_vec_sum2
+#define vec_sum_u128 __builtin_s390_vec_sum_u128
+#define vec_sum4 __builtin_s390_vec_sum4
+#define vec_test_mask __builtin_s390_vec_test_mask
+#define vec_find_any_eq_idx __builtin_s390_vec_find_any_eq_idx
+#define vec_find_any_ne_idx __builtin_s390_vec_find_any_ne_idx
+#define vec_find_any_eq_or_0_idx __builtin_s390_vec_find_any_eq_or_0_idx
+#define vec_find_any_ne_or_0_idx __builtin_s390_vec_find_any_ne_or_0_idx
+#define vec_find_any_eq __builtin_s390_vec_find_any_eq
+#define vec_find_any_ne __builtin_s390_vec_find_any_ne
+#define vec_find_any_eq_idx_cc __builtin_s390_vec_find_any_eq_idx_cc
+#define vec_find_any_ne_idx_cc __builtin_s390_vec_find_any_ne_idx_cc
+#define vec_find_any_eq_or_0_idx_cc __builtin_s390_vec_find_any_eq_or_0_idx_cc
+#define vec_find_any_ne_or_0_idx_cc __builtin_s390_vec_find_any_ne_or_0_idx_cc
+#define vec_find_any_eq_cc __builtin_s390_vec_find_any_eq_cc
+#define vec_find_any_ne_cc __builtin_s390_vec_find_any_ne_cc
+#define vec_cmpeq_idx __builtin_s390_vec_cmpeq_idx
+#define vec_cmpeq_or_0_idx __builtin_s390_vec_cmpeq_or_0_idx
+#define vec_cmpeq_idx_cc __builtin_s390_vec_cmpeq_idx_cc
+#define vec_cmpeq_or_0_idx_cc __builtin_s390_vec_cmpeq_or_0_idx_cc
+#define vec_cmpne_idx __builtin_s390_vec_cmpne_idx
+#define vec_cmpne_or_0_idx __builtin_s390_vec_cmpne_or_0_idx
+#define vec_cmpne_idx_cc __builtin_s390_vec_cmpne_idx_cc
+#define vec_cmpne_or_0_idx_cc __builtin_s390_vec_cmpne_or_0_idx_cc
+#define vec_cp_until_zero __builtin_s390_vec_cp_until_zero
+#define vec_cp_until_zero_cc __builtin_s390_vec_cp_until_zero_cc
+#define vec_cmprg_idx __builtin_s390_vec_cmprg_idx
+#define vec_cmpnrg_idx __builtin_s390_vec_cmpnrg_idx
+#define vec_cmprg_or_0_idx __builtin_s390_vec_cmprg_or_0_idx
+#define vec_cmpnrg_or_0_idx __builtin_s390_vec_cmpnrg_or_0_idx
+#define vec_cmprg __builtin_s390_vec_cmprg
+#define vec_cmpnrg __builtin_s390_vec_cmpnrg
+#define vec_cmprg_idx_cc __builtin_s390_vec_cmprg_idx_cc
+#define vec_cmpnrg_idx_cc __builtin_s390_vec_cmpnrg_idx_cc
+#define vec_cmprg_or_0_idx_cc __builtin_s390_vec_cmprg_or_0_idx_cc
+#define vec_cmpnrg_or_0_idx_cc __builtin_s390_vec_cmpnrg_or_0_idx_cc
+#define vec_cmprg_cc __builtin_s390_vec_cmprg_cc
+#define vec_cmpnrg_cc __builtin_s390_vec_cmpnrg_cc
+#define vec_all_nge __builtin_s390_vec_all_nge
+#define vec_all_ngt __builtin_s390_vec_all_ngt
+#define vec_any_nge __builtin_s390_vec_any_nge
+#define vec_any_ngt __builtin_s390_vec_any_ngt
+#define vec_ctd __builtin_s390_vec_ctd
+#define vec_ctd_s64 __builtin_s390_vec_ctd_s64
+#define vec_ctd_u64 __builtin_s390_vec_ctd_u64
+#define vec_ctsl __builtin_s390_vec_ctsl
+#define vec_ctul __builtin_s390_vec_ctul
+#define vec_ld2f __builtin_s390_vec_ld2f
+#define vec_st2f __builtin_s390_vec_st2f
+#endif /* __VEC__ */
+#endif /* _VECINTRIN_H */
--- gcc/config/s390/vector.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/vector.md	2016-05-11 18:11:04.000000000 +0200
@@ -0,0 +1,1229 @@
+;;- Instruction patterns for the System z vector facility
+;;  Copyright (C) 2015 Free Software Foundation, Inc.
+;;  Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com)
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it under
+;; the terms of the GNU General Public License as published by the Free
+;; Software Foundation; either version 3, or (at your option) any later
+;; version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+;; WARRANTY; without even the implied warranty of MERCHANTABILITY or
+;; FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+;; for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+; All vector modes supported in a vector register
+(define_mode_iterator V
+  [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1SF
+   V2SF V4SF V1DF V2DF])
+(define_mode_iterator VT
+  [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1SF
+   V2SF V4SF V1DF V2DF V1TF V1TI TI])
+
+; All vector modes directly supported by the hardware having full vector reg size
+; V_HW2 is duplicate of V_HW for having two iterators expanding
+; independently e.g. vcond
+(define_mode_iterator V_HW  [V16QI V8HI V4SI V2DI V2DF])
+(define_mode_iterator V_HW2 [V16QI V8HI V4SI V2DI V2DF])
+; Including TI for instructions that support it (va, vn, ...)
+(define_mode_iterator VT_HW [V16QI V8HI V4SI V2DI V2DF V1TI TI])
+
+; All full size integer vector modes supported in a vector register + TImode
+(define_mode_iterator VIT_HW    [V16QI V8HI V4SI V2DI V1TI TI])
+(define_mode_iterator VI_HW     [V16QI V8HI V4SI V2DI])
+(define_mode_iterator VI_HW_QHS [V16QI V8HI V4SI])
+(define_mode_iterator VI_HW_HS  [V8HI V4SI])
+(define_mode_iterator VI_HW_QH  [V16QI V8HI])
+
+; All integer vector modes supported in a vector register + TImode
+(define_mode_iterator VIT [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI V1TI TI])
+(define_mode_iterator VI  [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI V1DI V2DI])
+(define_mode_iterator VI_QHS [V1QI V2QI V4QI V8QI V16QI V1HI V2HI V4HI V8HI V1SI V2SI V4SI])
+
+(define_mode_iterator V_8   [V1QI])
+(define_mode_iterator V_16  [V2QI  V1HI])
+(define_mode_iterator V_32  [V4QI  V2HI V1SI V1SF])
+(define_mode_iterator V_64  [V8QI  V4HI V2SI V2SF V1DI V1DF])
+(define_mode_iterator V_128 [V16QI V8HI V4SI V4SF V2DI V2DF V1TI V1TF])
+
+; A blank for vector modes and a * for TImode.  This is used to hide
+; the TImode expander name in case it is defined already.  See addti3
+; for an example.
+(define_mode_attr ti* [(V1QI "") (V2QI "") (V4QI "") (V8QI "") (V16QI "")
+		       (V1HI "") (V2HI "") (V4HI "") (V8HI "")
+		       (V1SI "") (V2SI "") (V4SI "")
+		       (V1DI "") (V2DI "")
+		       (V1TI "*") (TI "*")])
+
+; The element type of the vector.
+(define_mode_attr non_vec[(V1QI "QI") (V2QI "QI") (V4QI "QI") (V8QI "QI") (V16QI "QI")
+			  (V1HI "HI") (V2HI "HI") (V4HI "HI") (V8HI "HI")
+			  (V1SI "SI") (V2SI "SI") (V4SI "SI")
+			  (V1DI "DI") (V2DI "DI")
+			  (V1TI "TI")
+			  (V1SF "SF") (V2SF "SF") (V4SF "SF")
+			  (V1DF "DF") (V2DF "DF")
+			  (V1TF "TF")])
+
+; The instruction suffix
+(define_mode_attr bhfgq[(V1QI "b") (V2QI "b") (V4QI "b") (V8QI "b") (V16QI "b")
+			(V1HI "h") (V2HI "h") (V4HI "h") (V8HI "h")
+			(V1SI "f") (V2SI "f") (V4SI "f")
+			(V1DI "g") (V2DI "g")
+			(V1TI "q") (TI "q")
+			(V1SF "f") (V2SF "f") (V4SF "f")
+			(V1DF "g") (V2DF "g")
+			(V1TF "q")])
+
+; This is for vmalhw. It gets an 'w' attached to avoid confusion with
+; multiply and add logical high vmalh.
+(define_mode_attr w [(V1QI "")  (V2QI "")  (V4QI "")  (V8QI "") (V16QI "")
+		     (V1HI "w") (V2HI "w") (V4HI "w") (V8HI "w")
+		     (V1SI "")  (V2SI "")  (V4SI "")
+		     (V1DI "")  (V2DI "")])
+
+; Resulting mode of a vector comparison.  For floating point modes an
+; integer vector mode with the same element size is picked.
+(define_mode_attr tointvec [(V1QI "V1QI") (V2QI "V2QI") (V4QI "V4QI") (V8QI "V8QI") (V16QI "V16QI")
+			    (V1HI "V1HI") (V2HI "V2HI") (V4HI "V4HI") (V8HI "V8HI")
+			    (V1SI "V1SI") (V2SI "V2SI") (V4SI "V4SI")
+			    (V1DI "V1DI") (V2DI "V2DI")
+			    (V1TI "V1TI")
+			    (V1SF "V1SI") (V2SF "V2SI") (V4SF "V4SI")
+			    (V1DF "V1DI") (V2DF "V2DI")
+			    (V1TF "V1TI")])
+
+; Vector with doubled element size.
+(define_mode_attr vec_double [(V1QI "V1HI") (V2QI "V1HI") (V4QI "V2HI") (V8QI "V4HI") (V16QI "V8HI")
+			      (V1HI "V1SI") (V2HI "V1SI") (V4HI "V2SI") (V8HI "V4SI")
+			      (V1SI "V1DI") (V2SI "V1DI") (V4SI "V2DI")
+			      (V1DI "V1TI") (V2DI "V1TI")
+			      (V1SF "V1DF") (V2SF "V1DF") (V4SF "V2DF")])
+
+; Vector with half the element size.
+(define_mode_attr vec_half [(V1HI "V2QI") (V2HI "V4QI") (V4HI "V8QI") (V8HI "V16QI")
+			    (V1SI "V2HI") (V2SI "V4HI") (V4SI "V8HI")
+			    (V1DI "V2SI") (V2DI "V4SI")
+			    (V1TI "V2DI")
+			    (V1DF "V2SF") (V2DF "V4SF")
+			    (V1TF "V1DF")])
+
+; The comparisons not setting CC iterate over the rtx code.
+(define_code_iterator VFCMP_HW_OP [eq gt ge])
+(define_code_attr asm_fcmp_op [(eq "e") (gt "h") (ge "he")])
+
+
+
+; Comparison operators on int and fp compares which are directly
+; supported by the HW.
+(define_code_iterator VICMP_HW_OP [eq gt gtu])
+; For int insn_cmp_op can be used in the insn name as well as in the asm output.
+(define_code_attr insn_cmp_op [(eq "eq") (gt "h") (gtu "hl") (ge "he")])
+
+; Flags for vector string instructions (vfae all 4, vfee only ZS and CS, vstrc all 4)
+(define_constants
+  [(VSTRING_FLAG_IN         8)   ; invert result
+   (VSTRING_FLAG_RT         4)   ; result type
+   (VSTRING_FLAG_ZS         2)   ; zero search
+   (VSTRING_FLAG_CS         1)]) ; condition code set
+
+(include "vx-builtins.md")
+
+; Full HW vector size moves
+(define_insn "mov<mode>"
+  [(set (match_operand:V_128 0 "nonimmediate_operand" "=v, v,QR,  v,  v,  v,  v,  v,v,d")
+	(match_operand:V_128 1 "general_operand"      " v,QR, v,j00,jm1,jyy,jxx,jKK,d,v"))]
+  "TARGET_VX"
+  "@
+   vlr\t%v0,%v1
+   vl\t%v0,%1
+   vst\t%v1,%0
+   vzero\t%v0
+   vone\t%v0
+   vgbm\t%v0,%t1
+   vgm<bhfgq>\t%v0,%s1,%e1
+   vrepi<bhfgq>\t%v0,%h1
+   vlvgp\t%v0,%1,%N1
+   #"
+  [(set_attr "op_type" "VRR,VRX,VRX,VRI,VRI,VRI,VRI,VRI,VRR,*")])
+
+(define_split
+  [(set (match_operand:V_128 0 "register_operand" "")
+	(match_operand:V_128 1 "register_operand" ""))]
+  "TARGET_VX && GENERAL_REG_P (operands[0]) && VECTOR_REG_P (operands[1])"
+  [(set (match_dup 2)
+	(unspec:DI [(subreg:V2DI (match_dup 1) 0)
+		    (const_int 0)] UNSPEC_VEC_EXTRACT))
+   (set (match_dup 3)
+	(unspec:DI [(subreg:V2DI (match_dup 1) 0)
+		    (const_int 1)] UNSPEC_VEC_EXTRACT))]
+{
+  operands[2] = operand_subword (operands[0], 0, 0, <MODE>mode);
+  operands[3] = operand_subword (operands[0], 1, 0, <MODE>mode);
+})
+
+; Moves for smaller vector modes.
+
+; In these patterns only the vlr, vone, and vzero instructions write
+; VR bytes outside the mode.  This should be ok since we disallow
+; formerly bigger modes being accessed with smaller modes via
+; subreg. Note: The vone, vzero instructions could easily be replaced
+; with vlei which would only access the bytes belonging to the mode.
+; However, this would probably be slower.
+
+(define_insn "mov<mode>"
+  [(set (match_operand:V_8 0 "nonimmediate_operand" "=v,v,d, v,QR,  v,  v,  v,  v,d,  Q,  S,  Q,  S,  d,  d,d,d,d,R,T")
+        (match_operand:V_8 1 "general_operand"      " v,d,v,QR, v,j00,jm1,jyy,jxx,d,j00,j00,jm1,jm1,j00,jm1,R,T,b,d,d"))]
+  ""
+  "@
+   vlr\t%v0,%v1
+   vlvgb\t%v0,%1,0
+   vlgvb\t%0,%v1,0
+   vleb\t%v0,%1,0
+   vsteb\t%v1,%0,0
+   vzero\t%v0
+   vone\t%v0
+   vgbm\t%v0,%t1
+   vgm\t%v0,%s1,%e1
+   lr\t%0,%1
+   mvi\t%0,0
+   mviy\t%0,0
+   mvi\t%0,-1
+   mviy\t%0,-1
+   lhi\t%0,0
+   lhi\t%0,-1
+   lh\t%0,%1
+   lhy\t%0,%1
+   lhrl\t%0,%1
+   stc\t%1,%0
+   stcy\t%1,%0"
+  [(set_attr "op_type"      "VRR,VRS,VRS,VRX,VRX,VRI,VRI,VRI,VRI,RR,SI,SIY,SI,SIY,RI,RI,RX,RXY,RIL,RX,RXY")])
+
+(define_insn "mov<mode>"
+  [(set (match_operand:V_16 0 "nonimmediate_operand" "=v,v,d, v,QR,  v,  v,  v,  v,d,  Q,  Q,  d,  d,d,d,d,R,T,b")
+        (match_operand:V_16 1 "general_operand"      " v,d,v,QR, v,j00,jm1,jyy,jxx,d,j00,jm1,j00,jm1,R,T,b,d,d,d"))]
+  ""
+  "@
+   vlr\t%v0,%v1
+   vlvgh\t%v0,%1,0
+   vlgvh\t%0,%v1,0
+   vleh\t%v0,%1,0
+   vsteh\t%v1,%0,0
+   vzero\t%v0
+   vone\t%v0
+   vgbm\t%v0,%t1
+   vgm\t%v0,%s1,%e1
+   lr\t%0,%1
+   mvhhi\t%0,0
+   mvhhi\t%0,-1
+   lhi\t%0,0
+   lhi\t%0,-1
+   lh\t%0,%1
+   lhy\t%0,%1
+   lhrl\t%0,%1
+   sth\t%1,%0
+   sthy\t%1,%0
+   sthrl\t%1,%0"
+  [(set_attr "op_type"      "VRR,VRS,VRS,VRX,VRX,VRI,VRI,VRI,VRI,RR,SIL,SIL,RI,RI,RX,RXY,RIL,RX,RXY,RIL")])
+
+(define_insn "mov<mode>"
+  [(set (match_operand:V_32 0 "nonimmediate_operand" "=f,f,f,R,T,v,v,d, v,QR,  f,  v,  v,  v,  v,  Q,  Q,  d,  d,d,d,d,d,R,T,b")
+	(match_operand:V_32 1 "general_operand"      " f,R,T,f,f,v,d,v,QR, v,j00,j00,jm1,jyy,jxx,j00,jm1,j00,jm1,b,d,R,T,d,d,d"))]
+  "TARGET_VX"
+  "@
+   lder\t%v0,%v1
+   lde\t%0,%1
+   ley\t%0,%1
+   ste\t%1,%0
+   stey\t%1,%0
+   vlr\t%v0,%v1
+   vlvgf\t%v0,%1,0
+   vlgvf\t%0,%v1,0
+   vlef\t%v0,%1,0
+   vstef\t%1,%0,0
+   lzer\t%v0
+   vzero\t%v0
+   vone\t%v0
+   vgbm\t%v0,%t1
+   vgm\t%v0,%s1,%e1
+   mvhi\t%0,0
+   mvhi\t%0,-1
+   lhi\t%0,0
+   lhi\t%0,-1
+   lrl\t%0,%1
+   lr\t%0,%1
+   l\t%0,%1
+   ly\t%0,%1
+   st\t%1,%0
+   sty\t%1,%0
+   strl\t%1,%0"
+  [(set_attr "op_type" "RRE,RXE,RXY,RX,RXY,VRR,VRS,VRS,VRX,VRX,RRE,VRI,VRI,VRI,VRI,SIL,SIL,RI,RI,
+                        RIL,RR,RX,RXY,RX,RXY,RIL")])
+
+(define_insn "mov<mode>"
+  [(set (match_operand:V_64 0 "nonimmediate_operand"
+         "=f,f,f,R,T,v,v,d, v,QR,  f,  v,  v,  v,  v,  Q,  Q,  d,  d,f,d,d,d, d,RT,b")
+        (match_operand:V_64 1 "general_operand"
+         " f,R,T,f,f,v,d,v,QR, v,j00,j00,jm1,jyy,jxx,j00,jm1,j00,jm1,d,f,b,d,RT, d,d"))]
+  "TARGET_ZARCH"
+  "@
+   ldr\t%0,%1
+   ld\t%0,%1
+   ldy\t%0,%1
+   std\t%1,%0
+   stdy\t%1,%0
+   vlr\t%v0,%v1
+   vlvgg\t%v0,%1,0
+   vlgvg\t%0,%v1,0
+   vleg\t%v0,%1,0
+   vsteg\t%v1,%0,0
+   lzdr\t%0
+   vzero\t%v0
+   vone\t%v0
+   vgbm\t%v0,%t1
+   vgm\t%v0,%s1,%e1
+   mvghi\t%0,0
+   mvghi\t%0,-1
+   lghi\t%0,0
+   lghi\t%0,-1
+   ldgr\t%0,%1
+   lgdr\t%0,%1
+   lgrl\t%0,%1
+   lgr\t%0,%1
+   lg\t%0,%1
+   stg\t%1,%0
+   stgrl\t%1,%0"
+  [(set_attr "op_type" "RRE,RX,RXY,RX,RXY,VRR,VRS,VRS,VRX,VRX,RRE,VRI,VRI,VRI,VRI,
+                        SIL,SIL,RI,RI,RRE,RRE,RIL,RR,RXY,RXY,RIL")])
+
+
+; vec_load_lanes?
+
+; vec_store_lanes?
+
+; FIXME: Support also vector mode operands for 1
+; FIXME: A target memory operand seems to be useful otherwise we end
+; up with vl vlvgg vst.  Shouldn't the middle-end be able to handle
+; that itself?
+(define_insn "*vec_set<mode>"
+  [(set (match_operand:V                    0 "register_operand"             "=v, v,v")
+	(unspec:V [(match_operand:<non_vec> 1 "general_operand"               "d,QR,K")
+		   (match_operand:SI        2 "shift_count_or_setmem_operand" "Y, I,I")
+		   (match_operand:V         3 "register_operand"              "0, 0,0")]
+		  UNSPEC_VEC_SET))]
+  "TARGET_VX"
+  "@
+   vlvg<bhfgq>\t%v0,%1,%Y2
+   vle<bhfgq>\t%v0,%1,%2
+   vlei<bhfgq>\t%v0,%1,%2"
+  [(set_attr "op_type" "VRS,VRX,VRI")])
+
+; vec_set is supposed to *modify* an existing vector so operand 0 is
+; duplicated as input operand.
+(define_expand "vec_set<mode>"
+  [(set (match_operand:V                    0 "register_operand"              "")
+	(unspec:V [(match_operand:<non_vec> 1 "general_operand"               "")
+		   (match_operand:SI        2 "shift_count_or_setmem_operand" "")
+		   (match_dup 0)]
+		   UNSPEC_VEC_SET))]
+  "TARGET_VX")
+
+; FIXME: Support also vector mode operands for 0
+; FIXME: This should be (vec_select ..) or something but it does only allow constant selectors :(
+; This is used via RTL standard name as well as for expanding the builtin
+(define_insn "vec_extract<mode>"
+  [(set (match_operand:<non_vec> 0 "nonimmediate_operand"                        "=d,QR")
+	(unspec:<non_vec> [(match_operand:V  1 "register_operand"                " v, v")
+			   (match_operand:SI 2 "shift_count_or_setmem_operand"   " Y, I")]
+			  UNSPEC_VEC_EXTRACT))]
+  "TARGET_VX"
+  "@
+   vlgv<bhfgq>\t%0,%v1,%Y2
+   vste<bhfgq>\t%v1,%0,%2"
+  [(set_attr "op_type" "VRS,VRX")])
+
+(define_expand "vec_init<V_HW:mode>"
+  [(match_operand:V_HW 0 "register_operand" "")
+   (match_operand:V_HW 1 "nonmemory_operand" "")]
+  "TARGET_VX"
+{
+  s390_expand_vec_init (operands[0], operands[1]);
+  DONE;
+})
+
+; Replicate from vector element
+(define_insn "*vec_splat<mode>"
+  [(set (match_operand:V_HW   0 "register_operand" "=v")
+	(vec_duplicate:V_HW
+	 (vec_select:<non_vec>
+	  (match_operand:V_HW 1 "register_operand"  "v")
+	  (parallel
+	   [(match_operand:QI 2 "const_mask_operand" "C")]))))]
+  "TARGET_VX && UINTVAL (operands[2]) < GET_MODE_NUNITS (<V_HW:MODE>mode)"
+  "vrep<bhfgq>\t%v0,%v1,%2"
+  [(set_attr "op_type" "VRI")])
+
+(define_insn "*vec_splats<mode>"
+  [(set (match_operand:V_HW                          0 "register_operand" "=v,v,v,v")
+	(vec_duplicate:V_HW (match_operand:<non_vec> 1 "general_operand"  "QR,K,v,d")))]
+  "TARGET_VX"
+  "@
+   vlrep<bhfgq>\t%v0,%1
+   vrepi<bhfgq>\t%v0,%h1
+   vrep<bhfgq>\t%v0,%v1,0
+   #"
+  [(set_attr "op_type" "VRX,VRI,VRI,*")])
+
+; vec_splats is supposed to replicate op1 into all elements of op0
+; This splitter first sets the rightmost element of op0 to op1 and
+; then does a vec_splat to replicate that element into all other
+; elements.
+(define_split
+  [(set (match_operand:V_HW                          0 "register_operand" "")
+	(vec_duplicate:V_HW (match_operand:<non_vec> 1 "register_operand" "")))]
+  "TARGET_VX && GENERAL_REG_P (operands[1])"
+  [(set (match_dup 0)
+	(unspec:V_HW [(match_dup 1) (match_dup 2) (match_dup 0)] UNSPEC_VEC_SET))
+   (set (match_dup 0)
+	(vec_duplicate:V_HW
+	 (vec_select:<non_vec>
+	  (match_dup 0) (parallel [(match_dup 2)]))))]
+{
+  operands[2] = GEN_INT (GET_MODE_NUNITS (<MODE>mode) - 1);
+})
+
+(define_expand "vcond<V_HW:mode><V_HW2:mode>"
+  [(set (match_operand:V_HW 0 "register_operand" "")
+	(if_then_else:V_HW
+	 (match_operator 3 "comparison_operator"
+			 [(match_operand:V_HW2 4 "register_operand" "")
+			  (match_operand:V_HW2 5 "register_operand" "")])
+	 (match_operand:V_HW 1 "nonmemory_operand" "")
+	 (match_operand:V_HW 2 "nonmemory_operand" "")))]
+  "TARGET_VX && GET_MODE_NUNITS (<V_HW:MODE>mode) == GET_MODE_NUNITS (<V_HW2:MODE>mode)"
+{
+  s390_expand_vcond (operands[0], operands[1], operands[2],
+		     GET_CODE (operands[3]), operands[4], operands[5]);
+  DONE;
+})
+
+(define_expand "vcondu<V_HW:mode><V_HW2:mode>"
+  [(set (match_operand:V_HW 0 "register_operand" "")
+	(if_then_else:V_HW
+	 (match_operator 3 "comparison_operator"
+			 [(match_operand:V_HW2 4 "register_operand" "")
+			  (match_operand:V_HW2 5 "register_operand" "")])
+	 (match_operand:V_HW 1 "nonmemory_operand" "")
+	 (match_operand:V_HW 2 "nonmemory_operand" "")))]
+  "TARGET_VX && GET_MODE_NUNITS (<V_HW:MODE>mode) == GET_MODE_NUNITS (<V_HW2:MODE>mode)"
+{
+  s390_expand_vcond (operands[0], operands[1], operands[2],
+		     GET_CODE (operands[3]), operands[4], operands[5]);
+  DONE;
+})
+
+; We only have HW support for byte vectors.  The middle-end is
+; supposed to lower the mode if required.
+(define_insn "vec_permv16qi"
+  [(set (match_operand:V16QI 0 "register_operand"               "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "v")
+		       (match_operand:V16QI 2 "register_operand" "v")
+		       (match_operand:V16QI 3 "register_operand" "v")]
+		      UNSPEC_VEC_PERM))]
+  "TARGET_VX"
+  "vperm\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+; vec_perm_const for V2DI using vpdi?
+
+;;
+;; Vector integer arithmetic instructions
+;;
+
+; vab, vah, vaf, vag, vaq
+
+; We use nonimmediate_operand instead of register_operand since it is
+; better to have the reloads into VRs instead of splitting the
+; operation into two DImode ADDs.
+(define_insn "<ti*>add<mode>3"
+  [(set (match_operand:VIT           0 "nonimmediate_operand" "=v")
+	(plus:VIT (match_operand:VIT 1 "nonimmediate_operand" "%v")
+		  (match_operand:VIT 2 "general_operand"       "v")))]
+  "TARGET_VX"
+  "va<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vsb, vsh, vsf, vsg, vsq
+(define_insn "<ti*>sub<mode>3"
+  [(set (match_operand:VIT            0 "nonimmediate_operand" "=v")
+	(minus:VIT (match_operand:VIT 1 "nonimmediate_operand"  "v")
+		   (match_operand:VIT 2 "general_operand"  "v")))]
+  "TARGET_VX"
+  "vs<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmlb, vmlhw, vmlf
+(define_insn "mul<mode>3"
+  [(set (match_operand:VI_QHS              0 "register_operand" "=v")
+	(mult:VI_QHS (match_operand:VI_QHS 1 "register_operand" "%v")
+		     (match_operand:VI_QHS 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vml<bhfgq><w>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vlcb, vlch, vlcf, vlcg
+(define_insn "neg<mode>2"
+  [(set (match_operand:VI         0 "register_operand" "=v")
+	(neg:VI (match_operand:VI 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vlc<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; vlpb, vlph, vlpf, vlpg
+(define_insn "abs<mode>2"
+  [(set (match_operand:VI         0 "register_operand" "=v")
+	(abs:VI (match_operand:VI 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vlp<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector sum across
+
+; Sum across DImode parts of the 1st operand and add the rightmost
+; element of 2nd operand
+; vsumgh, vsumgf
+(define_insn "*vec_sum2<mode>"
+  [(set (match_operand:V2DI 0 "register_operand" "=v")
+	(unspec:V2DI [(match_operand:VI_HW_HS 1 "register_operand" "v")
+		      (match_operand:VI_HW_HS 2 "register_operand" "v")]
+		     UNSPEC_VEC_VSUMG))]
+  "TARGET_VX"
+  "vsumg<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vsumb, vsumh
+(define_insn "*vec_sum4<mode>"
+  [(set (match_operand:V4SI 0 "register_operand" "=v")
+	(unspec:V4SI [(match_operand:VI_HW_QH 1 "register_operand" "v")
+		      (match_operand:VI_HW_QH 2 "register_operand" "v")]
+		     UNSPEC_VEC_VSUM))]
+  "TARGET_VX"
+  "vsum<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+;;
+;; Vector bit instructions (int + fp)
+;;
+
+; Vector and
+
+(define_insn "and<mode>3"
+  [(set (match_operand:VT         0 "register_operand" "=v")
+	(and:VT (match_operand:VT 1 "register_operand" "%v")
+		(match_operand:VT 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vn\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector or
+
+(define_insn "ior<mode>3"
+  [(set (match_operand:VT         0 "register_operand" "=v")
+	(ior:VT (match_operand:VT 1 "register_operand" "%v")
+		(match_operand:VT 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vo\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector xor
+
+(define_insn "xor<mode>3"
+  [(set (match_operand:VT         0 "register_operand" "=v")
+	(xor:VT (match_operand:VT 1 "register_operand" "%v")
+		(match_operand:VT 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vx\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Bitwise inversion of a vector - used for vec_cmpne
+(define_insn "*not<mode>"
+  [(set (match_operand:VT         0 "register_operand" "=v")
+	(not:VT (match_operand:VT 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vnot\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; Vector population count
+
+(define_insn "popcountv16qi2"
+  [(set (match_operand:V16QI                0 "register_operand" "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand"  "v")]
+		      UNSPEC_POPCNT))]
+  "TARGET_VX"
+  "vpopct\t%v0,%v1,0"
+  [(set_attr "op_type" "VRR")])
+
+; vpopct only counts bits in byte elements.  Bigger element sizes need
+; to be emulated.  Word and doubleword elements can use the sum across
+; instructions.  For halfword sized elements we do a shift of a copy
+; of the result, add it to the result and extend it to halfword
+; element size (unpack).
+
+(define_expand "popcountv8hi2"
+  [(set (match_dup 2)
+	(unspec:V16QI [(subreg:V16QI (match_operand:V8HI 1 "register_operand" "v") 0)]
+		      UNSPEC_POPCNT))
+   ; Make a copy of the result
+   (set (match_dup 3) (match_dup 2))
+   ; Generate the shift count operand in a VR (8->byte 7)
+   (set (match_dup 4) (match_dup 5))
+   (set (match_dup 4) (unspec:V16QI [(const_int 8)
+				     (const_int 7)
+				     (match_dup 4)] UNSPEC_VEC_SET))
+   ; Vector shift right logical by one byte
+   (set (match_dup 3)
+	(unspec:V16QI [(match_dup 3) (match_dup 4)] UNSPEC_VEC_SRLB))
+   ; Add the shifted and the original result
+   (set (match_dup 2)
+	(plus:V16QI (match_dup 2) (match_dup 3)))
+   ; Generate mask for the odd numbered byte elements
+   (set (match_dup 3)
+	(const_vector:V16QI [(const_int 0) (const_int 255)
+			     (const_int 0) (const_int 255)
+			     (const_int 0) (const_int 255)
+			     (const_int 0) (const_int 255)
+			     (const_int 0) (const_int 255)
+			     (const_int 0) (const_int 255)
+			     (const_int 0) (const_int 255)
+			     (const_int 0) (const_int 255)]))
+   ; Zero out the even indexed bytes
+   (set (match_operand:V8HI 0 "register_operand" "=v")
+	(and:V8HI (subreg:V8HI (match_dup 2) 0)
+		  (subreg:V8HI (match_dup 3) 0)))
+]
+  "TARGET_VX"
+{
+  operands[2] = gen_reg_rtx (V16QImode);
+  operands[3] = gen_reg_rtx (V16QImode);
+  operands[4] = gen_reg_rtx (V16QImode);
+  operands[5] = CONST0_RTX (V16QImode);
+})
+
+(define_expand "popcountv4si2"
+  [(set (match_dup 2)
+	(unspec:V16QI [(subreg:V16QI (match_operand:V4SI 1 "register_operand" "v") 0)]
+		      UNSPEC_POPCNT))
+   (set (match_operand:V4SI 0 "register_operand" "=v")
+	(unspec:V4SI [(match_dup 2) (match_dup 3)]
+		     UNSPEC_VEC_VSUM))]
+  "TARGET_VX"
+{
+  operands[2] = gen_reg_rtx (V16QImode);
+  operands[3] = force_reg (V16QImode, CONST0_RTX (V16QImode));
+})
+
+(define_expand "popcountv2di2"
+  [(set (match_dup 2)
+	(unspec:V16QI [(subreg:V16QI (match_operand:V2DI 1 "register_operand" "v") 0)]
+		      UNSPEC_POPCNT))
+   (set (match_dup 3)
+	(unspec:V4SI [(match_dup 2) (match_dup 4)]
+		     UNSPEC_VEC_VSUM))
+   (set (match_operand:V2DI 0 "register_operand" "=v")
+	(unspec:V2DI [(match_dup 3) (match_dup 5)]
+		     UNSPEC_VEC_VSUMG))]
+  "TARGET_VX"
+{
+  operands[2] = gen_reg_rtx (V16QImode);
+  operands[3] = gen_reg_rtx (V4SImode);
+  operands[4] = force_reg (V16QImode, CONST0_RTX (V16QImode));
+  operands[5] = force_reg (V4SImode, CONST0_RTX (V4SImode));
+})
+
+; Count leading zeros
+(define_insn "clz<mode>2"
+  [(set (match_operand:V        0 "register_operand" "=v")
+	(clz:V (match_operand:V 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vclz<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; Count trailing zeros
+(define_insn "ctz<mode>2"
+  [(set (match_operand:V        0 "register_operand" "=v")
+	(ctz:V (match_operand:V 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vctz<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector rotate instructions
+
+; Each vector element rotated by a scalar
+; verllb, verllh, verllf, verllg
+(define_insn "rotl<mode>3"
+  [(set (match_operand:VI            0 "register_operand"             "=v")
+	(rotate:VI (match_operand:VI 1 "register_operand"              "v")
+		   (match_operand:SI 2 "shift_count_or_setmem_operand" "Y")))]
+  "TARGET_VX"
+  "verll<bhfgq>\t%v0,%v1,%Y2"
+  [(set_attr "op_type" "VRS")])
+
+; Each vector element rotated by the corresponding vector element
+; verllvb, verllvh, verllvf, verllvg
+(define_insn "vrotl<mode>3"
+  [(set (match_operand:VI            0 "register_operand" "=v")
+	(rotate:VI (match_operand:VI 1 "register_operand"  "v")
+		   (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "verllv<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Shift each element by scalar value
+
+; veslb, veslh, veslf, veslg
+(define_insn "ashl<mode>3"
+  [(set (match_operand:VI            0 "register_operand"             "=v")
+	(ashift:VI (match_operand:VI 1 "register_operand"              "v")
+		   (match_operand:SI 2 "shift_count_or_setmem_operand" "Y")))]
+  "TARGET_VX"
+  "vesl<bhfgq>\t%v0,%v1,%Y2"
+  [(set_attr "op_type" "VRS")])
+
+; vesrab, vesrah, vesraf, vesrag
+(define_insn "ashr<mode>3"
+  [(set (match_operand:VI              0 "register_operand"             "=v")
+	(ashiftrt:VI (match_operand:VI 1 "register_operand"              "v")
+		     (match_operand:SI 2 "shift_count_or_setmem_operand" "Y")))]
+  "TARGET_VX"
+  "vesra<bhfgq>\t%v0,%v1,%Y2"
+  [(set_attr "op_type" "VRS")])
+
+; vesrlb, vesrlh, vesrlf, vesrlg
+(define_insn "lshr<mode>3"
+  [(set (match_operand:VI              0 "register_operand"             "=v")
+	(lshiftrt:VI (match_operand:VI 1 "register_operand"              "v")
+		     (match_operand:SI 2 "shift_count_or_setmem_operand" "Y")))]
+  "TARGET_VX"
+  "vesrl<bhfgq>\t%v0,%v1,%Y2"
+  [(set_attr "op_type" "VRS")])
+
+
+; Shift each element by corresponding vector element
+
+; veslvb, veslvh, veslvf, veslvg
+(define_insn "vashl<mode>3"
+  [(set (match_operand:VI            0 "register_operand" "=v")
+	(ashift:VI (match_operand:VI 1 "register_operand"  "v")
+		   (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "veslv<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vesravb, vesravh, vesravf, vesravg
+(define_insn "vashr<mode>3"
+  [(set (match_operand:VI              0 "register_operand" "=v")
+	(ashiftrt:VI (match_operand:VI 1 "register_operand"  "v")
+		     (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vesrav<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vesrlvb, vesrlvh, vesrlvf, vesrlvg
+(define_insn "vlshr<mode>3"
+  [(set (match_operand:VI              0 "register_operand" "=v")
+	(lshiftrt:VI (match_operand:VI 1 "register_operand"  "v")
+		     (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vesrlv<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; Vector shift right logical by byte
+
+; Pattern used by e.g. popcount
+(define_insn "*vec_srb<mode>"
+  [(set (match_operand:V_HW 0 "register_operand"                    "=v")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand"       "v")
+		      (match_operand:<tointvec> 2 "register_operand" "v")]
+		     UNSPEC_VEC_SRLB))]
+  "TARGET_VX"
+  "vsrlb\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; vmnb, vmnh, vmnf, vmng
+(define_insn "smin<mode>3"
+  [(set (match_operand:VI          0 "register_operand" "=v")
+	(smin:VI (match_operand:VI 1 "register_operand" "%v")
+		 (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vmn<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmxb, vmxh, vmxf, vmxg
+(define_insn "smax<mode>3"
+  [(set (match_operand:VI          0 "register_operand" "=v")
+	(smax:VI (match_operand:VI 1 "register_operand" "%v")
+		 (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vmx<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmnlb, vmnlh, vmnlf, vmnlg
+(define_insn "umin<mode>3"
+  [(set (match_operand:VI          0 "register_operand" "=v")
+	(umin:VI (match_operand:VI 1 "register_operand" "%v")
+		 (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vmnl<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmxlb, vmxlh, vmxlf, vmxlg
+(define_insn "umax<mode>3"
+  [(set (match_operand:VI          0 "register_operand" "=v")
+	(umax:VI (match_operand:VI 1 "register_operand" "%v")
+		 (match_operand:VI 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vmxl<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmeb, vmeh, vmef
+(define_insn "vec_widen_smult_even_<mode>"
+  [(set (match_operand:<vec_double>                 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_QHS 1 "register_operand" "%v")
+			      (match_operand:VI_QHS 2 "register_operand"  "v")]
+			     UNSPEC_VEC_SMULT_EVEN))]
+  "TARGET_VX"
+  "vme<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmleb, vmleh, vmlef
+(define_insn "vec_widen_umult_even_<mode>"
+  [(set (match_operand:<vec_double>                 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_QHS 1 "register_operand" "%v")
+			      (match_operand:VI_QHS 2 "register_operand"  "v")]
+			     UNSPEC_VEC_UMULT_EVEN))]
+  "TARGET_VX"
+  "vmle<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmob, vmoh, vmof
+(define_insn "vec_widen_smult_odd_<mode>"
+  [(set (match_operand:<vec_double>                 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_QHS 1 "register_operand" "%v")
+			      (match_operand:VI_QHS 2 "register_operand"  "v")]
+			     UNSPEC_VEC_SMULT_ODD))]
+  "TARGET_VX"
+  "vmo<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmlob, vmloh, vmlof
+(define_insn "vec_widen_umult_odd_<mode>"
+  [(set (match_operand:<vec_double>                 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_QHS 1 "register_operand" "%v")
+			      (match_operand:VI_QHS 2 "register_operand"  "v")]
+			     UNSPEC_VEC_UMULT_ODD))]
+  "TARGET_VX"
+  "vmlo<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vec_widen_umult_hi
+; vec_widen_umult_lo
+; vec_widen_smult_hi
+; vec_widen_smult_lo
+
+; vec_widen_ushiftl_hi
+; vec_widen_ushiftl_lo
+; vec_widen_sshiftl_hi
+; vec_widen_sshiftl_lo
+
+;;
+;; Vector floating point arithmetic instructions
+;;
+
+(define_insn "addv2df3"
+  [(set (match_operand:V2DF            0 "register_operand" "=v")
+	(plus:V2DF (match_operand:V2DF 1 "register_operand" "%v")
+		   (match_operand:V2DF 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vfadb\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "subv2df3"
+  [(set (match_operand:V2DF             0 "register_operand" "=v")
+	(minus:V2DF (match_operand:V2DF 1 "register_operand" "%v")
+		    (match_operand:V2DF 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vfsdb\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "mulv2df3"
+  [(set (match_operand:V2DF            0 "register_operand" "=v")
+	(mult:V2DF (match_operand:V2DF 1 "register_operand" "%v")
+		   (match_operand:V2DF 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vfmdb\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "divv2df3"
+  [(set (match_operand:V2DF           0 "register_operand" "=v")
+	(div:V2DF (match_operand:V2DF 1 "register_operand"  "v")
+		  (match_operand:V2DF 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vfddb\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "sqrtv2df2"
+  [(set (match_operand:V2DF            0 "register_operand" "=v")
+	(sqrt:V2DF (match_operand:V2DF 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vfsqdb\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "fmav2df4"
+  [(set (match_operand:V2DF           0 "register_operand" "=v")
+	(fma:V2DF (match_operand:V2DF 1 "register_operand" "%v")
+		  (match_operand:V2DF 2 "register_operand"  "v")
+		  (match_operand:V2DF 3 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vfmadb\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "fmsv2df4"
+  [(set (match_operand:V2DF                     0 "register_operand" "=v")
+	(fma:V2DF (match_operand:V2DF           1 "register_operand" "%v")
+		  (match_operand:V2DF           2 "register_operand"  "v")
+		  (neg:V2DF (match_operand:V2DF 3 "register_operand"  "v"))))]
+  "TARGET_VX"
+  "vfmsdb\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "negv2df2"
+  [(set (match_operand:V2DF           0 "register_operand" "=v")
+	(neg:V2DF (match_operand:V2DF 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vflcdb\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "absv2df2"
+  [(set (match_operand:V2DF           0 "register_operand" "=v")
+	(abs:V2DF (match_operand:V2DF 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vflpdb\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "*negabsv2df2"
+  [(set (match_operand:V2DF                     0 "register_operand" "=v")
+	(neg:V2DF (abs:V2DF (match_operand:V2DF 1 "register_operand"  "v"))))]
+  "TARGET_VX"
+  "vflndb\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; Emulate with compare + select
+(define_insn_and_split "smaxv2df3"
+  [(set (match_operand:V2DF            0 "register_operand" "=v")
+	(smax:V2DF (match_operand:V2DF 1 "register_operand" "%v")
+		   (match_operand:V2DF 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "#"
+  ""
+  [(set (match_dup 3)
+	(gt:V2DI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0)
+	(if_then_else:V2DF
+	 (eq (match_dup 3) (match_dup 4))
+	 (match_dup 2)
+	 (match_dup 1)))]
+{
+  operands[3] = gen_reg_rtx (V2DImode);
+  operands[4] = CONST0_RTX (V2DImode);
+})
+
+; Emulate with compare + select
+(define_insn_and_split "sminv2df3"
+  [(set (match_operand:V2DF            0 "register_operand" "=v")
+	(smin:V2DF (match_operand:V2DF 1 "register_operand" "%v")
+		   (match_operand:V2DF 2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "#"
+  ""
+  [(set (match_dup 3)
+	(gt:V2DI (match_dup 1) (match_dup 2)))
+   (set (match_dup 0)
+	(if_then_else:V2DF
+	 (eq (match_dup 3) (match_dup 4))
+	 (match_dup 1)
+	 (match_dup 2)))]
+{
+  operands[3] = gen_reg_rtx (V2DImode);
+  operands[4] = CONST0_RTX (V2DImode);
+})
+
+
+;;
+;; Integer compares
+;;
+
+(define_insn "*vec_cmp<VICMP_HW_OP:code><VI:mode>_nocc"
+  [(set (match_operand:VI                 2 "register_operand" "=v")
+	(VICMP_HW_OP:VI (match_operand:VI 0 "register_operand"  "v")
+			(match_operand:VI 1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vc<VICMP_HW_OP:insn_cmp_op><VI:bhfgq>\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+
+;;
+;; Floating point compares
+;;
+
+; EQ, GT, GE
+(define_insn "*vec_cmp<VFCMP_HW_OP:code>v2df_nocc"
+  [(set (match_operand:V2DI                   0 "register_operand" "=v")
+	(VFCMP_HW_OP:V2DI (match_operand:V2DF 1 "register_operand"  "v")
+			  (match_operand:V2DF 2 "register_operand"  "v")))]
+   "TARGET_VX"
+   "vfc<VFCMP_HW_OP:asm_fcmp_op>db\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; Expanders for not directly supported comparisons
+
+; UNEQ a u== b -> !(a > b | b > a)
+(define_expand "vec_cmpuneqv2df"
+  [(set (match_operand:V2DI          0 "register_operand" "=v")
+	(gt:V2DI (match_operand:V2DF 1 "register_operand"  "v")
+		 (match_operand:V2DF 2 "register_operand"  "v")))
+   (set (match_dup 3)
+	(gt:V2DI (match_dup 2) (match_dup 1)))
+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))
+   (set (match_dup 0) (not:V2DI (match_dup 0)))]
+  "TARGET_VX"
+{
+  operands[3] = gen_reg_rtx (V2DImode);
+})
+
+; LTGT a <> b -> a > b | b > a
+(define_expand "vec_cmpltgtv2df"
+  [(set (match_operand:V2DI          0 "register_operand" "=v")
+	(gt:V2DI (match_operand:V2DF 1 "register_operand"  "v")
+		 (match_operand:V2DF 2 "register_operand"  "v")))
+   (set (match_dup 3) (gt:V2DI (match_dup 2) (match_dup 1)))
+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))]
+  "TARGET_VX"
+{
+  operands[3] = gen_reg_rtx (V2DImode);
+})
+
+; ORDERED (a, b): a >= b | b > a
+(define_expand "vec_orderedv2df"
+  [(set (match_operand:V2DI          0 "register_operand" "=v")
+	(ge:V2DI (match_operand:V2DF 1 "register_operand"  "v")
+		 (match_operand:V2DF 2 "register_operand"  "v")))
+   (set (match_dup 3) (gt:V2DI (match_dup 2) (match_dup 1)))
+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))]
+  "TARGET_VX"
+{
+  operands[3] = gen_reg_rtx (V2DImode);
+})
+
+; UNORDERED (a, b): !ORDERED (a, b)
+(define_expand "vec_unorderedv2df"
+  [(set (match_operand:V2DI          0 "register_operand" "=v")
+	(ge:V2DI (match_operand:V2DF 1 "register_operand"  "v")
+		 (match_operand:V2DF 2 "register_operand"  "v")))
+   (set (match_dup 3) (gt:V2DI (match_dup 2) (match_dup 1)))
+   (set (match_dup 0) (ior:V2DI (match_dup 0) (match_dup 3)))
+   (set (match_dup 0) (not:V2DI (match_dup 0)))]
+  "TARGET_VX"
+{
+  operands[3] = gen_reg_rtx (V2DImode);
+})
+
+(define_insn "*vec_load_pairv2di"
+  [(set (match_operand:V2DI                0 "register_operand" "=v")
+	(vec_concat:V2DI (match_operand:DI 1 "register_operand"  "d")
+			 (match_operand:DI 2 "register_operand"  "d")))]
+  "TARGET_VX"
+  "vlvgp\t%v0,%1,%2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vllv16qi"
+  [(set (match_operand:V16QI              0 "register_operand" "=v")
+	(unspec:V16QI [(match_operand:SI  1 "register_operand"  "d")
+		       (match_operand:BLK 2 "memory_operand"    "Q")]
+		      UNSPEC_VEC_LOAD_LEN))]
+  "TARGET_VX"
+  "vll\t%v0,%1,%2"
+  [(set_attr "op_type" "VRS")])
+
+; vfenebs, vfenehs, vfenefs
+; vfenezbs, vfenezhs, vfenezfs
+(define_insn "vec_vfenes<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "v")
+			   (match_operand:QI 3 "const_mask_operand" "C")]
+			  UNSPEC_VEC_VFENE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)]
+		      UNSPEC_VEC_VFENECC))]
+  "TARGET_VX"
+{
+  unsigned HOST_WIDE_INT flags = INTVAL (operands[3]);
+
+  gcc_assert (!(flags & ~(VSTRING_FLAG_ZS | VSTRING_FLAG_CS)));
+  flags &= ~VSTRING_FLAG_CS;
+
+  if (flags == VSTRING_FLAG_ZS)
+    return "vfenez<bhfgq>s\t%v0,%v1,%v2";
+  return "vfene<bhfgq>s\t%v0,%v1,%v2";
+}
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector select
+
+; The following splitters simplify vec_sel for constant 0 or -1
+; selection sources.  This is required to generate efficient code for
+; vcond.
+
+; a = b == c;
+(define_split
+  [(set (match_operand:V 0 "register_operand" "")
+	(if_then_else:V
+	 (eq (match_operand:<tointvec> 3 "register_operand" "")
+	     (match_operand:V 4 "const0_operand" ""))
+	 (match_operand:V 1 "const0_operand" "")
+	 (match_operand:V 2 "all_ones_operand" "")))]
+  "TARGET_VX"
+  [(set (match_dup 0) (match_dup 3))]
+{
+  PUT_MODE (operands[3], <V:MODE>mode);
+})
+
+; a = ~(b == c)
+(define_split
+  [(set (match_operand:V 0 "register_operand" "")
+	(if_then_else:V
+	 (eq (match_operand:<tointvec> 3 "register_operand" "")
+	     (match_operand:V 4 "const0_operand" ""))
+	 (match_operand:V 1 "all_ones_operand" "")
+	 (match_operand:V 2 "const0_operand" "")))]
+  "TARGET_VX"
+  [(set (match_dup 0) (not:V (match_dup 3)))]
+{
+  PUT_MODE (operands[3], <V:MODE>mode);
+})
+
+; a = b != c
+(define_split
+  [(set (match_operand:V 0 "register_operand" "")
+	(if_then_else:V
+	 (ne (match_operand:<tointvec> 3 "register_operand" "")
+	     (match_operand:V 4 "const0_operand" ""))
+	 (match_operand:V 1 "all_ones_operand" "")
+	 (match_operand:V 2 "const0_operand" "")))]
+  "TARGET_VX"
+  [(set (match_dup 0) (match_dup 3))]
+{
+  PUT_MODE (operands[3], <V:MODE>mode);
+})
+
+; a = ~(b != c)
+(define_split
+  [(set (match_operand:V 0 "register_operand" "")
+	(if_then_else:V
+	 (ne (match_operand:<tointvec> 3 "register_operand" "")
+	     (match_operand:V 4 "const0_operand" ""))
+	 (match_operand:V 1 "const0_operand" "")
+	 (match_operand:V 2 "all_ones_operand" "")))]
+  "TARGET_VX"
+  [(set (match_dup 0) (not:V (match_dup 3)))]
+{
+  PUT_MODE (operands[3], <V:MODE>mode);
+})
+
+; op0 = op3 == 0 ? op1 : op2
+(define_insn "*vec_sel0<mode>"
+  [(set (match_operand:V 0 "register_operand" "=v")
+	(if_then_else:V
+	 (eq (match_operand:<tointvec> 3 "register_operand" "v")
+	     (match_operand:<tointvec> 4 "const0_operand" ""))
+	 (match_operand:V 1 "register_operand" "v")
+	 (match_operand:V 2 "register_operand" "v")))]
+  "TARGET_VX"
+  "vsel\t%v0,%2,%1,%3"
+  [(set_attr "op_type" "VRR")])
+
+; op0 = !op3 == 0 ? op1 : op2
+(define_insn "*vec_sel0<mode>"
+  [(set (match_operand:V 0 "register_operand" "=v")
+	(if_then_else:V
+	 (eq (not:<tointvec> (match_operand:<tointvec> 3 "register_operand" "v"))
+	     (match_operand:<tointvec> 4 "const0_operand" ""))
+	 (match_operand:V 1 "register_operand" "v")
+	 (match_operand:V 2 "register_operand" "v")))]
+  "TARGET_VX"
+  "vsel\t%v0,%1,%2,%3"
+  [(set_attr "op_type" "VRR")])
+
+; op0 = op3 == -1 ? op1 : op2
+(define_insn "*vec_sel1<mode>"
+  [(set (match_operand:V 0 "register_operand" "=v")
+	(if_then_else:V
+	 (eq (match_operand:<tointvec> 3 "register_operand" "v")
+	     (match_operand:<tointvec> 4 "all_ones_operand" ""))
+	 (match_operand:V 1 "register_operand" "v")
+	 (match_operand:V 2 "register_operand" "v")))]
+  "TARGET_VX"
+  "vsel\t%v0,%1,%2,%3"
+  [(set_attr "op_type" "VRR")])
+
+; op0 = !op3 == -1 ? op1 : op2
+(define_insn "*vec_sel1<mode>"
+  [(set (match_operand:V 0 "register_operand" "=v")
+	(if_then_else:V
+	 (eq (not:<tointvec> (match_operand:<tointvec> 3 "register_operand" "v"))
+	     (match_operand:<tointvec> 4 "all_ones_operand" ""))
+	 (match_operand:V 1 "register_operand" "v")
+	 (match_operand:V 2 "register_operand" "v")))]
+  "TARGET_VX"
+  "vsel\t%v0,%2,%1,%3"
+  [(set_attr "op_type" "VRR")])
+
+
+
+; reduc_smin
+; reduc_smax
+; reduc_umin
+; reduc_umax
+
+; vec_shl vrep + vsl
+; vec_shr
+
+; vec_pack_trunc
+; vec_pack_ssat
+; vec_pack_usat
+; vec_pack_sfix_trunc
+; vec_pack_ufix_trunc
+; vec_unpacks_hi
+; vec_unpacks_low
+; vec_unpacku_hi
+; vec_unpacku_low
+; vec_unpacks_float_hi
+; vec_unpacks_float_lo
+; vec_unpacku_float_hi
+; vec_unpacku_float_lo
--- gcc/config/s390/vx-builtins.md	1970-01-01 01:00:00.000000000 +0100
+++ gcc/config/s390/vx-builtins.md	2016-05-11 19:46:05.504890170 +0200
@@ -0,0 +1,2081 @@
+;;- Instruction patterns for the System z vector facility builtins.
+;;  Copyright (C) 2015 Free Software Foundation, Inc.
+;;  Contributed by Andreas Krebbel (Andreas.Krebbel@de.ibm.com)
+
+;; This file is part of GCC.
+
+;; GCC is free software; you can redistribute it and/or modify it under
+;; the terms of the GNU General Public License as published by the Free
+;; Software Foundation; either version 3, or (at your option) any later
+;; version.
+
+;; GCC is distributed in the hope that it will be useful, but WITHOUT ANY
+;; WARRANTY; without even the implied warranty of MERCHANTABILITY or
+;; FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+;; for more details.
+
+;; You should have received a copy of the GNU General Public License
+;; along with GCC; see the file COPYING3.  If not see
+;; <http://www.gnu.org/licenses/>.
+
+; The patterns in this file are enabled with -mzvector
+
+(define_mode_iterator V_HW_64 [V2DI V2DF])
+(define_mode_iterator V_HW_32_64 [V4SI V2DI V2DF])
+(define_mode_iterator VI_HW_SD [V4SI V2DI])
+(define_mode_iterator V_HW_HSD [V8HI V4SI V2DI V2DF])
+(define_mode_iterator VI_HW_HSD [V8HI V4SI V2DI])
+
+; The element type of the vector with floating point modes translated
+; to int modes of the same size.
+(define_mode_attr non_vec_int[(V1QI "QI") (V2QI "QI") (V4QI "QI") (V8QI "QI") (V16QI "QI")
+			      (V1HI "HI") (V2HI "HI") (V4HI "HI") (V8HI "HI")
+			      (V1SI "SI") (V2SI "SI") (V4SI "SI")
+			      (V1DI "DI") (V2DI "DI")
+			      (V1SF "SI") (V2SF "SI") (V4SF "SI")
+			      (V1DF "DI") (V2DF "DI")])
+
+; Condition code modes generated by int comparisons
+(define_mode_iterator VICMP [CCVEQ CCVH CCVHU])
+
+; Comparisons supported by the vec_cmp* builtins
+(define_code_iterator intcmp [eq gt gtu ge geu lt ltu le leu])
+(define_code_iterator fpcmp  [eq gt ge lt le])
+
+; Comparisons supported by the vec_all/any* builtins
+(define_code_iterator intcmpcc [eq ne gt ge lt le gtu geu ltu leu])
+(define_code_iterator fpcmpcc  [eq ne gt ge unle unlt lt le])
+
+; Flags for vector string instructions (vfae all 4, vfee only ZS and CS, vstrc all 4)
+(define_constants
+  [(VSTRING_FLAG_IN         8)   ; invert result
+   (VSTRING_FLAG_RT         4)   ; result type
+   (VSTRING_FLAG_ZS         2)   ; zero search
+   (VSTRING_FLAG_CS         1)]) ; condition code set
+
+; Rounding modes as being used for e.g. VFI
+(define_constants
+  [(VEC_RND_CURRENT                0)
+   (VEC_RND_NEAREST_AWAY_FROM_ZERO 1)
+   (VEC_RND_SHORT_PREC             3)
+   (VEC_RND_NEAREST_TO_EVEN        4)
+   (VEC_RND_TO_ZERO                5)
+   (VEC_RND_TO_INF                 6)
+   (VEC_RND_TO_MINF                7)])
+
+
+; Vector gather element
+
+(define_insn "vec_gather_element<mode>"
+  [(set (match_operand:V_HW_32_64                     0 "register_operand"  "=v")
+	(unspec:V_HW_32_64 [(match_operand:V_HW_32_64 1 "register_operand"   "0")
+			    (match_operand:<tointvec> 2 "register_operand"   "v")
+			    (match_operand:BLK        3 "memory_operand"    "QR")
+			    (match_operand:QI         4 "const_mask_operand" "C")]
+			   UNSPEC_VEC_GATHER))]
+  "TARGET_VX && UINTVAL (operands[4]) < GET_MODE_NUNITS (<V_HW_32_64:MODE>mode)"
+  "vge<bhfgq>\t%0,%O3(%v2,%R3),%b4"
+  [(set_attr "op_type" "VRV")])
+
+(define_expand "vec_genmask<mode>"
+  [(match_operand:VI_HW 0 "register_operand" "=v")
+   (match_operand:QI    1 "const_int_operand" "C")
+   (match_operand:QI    2 "const_int_operand" "C")]
+  "TARGET_VX"
+{
+  int nunits = GET_MODE_NUNITS (<VI_HW:MODE>mode);
+  int bitlen = GET_MODE_UNIT_BITSIZE (<VI_HW:MODE>mode);
+  /* To bit little endian style.  */
+  int end = bitlen - 1 - INTVAL (operands[1]);
+  int start = bitlen - 1 - INTVAL (operands[2]);
+  rtx const_vec[16];
+  int i;
+  unsigned HOST_WIDE_INT mask;
+  bool swapped_p = false;
+
+  if (start > end)
+    {
+      i = start - 1; start = end + 1; end = i;
+      swapped_p = true;
+    }
+  if (end == 63)
+    mask = (unsigned HOST_WIDE_INT) -1;
+  else
+    mask = ((unsigned HOST_WIDE_INT) 1 << (end + 1)) - 1;
+
+  mask &= ~(((unsigned HOST_WIDE_INT) 1 << start) - 1);
+
+  if (swapped_p)
+    mask = ~mask;
+
+  for (i = 0; i < nunits; i++)
+    const_vec[i] = GEN_INT (trunc_int_for_mode (mask,
+			      GET_MODE_INNER (<VI_HW:MODE>mode)));
+
+  emit_insn (gen_rtx_SET (VOIDmode, operands[0],
+			  gen_rtx_CONST_VECTOR (<VI_HW:MODE>mode,
+						gen_rtvec_v (nunits, const_vec))));
+  DONE;
+})
+
+(define_expand "vec_genbytemaskv16qi"
+  [(match_operand:V16QI 0 "register_operand"  "")
+   (match_operand:HI    1 "const_int_operand" "")]
+  "TARGET_VX"
+{
+  int i;
+  unsigned mask = 0x8000;
+  rtx const_vec[16];
+  unsigned HOST_WIDE_INT byte_mask = INTVAL (operands[1]);
+
+  for (i = 0; i < 16; i++)
+    {
+      if (mask & byte_mask)
+	const_vec[i] = constm1_rtx;
+      else
+	const_vec[i] = const0_rtx;
+      mask = mask >> 1;
+    }
+  emit_insn (gen_rtx_SET (VOIDmode, operands[0],
+			  gen_rtx_CONST_VECTOR (V16QImode,
+						gen_rtvec_v (16, const_vec))));
+  DONE;
+})
+
+(define_expand "vec_splats<mode>"
+  [(set (match_operand:V_HW                          0 "register_operand" "")
+	(vec_duplicate:V_HW (match_operand:<non_vec> 1 "general_operand"  "")))]
+  "TARGET_VX")
+
+(define_expand "vec_insert<mode>"
+  [(set (match_operand:V_HW                    0 "register_operand" "")
+	(unspec:V_HW [(match_operand:<non_vec> 2 "register_operand" "")
+		      (match_operand:SI        3 "shift_count_or_setmem_operand" "")
+		      (match_operand:V_HW      1 "register_operand" "")]
+		     UNSPEC_VEC_SET))]
+  "TARGET_VX"
+  "")
+
+; This is vec_set + modulo arithmetic on the element selector (op 2)
+(define_expand "vec_promote<mode>"
+  [(set (match_operand:V_HW                    0 "register_operand" "")
+	(unspec:V_HW [(match_operand:<non_vec> 1 "register_operand" "")
+		      (match_operand:SI        2 "shift_count_or_setmem_operand" "")
+		      (match_dup 0)]
+		     UNSPEC_VEC_SET))]
+  "TARGET_VX"
+  "")
+
+; vec_extract is also an RTL standard name -> vector.md
+
+(define_insn "vec_insert_and_zero<mode>"
+  [(set (match_operand:V_HW                    0 "register_operand" "=v")
+	(unspec:V_HW [(match_operand:<non_vec> 1 "memory_operand"   "QR")]
+		     UNSPEC_VEC_INSERT_AND_ZERO))]
+  "TARGET_VX"
+  "vllez<bhfgq>\t%v0,%1"
+  [(set_attr "op_type" "VRX")])
+
+(define_insn "vlbb"
+  [(set (match_operand:V16QI              0 "register_operand"   "=v")
+	(unspec:V16QI [(match_operand:BLK 1 "memory_operand"     "QR")
+		       (match_operand:QI  2 "const_mask_operand"  "C")]
+		      UNSPEC_VEC_LOAD_BNDRY))]
+  "TARGET_VX && UINTVAL (operands[2]) < 7"
+  "vlbb\t%v0,%1,%2"
+  [(set_attr "op_type" "VRX")])
+
+; FIXME: The following two patterns might using vec_merge. But what is
+; the canonical form: (vec_select (vec_merge op0 op1)) or (vec_merge
+; (vec_select op0) (vec_select op1)
+(define_insn "vec_mergeh<mode>"
+  [(set (match_operand:V_HW               0 "register_operand" "=v")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand"  "v")
+		      (match_operand:V_HW 2 "register_operand"  "v")]
+		     UNSPEC_VEC_MERGEH))]
+  "TARGET_VX"
+  "vmrh<bhfgq>\t%v0,%1,%2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_mergel<mode>"
+  [(set (match_operand:V_HW               0 "register_operand" "=v")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand"  "v")
+		      (match_operand:V_HW 2 "register_operand"  "v")]
+		     UNSPEC_VEC_MERGEL))]
+  "TARGET_VX"
+  "vmrl<bhfgq>\t%v0,%1,%2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector pack
+
+(define_insn "vec_pack<mode>"
+  [(set (match_operand:<vec_half>                    0 "register_operand" "=v")
+	(unspec:<vec_half> [(match_operand:VI_HW_HSD 1 "register_operand"  "v")
+			    (match_operand:VI_HW_HSD 2 "register_operand"  "v")]
+			   UNSPEC_VEC_PACK))]
+  "TARGET_VX"
+  "vpk<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector pack saturate
+
+(define_insn "vec_packs<mode>"
+  [(set (match_operand:<vec_half>                    0 "register_operand" "=v")
+	(unspec:<vec_half> [(match_operand:VI_HW_HSD 1 "register_operand"  "v")
+			    (match_operand:VI_HW_HSD 2 "register_operand"  "v")]
+			   UNSPEC_VEC_PACK_SATURATE))]
+  "TARGET_VX"
+  "vpks<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; This is vec_packs_cc + loading cc into a caller specified memory location.
+(define_expand "vec_packs_cc<mode>"
+  [(parallel
+    [(set (reg:CCRAW CC_REGNUM)
+	  (unspec:CCRAW [(match_operand:VI_HW_HSD 1 "register_operand" "")
+			 (match_operand:VI_HW_HSD 2 "register_operand" "")]
+			UNSPEC_VEC_PACK_SATURATE_GENCC))
+     (set (match_operand:<vec_half> 0 "register_operand" "")
+	  (unspec:<vec_half> [(match_dup 1) (match_dup 2)]
+			     UNSPEC_VEC_PACK_SATURATE_CC))])
+   (set (match_dup 4)
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))
+   (set (match_operand:SI 3 "memory_operand" "")
+	(match_dup 4))]
+  "TARGET_VX"
+{
+  operands[4] = gen_reg_rtx (SImode);
+})
+
+(define_insn "*vec_packs_cc<mode>"
+  [(set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_operand:VI_HW_HSD 1 "register_operand" "v")
+		       (match_operand:VI_HW_HSD 2 "register_operand" "v")]
+		      UNSPEC_VEC_PACK_SATURATE_GENCC))
+   (set (match_operand:<vec_half> 0 "register_operand" "=v")
+	(unspec:<vec_half> [(match_dup 1) (match_dup 2)]
+			   UNSPEC_VEC_PACK_SATURATE_CC))]
+  "TARGET_VX"
+  "vpks<bhfgq>s\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector pack logical saturate
+
+(define_insn "vec_packsu<mode>"
+  [(set (match_operand:<vec_half>                    0 "register_operand" "=v")
+	(unspec:<vec_half> [(match_operand:VI_HW_HSD 1 "register_operand"  "v")
+			    (match_operand:VI_HW_HSD 2 "register_operand"  "v")]
+			   UNSPEC_VEC_PACK_UNSIGNED_SATURATE))]
+  "TARGET_VX"
+  "vpkls<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; Emulate saturate unsigned pack on signed operands.
+; Zero out negative elements and continue with the unsigned saturating pack.
+(define_expand "vec_packsu_u<mode>"
+  [(set (match_operand:<vec_half>                    0 "register_operand" "=v")
+	(unspec:<vec_half> [(match_operand:VI_HW_HSD 1 "register_operand"  "v")
+			    (match_operand:VI_HW_HSD 2 "register_operand"  "v")]
+			   UNSPEC_VEC_PACK_UNSIGNED_SATURATE))]
+  "TARGET_VX"
+{
+   rtx null_vec = CONST0_RTX(<MODE>mode);
+   enum machine_mode half_mode;
+   switch (<MODE>mode)
+   {
+     case V8HImode: half_mode = V16QImode; break;
+     case V4SImode: half_mode = V8HImode; break;
+     case V2DImode: half_mode = V4SImode; break;
+     default: gcc_unreachable ();
+   }
+   s390_expand_vcond (operands[1], operands[1], null_vec,
+		      GE, operands[1], null_vec);
+   s390_expand_vcond (operands[2], operands[2], null_vec,
+		      GE, operands[2], null_vec);
+   emit_insn (gen_rtx_SET (VOIDmode, operands[0],
+			   gen_rtx_UNSPEC (half_mode,
+					   gen_rtvec (2, operands[1], operands[2]),
+					   UNSPEC_VEC_PACK_UNSIGNED_SATURATE)));
+   DONE;
+})
+
+; This is vec_packsu_cc + loading cc into a caller specified memory location.
+; FIXME: The reg to target mem copy should be issued by reload?!
+(define_expand "vec_packsu_cc<mode>"
+  [(parallel
+    [(set (reg:CCRAW CC_REGNUM)
+	  (unspec:CCRAW [(match_operand:VI_HW_HSD 1 "register_operand" "")
+			 (match_operand:VI_HW_HSD 2 "register_operand" "")]
+			UNSPEC_VEC_PACK_UNSIGNED_SATURATE_GENCC))
+     (set (match_operand:<vec_half> 0 "register_operand" "")
+	  (unspec:<vec_half> [(match_dup 1) (match_dup 2)]
+			     UNSPEC_VEC_PACK_UNSIGNED_SATURATE_CC))])
+   (set (match_dup 4)
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))
+   (set (match_operand:SI 3 "memory_operand" "")
+	(match_dup 4))]
+  "TARGET_VX"
+{
+  operands[4] = gen_reg_rtx (SImode);
+})
+
+(define_insn "*vec_packsu_cc<mode>"
+  [(set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_operand:VI_HW_HSD 1 "register_operand" "v")
+		       (match_operand:VI_HW_HSD 2 "register_operand" "v")]
+		      UNSPEC_VEC_PACK_UNSIGNED_SATURATE_GENCC))
+   (set (match_operand:<vec_half> 0 "register_operand" "=v")
+	(unspec:<vec_half> [(match_dup 1) (match_dup 2)]
+			   UNSPEC_VEC_PACK_UNSIGNED_SATURATE_CC))]
+  "TARGET_VX"
+  "vpkls<bhfgq>s\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector permute
+
+; vec_perm is also RTL standard name, but we can only use it for V16QI
+
+(define_insn "vec_zperm<mode>"
+  [(set (match_operand:V_HW_HSD                   0 "register_operand" "=v")
+	(unspec:V_HW_HSD [(match_operand:V_HW_HSD 1 "register_operand"  "v")
+			  (match_operand:V_HW_HSD 2 "register_operand"  "v")
+			  (match_operand:V16QI    3 "register_operand"  "v")]
+			 UNSPEC_VEC_PERM))]
+  "TARGET_VX"
+  "vperm\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vec_permi<mode>"
+  [(set (match_operand:V_HW_64                  0 "register_operand"   "")
+	(unspec:V_HW_64 [(match_operand:V_HW_64 1 "register_operand"   "")
+			 (match_operand:V_HW_64 2 "register_operand"   "")
+			 (match_operand:QI      3 "const_mask_operand" "")]
+			UNSPEC_VEC_PERMI))]
+  "TARGET_VX"
+{
+  HOST_WIDE_INT val = INTVAL (operands[3]);
+  operands[3] = GEN_INT ((val & 1) | (val & 2) << 1);
+})
+
+(define_insn "*vec_permi<mode>"
+  [(set (match_operand:V_HW_64                  0 "register_operand"  "=v")
+	(unspec:V_HW_64 [(match_operand:V_HW_64 1 "register_operand"   "v")
+			 (match_operand:V_HW_64 2 "register_operand"   "v")
+			 (match_operand:QI      3 "const_mask_operand" "C")]
+			UNSPEC_VEC_PERMI))]
+  "TARGET_VX && (UINTVAL (operands[3]) & 10) == 0"
+  "vpdi\t%v0,%v1,%v2,%b3"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector replicate
+
+
+; Replicate from vector element
+(define_expand "vec_splat<mode>"
+  [(set (match_operand:V_HW                      0 "register_operand"  "")
+	(vec_duplicate:V_HW (vec_select:<non_vec>
+			     (match_operand:V_HW 1 "register_operand"  "")
+			     (parallel
+			      [(match_operand:QI 2 "const_mask_operand" "")]))))]
+  "TARGET_VX")
+
+; Vector scatter element
+
+; vscef, vsceg
+
+; A 64 bit target adress generated from 32 bit elements
+(define_insn "vec_scatter_elementv4si_DI"
+  [(set (mem:SI
+	 (plus:DI (zero_extend:DI
+		   (unspec:SI [(match_operand:V4SI 1 "register_operand"   "v")
+			       (match_operand:QI   3 "const_mask_operand" "C")]
+			      UNSPEC_VEC_EXTRACT))
+		  (match_operand:SI                2 "address_operand"   "ZQ")))
+	(unspec:SI [(match_operand:V4SI            0 "register_operand"   "v")
+		    (match_dup 3)] UNSPEC_VEC_EXTRACT))]
+  "TARGET_VX && TARGET_64BIT && UINTVAL (operands[3]) < 4"
+  "vscef\t%v0,%O2(%v1,%R2),%3"
+  [(set_attr "op_type" "VRV")])
+
+; A 31 bit target address is generated from 64 bit elements
+(define_insn "vec_scatter_element<V_HW_64:mode>_SI"
+  [(set (mem:<non_vec>
+	 (plus:SI (subreg:SI
+		   (unspec:<non_vec_int> [(match_operand:V_HW_64 1 "register_operand"   "v")
+					  (match_operand:QI      3 "const_mask_operand" "C")]
+					 UNSPEC_VEC_EXTRACT) 4)
+		  (match_operand:SI                              2 "address_operand"   "ZQ")))
+	(unspec:<non_vec> [(match_operand:V_HW_64                0 "register_operand"   "v")
+			   (match_dup 3)] UNSPEC_VEC_EXTRACT))]
+  "TARGET_VX && !TARGET_64BIT && UINTVAL (operands[3]) < GET_MODE_NUNITS (<V_HW_64:MODE>mode)"
+  "vsce<V_HW_64:bhfgq>\t%v0,%O2(%v1,%R2),%3"
+  [(set_attr "op_type" "VRV")])
+
+; Element size and target adress size is the same
+(define_insn "vec_scatter_element<mode>_<non_vec_int>"
+  [(set (mem:<non_vec>
+	 (plus:<non_vec_int> (unspec:<non_vec_int>
+			      [(match_operand:<tointvec> 1 "register_operand"   "v")
+			       (match_operand:QI         3 "const_mask_operand" "C")]
+			      UNSPEC_VEC_EXTRACT)
+			     (match_operand:DI           2 "address_operand"   "ZQ")))
+	(unspec:<non_vec> [(match_operand:V_HW_32_64     0 "register_operand"   "v")
+			   (match_dup 3)] UNSPEC_VEC_EXTRACT))]
+  "TARGET_VX && UINTVAL (operands[3]) < GET_MODE_NUNITS (<V_HW_32_64:MODE>mode)"
+  "vsce<bhfgq>\t%v0,%O2(%v1,%R2),%3"
+  [(set_attr "op_type" "VRV")])
+
+; Depending on the address size we have to expand a different pattern.
+; This however cannot be represented in s390-builtins.def so we do the
+; multiplexing here in the expander.
+(define_expand "vec_scatter_element<V_HW_32_64:mode>"
+  [(match_operand:V_HW_32_64 0 "register_operand" "")
+   (match_operand:<tointvec> 1 "register_operand" "")
+   (match_operand 2 "address_operand" "")
+   (match_operand:QI 3 "const_mask_operand" "")]
+  "TARGET_VX"
+{
+  if (TARGET_64BIT)
+    {
+      PUT_MODE (operands[2], DImode);
+      emit_insn (
+	gen_vec_scatter_element<V_HW_32_64:mode>_DI (operands[0], operands[1],
+						     operands[2], operands[3]));
+    }
+  else
+    {
+      PUT_MODE (operands[2], SImode);
+      emit_insn (
+	gen_vec_scatter_element<V_HW_32_64:mode>_SI (operands[0], operands[1],
+						     operands[2], operands[3]));
+    }
+  DONE;
+})
+
+
+; Vector select
+
+; Operand 3 selects bits from either OP1 (0) or OP2 (1)
+
+; Comparison operator should not matter as long as we always use the same ?!
+
+; Operands 1 and 2 are swapped in order to match the altivec builtin.
+; If operand 3 is a const_int bitmask this would be vec_merge
+(define_expand "vec_sel<mode>"
+  [(set (match_operand:V_HW 0 "register_operand" "")
+	(if_then_else:V_HW
+	 (eq (match_operand:<tointvec> 3 "register_operand"  "")
+	     (match_dup 4))
+	 (match_operand:V_HW 2 "register_operand"  "")
+	 (match_operand:V_HW 1 "register_operand"  "")))]
+  "TARGET_VX"
+{
+  operands[4] = CONST0_RTX (<tointvec>mode);
+})
+
+
+; Vector sign extend to doubleword
+
+; Sign extend of right most vector element to respective double-word
+(define_insn "vec_extend<mode>"
+  [(set (match_operand:VI_HW_QHS                    0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"  "v")]
+			  UNSPEC_VEC_EXTEND))]
+  "TARGET_VX"
+  "vseg<bhfgq>\t%v0,%1"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector store with length
+
+; Store bytes in OP1 from OP0 with the highest indexed byte to be
+; stored from OP0 given by OP2
+(define_insn "vstl<mode>"
+  [(set (match_operand:BLK             2 "memory_operand"   "=Q")
+	(unspec:BLK [(match_operand:V  0 "register_operand"  "v")
+		     (match_operand:SI 1 "register_operand"  "d")]
+		    UNSPEC_VEC_STORE_LEN))]
+  "TARGET_VX"
+  "vstl\t%v0,%1,%2"
+  [(set_attr "op_type" "VRS")])
+
+
+; Vector unpack high
+
+; vuphb, vuphh, vuphf
+(define_insn "vec_unpackh<mode>"
+  [(set (match_operand:<vec_double>                    0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand"  "v")]
+			     UNSPEC_VEC_UNPACKH))]
+  "TARGET_VX"
+  "vuph<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; vuplhb, vuplhh, vuplhf
+(define_insn "vec_unpackh_l<mode>"
+  [(set (match_operand:<vec_double>                    0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand"  "v")]
+			     UNSPEC_VEC_UNPACKH_L))]
+  "TARGET_VX"
+  "vuplh<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector unpack low
+
+; vuplb, vuplhw, vuplf
+(define_insn "vec_unpackl<mode>"
+  [(set (match_operand:<vec_double>                    0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand"  "v")]
+			     UNSPEC_VEC_UNPACKL))]
+  "TARGET_VX"
+  "vupl<bhfgq><w>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; vupllb, vupllh, vupllf
+(define_insn "vec_unpackl_l<mode>"
+  [(set (match_operand:<vec_double>                    0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand"  "v")]
+			     UNSPEC_VEC_UNPACKL_L))]
+  "TARGET_VX"
+  "vupll<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector add
+
+; vaq
+
+; zvector builtins uses V16QI operands.  So replace the modes in order
+; to map this to a TImode add.  We have to keep the V16QI mode
+; operands in the expander in order to allow some operand type
+; checking when expanding the builtin.
+(define_expand "vec_add_u128"
+  [(match_operand:V16QI 0 "register_operand" "")
+   (match_operand:V16QI 1 "register_operand" "")
+   (match_operand:V16QI 2 "register_operand" "")]
+  "TARGET_VX"
+{
+  rtx op0 = gen_rtx_SUBREG (TImode, operands[0], 0);
+  rtx op1 = gen_rtx_SUBREG (TImode, operands[1], 0);
+  rtx op2 = gen_rtx_SUBREG (TImode, operands[2], 0);
+
+  emit_insn (gen_rtx_SET (VOIDmode, op0,
+			  gen_rtx_PLUS (TImode, op1, op2)));
+  DONE;
+})
+
+; Vector add compute carry
+
+(define_insn "vec_addc<mode>"
+  [(set (match_operand:VI_HW                0 "register_operand" "=v")
+	(unspec:VI_HW [(match_operand:VI_HW 1 "register_operand" "%v")
+		       (match_operand:VI_HW 2 "register_operand"  "v")]
+		      UNSPEC_VEC_ADDC))]
+  "TARGET_VX"
+  "vacc<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_addc_u128"
+  [(set (match_operand:V16QI                0 "register_operand" "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "%v")
+		       (match_operand:V16QI 2 "register_operand"  "v")]
+		      UNSPEC_VEC_ADDC_U128))]
+  "TARGET_VX"
+  "vaccq\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector add with carry
+
+(define_insn "vec_adde_u128"
+  [(set (match_operand:V16QI                0 "register_operand" "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "%v")
+		       (match_operand:V16QI 2 "register_operand"  "v")
+		       (match_operand:V16QI 3 "register_operand"  "v")]
+		      UNSPEC_VEC_ADDE_U128))]
+  "TARGET_VX"
+  "vacq\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector add with carry compute carry
+
+(define_insn "vec_addec_u128"
+  [(set (match_operand:V16QI                0 "register_operand" "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "%v")
+		       (match_operand:V16QI 2 "register_operand"  "v")
+		       (match_operand:V16QI 3 "register_operand"  "v")]
+		      UNSPEC_VEC_ADDEC_U128))]
+  "TARGET_VX"
+  "vacccq\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector and
+
+; The following two patterns allow mixed mode and's as required for the intrinsics.
+(define_insn "and_av2df3"
+  [(set (match_operand:V2DF                        0 "register_operand" "=v")
+	(and:V2DF (subreg:V2DF (match_operand:V2DI 1 "register_operand"  "v") 0)
+		  (match_operand:V2DF              2 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vn\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "and_cv2df3"
+  [(set (match_operand:V2DF                        0 "register_operand" "=v")
+	(and:V2DF (match_operand:V2DF              1 "register_operand"  "v")
+		  (subreg:V2DF (match_operand:V2DI 2 "register_operand"  "v") 0)))]
+  "TARGET_VX"
+  "vn\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector and with complement
+
+; vnc
+(define_insn "vec_andc<mode>3"
+  [(set (match_operand:VT_HW                       0 "register_operand" "=v")
+	(and:VT_HW (not:VT_HW (match_operand:VT_HW 2 "register_operand"  "v"))
+		  (match_operand:VT_HW             1 "register_operand"  "v")))]
+  "TARGET_VX"
+  "vnc\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; The following two patterns allow mixed mode and's as required for the intrinsics.
+(define_insn "vec_andc_av2df3"
+  [(set (match_operand:V2DF                        0 "register_operand" "=v")
+	(and:V2DF (not:V2DF (match_operand:V2DF    2 "register_operand"  "v"))
+		  (subreg:V2DF (match_operand:V2DI 1 "register_operand"  "v") 0)))]
+
+  "TARGET_VX"
+  "vnc\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_andc_cv2df3"
+  [(set (match_operand:V2DF 0 "register_operand" "=v")
+	(and:V2DF (not:V2DF (subreg:V2DF (match_operand:V2DI 2 "register_operand" "v") 0))
+		  (match_operand:V2DF 1 "register_operand" "v")))]
+  "TARGET_VX"
+  "vnc\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector average
+
+(define_insn "vec_avg<mode>"
+  [(set (match_operand:VI_HW                0 "register_operand" "=v")
+	(unspec:VI_HW [(match_operand:VI_HW 1 "register_operand" "%v")
+		       (match_operand:VI_HW 2 "register_operand"  "v")]
+		      UNSPEC_VEC_AVG))]
+  "TARGET_VX"
+  "vavg<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; Vector average logical
+
+(define_insn "vec_avgu<mode>"
+  [(set (match_operand:VI_HW                0 "register_operand" "=v")
+	(unspec:VI_HW [(match_operand:VI_HW 1 "register_operand" "%v")
+		       (match_operand:VI_HW 2 "register_operand"  "v")]
+		      UNSPEC_VEC_AVGU))]
+  "TARGET_VX"
+  "vavgl<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector checksum
+
+(define_insn "vec_checksum"
+  [(set (match_operand:V4SI               0 "register_operand" "=v")
+	(unspec:V4SI [(match_operand:V4SI 1 "register_operand"  "v")
+		      (match_operand:V4SI 2 "register_operand"  "v")]
+		     UNSPEC_VEC_CHECKSUM))]
+  "TARGET_VX"
+  "vcksm\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+;;
+;; Vector compare
+;;
+
+; vec_all/any int compares
+
+(define_expand "vec_all_<intcmpcc:code><VI_HW:mode>"
+  [(match_operand:SI                0 "register_operand" "")
+   (intcmpcc (match_operand:VI_HW 1 "register_operand" "")
+	     (match_operand:VI_HW 2 "register_operand" ""))]
+  "TARGET_VX"
+{
+  s390_expand_vec_compare_cc (operands[0],
+			      <intcmpcc:CODE>,
+			      operands[1],
+			      operands[2],
+			      true);
+  DONE;
+})
+
+(define_expand "vec_any_<intcmpcc:code><VI_HW:mode>"
+  [(match_operand:SI                0 "register_operand" "")
+   (intcmpcc (match_operand:VI_HW 1 "register_operand" "")
+	     (match_operand:VI_HW 2 "register_operand" ""))]
+  "TARGET_VX"
+{
+  s390_expand_vec_compare_cc (operands[0],
+			      <intcmpcc:CODE>,
+			      operands[1],
+			      operands[2],
+			      false);
+  DONE;
+})
+
+; vec_all/any fp compares
+
+(define_expand "vec_all_<fpcmpcc:code>v2df"
+  [(match_operand:SI            0 "register_operand" "")
+   (fpcmpcc (match_operand:V2DF 1 "register_operand" "")
+	    (match_operand:V2DF 2 "register_operand" ""))]
+  "TARGET_VX"
+{
+  s390_expand_vec_compare_cc (operands[0],
+			      <fpcmpcc:CODE>,
+			      operands[1],
+			      operands[2],
+			      true);
+  DONE;
+})
+
+(define_expand "vec_any_<fpcmpcc:code>v2df"
+  [(match_operand:SI            0 "register_operand" "")
+   (fpcmpcc (match_operand:V2DF 1 "register_operand" "")
+	    (match_operand:V2DF 2 "register_operand" ""))]
+  "TARGET_VX"
+{
+  s390_expand_vec_compare_cc (operands[0],
+			      <fpcmpcc:CODE>,
+			      operands[1],
+			      operands[2],
+			      false);
+  DONE;
+})
+
+
+; Compare without generating CC
+
+(define_expand "vec_cmp<intcmp:code><VI_HW:mode>"
+  [(set (match_operand:VI_HW               0 "register_operand" "=v")
+	(intcmp:VI_HW (match_operand:VI_HW 1 "register_operand"  "v")
+		      (match_operand:VI_HW 2 "register_operand"  "v")))]
+  "TARGET_VX"
+{
+  s390_expand_vec_compare (operands[0], <intcmp:CODE>, operands[1], operands[2]);
+  DONE;
+})
+
+(define_expand "vec_cmp<fpcmp:code>v2df"
+  [(set (match_operand:V2DI             0 "register_operand" "=v")
+	(fpcmp:V2DI (match_operand:V2DF 1 "register_operand"  "v")
+		    (match_operand:V2DF 2 "register_operand"  "v")))]
+  "TARGET_VX"
+{
+  s390_expand_vec_compare (operands[0], <fpcmp:CODE>, operands[1], operands[2]);
+  DONE;
+})
+
+
+; Vector count leading zeros
+
+; vec_cntlz -> clz
+; vec_cnttz -> ctz
+
+; Vector xor
+
+; vec_xor -> xor
+
+; The following two patterns allow mixed mode xor's as required for the intrinsics.
+(define_insn "xor_av2df3"
+  [(set (match_operand:V2DF 0 "register_operand" "=v")
+	(xor:V2DF (subreg:V2DF (match_operand:V2DI 1 "register_operand" "v") 0)
+		  (match_operand:V2DF 2 "register_operand" "v")))]
+  "TARGET_VX"
+  "vx\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "xor_cv2df3"
+  [(set (match_operand:V2DF 0 "register_operand" "=v")
+	(xor:V2DF (match_operand:V2DF 1 "register_operand" "v")
+		  (subreg:V2DF (match_operand:V2DI 2 "register_operand" "v") 0)))]
+  "TARGET_VX"
+  "vx\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector Galois field multiply sum
+
+(define_insn "vec_gfmsum<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "v")]
+			  UNSPEC_VEC_GFMSUM))]
+  "TARGET_VX"
+  "vgfm<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_gfmsum_128"
+  [(set (match_operand:V16QI 0 "register_operand" "=v")
+	(unspec:V16QI [(match_operand:V2DI 1 "register_operand" "v")
+		       (match_operand:V2DI 2 "register_operand" "v")]
+		      UNSPEC_VEC_GFMSUM_128))]
+  "TARGET_VX"
+  "vgfmg\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_gfmsum_accum<mode>"
+  [(set (match_operand:<vec_double> 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand" "v")
+			      (match_operand:VI_HW_QHS 2 "register_operand" "v")
+			      (match_operand:<vec_double> 3 "register_operand" "v")]
+			     UNSPEC_VEC_GFMSUM_ACCUM))]
+  "TARGET_VX"
+  "vgfma<bhfgq>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_gfmsum_accum_128"
+  [(set (match_operand:V16QI 0 "register_operand" "=v")
+	(unspec:V16QI [(match_operand:V2DI 1 "register_operand" "v")
+		       (match_operand:V2DI 2 "register_operand" "v")
+		       (match_operand:V16QI 3 "register_operand" "v")]
+		      UNSPEC_VEC_GFMSUM_ACCUM_128))]
+  "TARGET_VX"
+  "vgfmag\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+
+; FIXME: vec_neg ?
+
+; Vector load positive: vec_abs -> abs
+; Vector maximum vec_max -> smax, logical vec_max -> umax
+; Vector maximum vec_min -> smin, logical vec_min -> umin
+
+
+; Vector multiply and add high
+
+; vec_mladd -> vec_vmal
+; vmalb, vmalh, vmalf, vmalg
+(define_insn "vec_vmal<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "%v")
+			   (match_operand:VI_HW_QHS 2 "register_operand"  "v")
+			   (match_operand:VI_HW_QHS 3 "register_operand"  "v")]
+			  UNSPEC_VEC_VMAL))]
+  "TARGET_VX"
+  "vmal<bhfgq><w>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+; vec_mhadd -> vec_vmah/vec_vmalh
+
+; vmahb; vmahh, vmahf, vmahg
+(define_insn "vec_vmah<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "%v")
+			   (match_operand:VI_HW_QHS 2 "register_operand"  "v")
+			   (match_operand:VI_HW_QHS 3 "register_operand"  "v")]
+			  UNSPEC_VEC_VMAH))]
+  "TARGET_VX"
+  "vmah<bhfgq>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+; vmalhb; vmalhh, vmalhf, vmalhg
+(define_insn "vec_vmalh<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "%v")
+			   (match_operand:VI_HW_QHS 2 "register_operand"  "v")
+			   (match_operand:VI_HW_QHS 3 "register_operand"  "v")]
+			  UNSPEC_VEC_VMALH))]
+  "TARGET_VX"
+  "vmalh<bhfgq>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+; vec_meadd -> vec_vmae/vec_vmale
+
+; vmaeb; vmaeh, vmaef, vmaeg
+(define_insn "vec_vmae<mode>"
+  [(set (match_operand:<vec_double> 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand"   "%v")
+			      (match_operand:VI_HW_QHS 2 "register_operand"    "v")
+			      (match_operand:<vec_double> 3 "register_operand" "v")]
+			     UNSPEC_VEC_VMAE))]
+  "TARGET_VX"
+  "vmae<bhfgq>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+; vmaleb; vmaleh, vmalef, vmaleg
+(define_insn "vec_vmale<mode>"
+  [(set (match_operand:<vec_double> 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand" "%v")
+			      (match_operand:VI_HW_QHS 2 "register_operand" "v")
+			      (match_operand:<vec_double> 3 "register_operand" "v")]
+			     UNSPEC_VEC_VMALE))]
+  "TARGET_VX"
+  "vmale<bhfgq>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+; vec_moadd -> vec_vmao/vec_vmalo
+
+; vmaob; vmaoh, vmaof, vmaog
+(define_insn "vec_vmao<mode>"
+  [(set (match_operand:<vec_double> 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand" "%v")
+			      (match_operand:VI_HW_QHS 2 "register_operand" "v")
+			      (match_operand:<vec_double> 3 "register_operand" "v")]
+			     UNSPEC_VEC_VMAO))]
+  "TARGET_VX"
+  "vmao<bhfgq>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+; vmalob; vmaloh, vmalof, vmalog
+(define_insn "vec_vmalo<mode>"
+  [(set (match_operand:<vec_double> 0 "register_operand" "=v")
+	(unspec:<vec_double> [(match_operand:VI_HW_QHS 1 "register_operand" "v")
+			      (match_operand:VI_HW_QHS 2 "register_operand" "v")
+			      (match_operand:<vec_double> 3 "register_operand" "v")]
+			     UNSPEC_VEC_VMALO))]
+  "TARGET_VX"
+  "vmalo<bhfgq>\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector multiply high
+
+; vec_mulh -> vec_smulh/vec_umulh
+
+; vmhb, vmhh, vmhf
+(define_insn "vec_smulh<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "%v")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "v")]
+			  UNSPEC_VEC_SMULT_HI))]
+  "TARGET_VX"
+  "vmh<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vmlhb, vmlhh, vmlhf
+(define_insn "vec_umulh<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "%v")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "v")]
+			  UNSPEC_VEC_UMULT_HI))]
+  "TARGET_VX"
+  "vmlh<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector multiply low
+
+; vec_mule -> vec_widen_umult_even/vec_widen_smult_even
+; vec_mulo -> vec_widen_umult_odd/vec_widen_smult_odd
+
+
+; Vector nor
+
+(define_insn "vec_nor<mode>3"
+  [(set (match_operand:VT_HW 0 "register_operand" "=v")
+	(not:VT_HW (ior:VT_HW (match_operand:VT_HW 1 "register_operand" "%v")
+			      (match_operand:VT_HW 2 "register_operand" "v"))))]
+  "TARGET_VX"
+  "vno\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; The following two patterns allow mixed mode and's as required for the intrinsics.
+(define_insn "vec_nor_av2df3"
+  [(set (match_operand:V2DF 0 "register_operand" "=v")
+	(not:V2DF (ior:V2DF (subreg:V2DF (match_operand:V2DI 1 "register_operand" "v") 0)
+			    (match_operand:V2DF 2 "register_operand" "v"))))]
+  "TARGET_VX"
+  "vno\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_nor_cv2df3"
+  [(set (match_operand:V2DF 0 "register_operand" "=v")
+	(not:V2DF (ior:V2DF (match_operand:V2DF 1 "register_operand" "v")
+			    (subreg:V2DF (match_operand:V2DI 2 "register_operand" "v") 0))))]
+  "TARGET_VX"
+  "vno\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector or
+
+; The following two patterns allow mixed mode or's as required for the intrinsics.
+(define_insn "ior_av2df3"
+  [(set (match_operand:V2DF 0 "register_operand" "=v")
+	(ior:V2DF (subreg:V2DF (match_operand:V2DI 1 "register_operand" "v") 0)
+		  (match_operand:V2DF 2 "register_operand" "v")))]
+  "TARGET_VX"
+  "vo\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "ior_cv2df3"
+  [(set (match_operand:V2DF 0 "register_operand" "=v")
+	(ior:V2DF (match_operand:V2DF 1 "register_operand" "v")
+		  (subreg:V2DF (match_operand:V2DI 2 "register_operand" "v") 0)))]
+  "TARGET_VX"
+  "vo\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector population count vec_popcnt -> popcount
+; Vector element rotate left logical vec_rl -> vrotl, vec_rli -> rot
+
+; Vector element rotate and insert under mask
+
+; verimb, verimh, verimf, verimg
+(define_insn "verim<mode>"
+  [(set (match_operand:VI_HW                0 "register_operand" "=v")
+	(unspec:VI_HW [(match_operand:VI_HW 1 "register_operand"  "0")
+		       (match_operand:VI_HW 2 "register_operand"  "v")
+		       (match_operand:VI_HW 3 "register_operand"  "v")
+		       (match_operand:QI    4 "const_int_operand" "C")]
+		      UNSPEC_VEC_RL_MASK))]
+  "TARGET_VX"
+  "verim<bhfgq>\t%v0,%v2,%v3,%b4"
+  [(set_attr "op_type" "VRI")])
+
+
+; Vector shift left
+
+(define_insn "vec_sll<VI_HW:mode><VI_HW_QHS:mode>"
+  [(set (match_operand:VI_HW                    0 "register_operand" "=v")
+	(unspec:VI_HW [(match_operand:VI_HW     1 "register_operand"  "v")
+		       (match_operand:VI_HW_QHS 2 "register_operand"  "v")]
+		      UNSPEC_VEC_SLL))]
+  "TARGET_VX"
+  "vsl\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector shift left by byte
+
+(define_insn "vec_slb<mode>"
+  [(set (match_operand:V_HW 0 "register_operand"                    "=v")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand"       "v")
+		      (match_operand:<tointvec> 2 "register_operand" "v")]
+		     UNSPEC_VEC_SLB))]
+  "TARGET_VX"
+  "vslb\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector shift left double by byte
+
+(define_insn "vec_sld<mode>"
+  [(set (match_operand:V_HW 0 "register_operand"              "=v")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand" "v")
+		      (match_operand:V_HW 2 "register_operand" "v")
+		      (match_operand:QI 3 "const_int_operand"  "C")]
+		     UNSPEC_VEC_SLDB))]
+  "TARGET_VX"
+  "vsldb\t%v0,%v1,%v2,%b3"
+  [(set_attr "op_type" "VRI")])
+
+(define_expand "vec_sldw<mode>"
+  [(set (match_operand:V_HW 0 "register_operand"               "")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand" "")
+		      (match_operand:V_HW 2 "register_operand" "")
+		      (match_operand:QI 3 "const_int_operand"  "")]
+		     UNSPEC_VEC_SLDB))]
+  "TARGET_VX"
+{
+  operands[3] = GEN_INT (INTVAL (operands[3]) << 2);
+})
+
+; Vector shift right arithmetic
+
+(define_insn "vec_sral<VI_HW:mode><VI_HW_QHS:mode>"
+  [(set (match_operand:VI_HW                    0 "register_operand" "=v")
+	(unspec:VI_HW [(match_operand:VI_HW     1 "register_operand"  "v")
+		       (match_operand:VI_HW_QHS 2 "register_operand"  "v")]
+		      UNSPEC_VEC_SRAL))]
+  "TARGET_VX"
+  "vsra\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector shift right arithmetic by byte
+
+(define_insn "vec_srab<mode>"
+  [(set (match_operand:V_HW 0 "register_operand"                    "=v")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand"       "v")
+		      (match_operand:<tointvec> 2 "register_operand" "v")]
+		     UNSPEC_VEC_SRAB))]
+  "TARGET_VX"
+  "vsrab\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector shift right logical
+
+(define_insn "vec_srl<VI_HW:mode><VI_HW_QHS:mode>"
+  [(set (match_operand:VI_HW                    0 "register_operand" "=v")
+	(unspec:VI_HW [(match_operand:VI_HW     1 "register_operand"  "v")
+		       (match_operand:VI_HW_QHS 2 "register_operand"  "v")]
+		      UNSPEC_VEC_SRL))]
+  "TARGET_VX"
+  "vsrl\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector shift right logical by byte
+
+; Pattern definition in vector.md
+(define_expand "vec_srb<mode>"
+  [(set (match_operand:V_HW 0 "register_operand"                     "")
+	(unspec:V_HW [(match_operand:V_HW 1 "register_operand"       "")
+		      (match_operand:<tointvec> 2 "register_operand" "")]
+		     UNSPEC_VEC_SRLB))]
+  "TARGET_VX")
+
+
+; Vector subtract
+
+(define_insn "vec_sub_u128"
+  [(set (match_operand:V16QI 0 "register_operand"               "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "v")
+		       (match_operand:V16QI 2 "register_operand" "v")]
+		     UNSPEC_VEC_SUB_U128))]
+  "TARGET_VX"
+  "vsq\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector subtract compute borrow indication
+
+(define_insn "vec_subc<mode>"
+  [(set (match_operand:VI_HW 0 "register_operand"               "=v")
+	(unspec:VI_HW [(match_operand:VI_HW 1 "register_operand" "v")
+		       (match_operand:VI_HW 2 "register_operand" "v")]
+		      UNSPEC_VEC_SUBC))]
+  "TARGET_VX"
+  "vscbi<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "vec_subc_u128"
+  [(set (match_operand:V16QI 0 "register_operand"               "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "v")
+		       (match_operand:V16QI 2 "register_operand" "v")]
+		     UNSPEC_VEC_SUBC_U128))]
+  "TARGET_VX"
+  "vscbiq\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector subtract with borrow indication
+
+(define_insn "vec_sube_u128"
+  [(set (match_operand:V16QI 0 "register_operand"               "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "v")
+		       (match_operand:V16QI 2 "register_operand" "v")
+		       (match_operand:V16QI 3 "register_operand" "v")]
+		      UNSPEC_VEC_SUBE_U128))]
+  "TARGET_VX"
+  "vsbiq\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector subtract with borrow compute and borrow indication
+
+(define_insn "vec_subec_u128"
+  [(set (match_operand:V16QI 0 "register_operand"               "=v")
+	(unspec:V16QI [(match_operand:V16QI 1 "register_operand" "v")
+		       (match_operand:V16QI 2 "register_operand" "v")
+		       (match_operand:V16QI 3 "register_operand" "v")]
+		      UNSPEC_VEC_SUBEC_U128))]
+  "TARGET_VX"
+  "vsbcbiq\t%v0,%v1,%v2,%v3"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector sum across
+
+; Sum across DImode parts of the 1st operand and add the rightmost
+; element of 2nd operand
+; vsumgh, vsumgf
+(define_expand "vec_sum2<mode>"
+  [(set (match_operand:V2DI 0 "register_operand" "")
+	(unspec:V2DI [(match_operand:VI_HW_HS 1 "register_operand" "")
+		      (match_operand:VI_HW_HS 2 "register_operand" "")]
+		     UNSPEC_VEC_VSUMG))]
+  "TARGET_VX")
+
+; vsumqh, vsumqf
+(define_insn "vec_sum_u128<mode>"
+  [(set (match_operand:V2DI 0 "register_operand" "=v")
+	(unspec:V2DI [(match_operand:VI_HW_SD 1 "register_operand" "v")
+		      (match_operand:VI_HW_SD 2 "register_operand" "v")]
+		     UNSPEC_VEC_VSUMQ))]
+  "TARGET_VX"
+  "vsumq<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+; vsumb, vsumh
+(define_expand "vec_sum4<mode>"
+  [(set (match_operand:V4SI 0 "register_operand" "")
+	(unspec:V4SI [(match_operand:VI_HW_QH 1 "register_operand" "")
+		      (match_operand:VI_HW_QH 2 "register_operand" "")]
+		     UNSPEC_VEC_VSUM))]
+  "TARGET_VX")
+
+
+; Vector test under mask
+
+(define_expand "vec_test_mask_int<mode>"
+  [(set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_operand:V_HW 1 "register_operand" "")
+		       (match_operand:<tointvec> 2 "register_operand" "")]
+		      UNSPEC_VEC_TEST_MASK))
+   (set (match_operand:SI 0 "register_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+(define_insn "*vec_test_mask<mode>"
+  [(set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_operand:V_HW 0 "register_operand" "v")
+		       (match_operand:<tointvec> 1 "register_operand" "v")]
+		      UNSPEC_VEC_TEST_MASK))]
+  "TARGET_VX"
+  "vtm\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+
+; Vector find any element equal
+
+; vfaeb, vfaeh, vfaef
+; vfaezb, vfaezh, vfaezf
+(define_insn "vfae<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "v")
+			   (match_operand:QI        3 "const_mask_operand" "C")]
+			  UNSPEC_VEC_VFAE))]
+  "TARGET_VX"
+{
+  unsigned HOST_WIDE_INT flags = INTVAL (operands[3]);
+
+  if (flags & VSTRING_FLAG_ZS)
+    {
+      flags &= ~VSTRING_FLAG_ZS;
+      operands[3] = GEN_INT (flags);
+      return "vfaez<bhfgq>\t%v0,%v1,%v2,%b3";
+    }
+  return "vfae<bhfgq>\t%v0,%v1,%v2,%b3";
+}
+[(set_attr "op_type" "VRR")])
+
+; vfaebs, vfaehs, vfaefs
+; vfaezbs, vfaezhs, vfaezfs
+(define_insn "*vfaes<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"   "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand"   "v")
+			   (match_operand:QI        3 "const_mask_operand" "C")]
+			  UNSPEC_VEC_VFAE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)]
+		      UNSPEC_VEC_VFAECC))]
+  "TARGET_VX"
+{
+  unsigned HOST_WIDE_INT flags = INTVAL (operands[3]);
+
+  if (flags & VSTRING_FLAG_ZS)
+    {
+      flags &= ~VSTRING_FLAG_ZS;
+      operands[3] = GEN_INT (flags);
+      return "vfaez<bhfgq>s\t%v0,%v1,%v2,%b3";
+    }
+  return "vfae<bhfgq>s\t%v0,%v1,%v2,%b3";
+}
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vfaez<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"  "")
+			   (match_operand:VI_HW_QHS 2 "register_operand"  "")
+			   (match_operand:QI        3 "const_mask_operand" "")]
+			  UNSPEC_VEC_VFAE))]
+  "TARGET_VX"
+{
+  operands[3] = GEN_INT (INTVAL (operands[3]) | VSTRING_FLAG_ZS);
+})
+
+(define_expand "vfaes<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS 0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"  "")
+			   (match_operand:VI_HW_QHS 2 "register_operand"  "")
+			   (match_operand:QI        3 "const_mask_operand" "")]
+			  UNSPEC_VEC_VFAE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)]
+		      UNSPEC_VEC_VFAECC))])
+   (set (match_operand:SI 4 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX"
+{
+  operands[3] = GEN_INT (INTVAL (operands[3]) | VSTRING_FLAG_CS);
+})
+
+(define_expand "vfaezs<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS 0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"  "")
+			   (match_operand:VI_HW_QHS 2 "register_operand"  "")
+			   (match_operand:SI        3 "const_mask_operand" "")]
+			  UNSPEC_VEC_VFAE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)]
+		      UNSPEC_VEC_VFAECC))])
+   (set (match_operand:SI 4 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX"
+{
+  operands[3] = GEN_INT (INTVAL (operands[3]) | VSTRING_FLAG_CS | VSTRING_FLAG_ZS);
+})
+
+
+; Vector find element equal
+
+; vfeebs, vfeehs, vfeefs
+; vfeezbs, vfeezhs, vfeezfs
+(define_insn "*vfees<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "v")
+			   (match_operand:QI 3 "const_mask_operand" "C")]
+			  UNSPEC_VEC_VFEE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)]
+		      UNSPEC_VEC_VFEECC))]
+  "TARGET_VX"
+{
+  unsigned HOST_WIDE_INT flags = INTVAL (operands[3]);
+
+  gcc_assert (!(flags & ~(VSTRING_FLAG_ZS | VSTRING_FLAG_CS)));
+  flags &= ~VSTRING_FLAG_CS;
+
+  if (flags == VSTRING_FLAG_ZS)
+    return "vfeez<bhfgq>s\t%v0,%v1,%v2";
+  return "vfee<bhfgq>s\t%v0,%v1,%v2,%b3";
+}
+  [(set_attr "op_type" "VRR")])
+
+; vfeeb, vfeeh, vfeef
+(define_insn "vfee<mode>"
+  [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "")
+			   (const_int 0)]
+			  UNSPEC_VEC_VFEE))]
+  "TARGET_VX"
+  "vfee<bhfgq>\t%v0,%v1,%v2,0"
+  [(set_attr "op_type" "VRR")])
+
+; vfeezb, vfeezh, vfeezf
+(define_insn "vfeez<mode>"
+  [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "")
+			   (const_int VSTRING_FLAG_ZS)]
+			  UNSPEC_VEC_VFEE))]
+  "TARGET_VX"
+  "vfeez<bhfgq>s\t%v0,%v1,%v2,2"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vfees<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	  (unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			     (match_operand:VI_HW_QHS 2 "register_operand" "")
+			     (const_int VSTRING_FLAG_CS)]
+			    UNSPEC_VEC_VFEE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (const_int VSTRING_FLAG_CS)]
+		      UNSPEC_VEC_VFEECC))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+(define_expand "vfeezs<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	  (unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			     (match_operand:VI_HW_QHS 2 "register_operand" "")
+			     (match_dup 4)]
+			    UNSPEC_VEC_VFEE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 4)]
+		      UNSPEC_VEC_VFEECC))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX"
+{
+  operands[4] = GEN_INT (VSTRING_FLAG_ZS | VSTRING_FLAG_CS);
+})
+
+; Vector find element not equal
+
+; vfeneb, vfeneh, vfenef
+(define_insn "vfene<mode>"
+  [(set (match_operand:VI_HW_QHS                    0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"  "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand"  "v")
+			   (const_int 0)]
+			  UNSPEC_VEC_VFENE))]
+  "TARGET_VX"
+  "vfene<bhfgq>\t%v0,%v1,%v2,0"
+  [(set_attr "op_type" "VRR")])
+
+; vec_vfenes can be found in vector.md since it is used for strlen
+
+; vfenezb, vfenezh, vfenezf
+(define_insn "vfenez<mode>"
+  [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "")
+			   (const_int VSTRING_FLAG_ZS)]
+			  UNSPEC_VEC_VFENE))]
+  "TARGET_VX"
+  "vfenez<bhfgq>\t%v0,%v1,%v2"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vfenes<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	  (unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			     (match_operand:VI_HW_QHS 2 "register_operand" "")
+			     (const_int VSTRING_FLAG_CS)]
+			    UNSPEC_VEC_VFENE))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (const_int VSTRING_FLAG_CS)]
+		      UNSPEC_VEC_VFENECC))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+(define_expand "vfenezs<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	  (unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			     (match_operand:VI_HW_QHS 2 "register_operand" "")
+			     (match_dup 4)]
+			    UNSPEC_VEC_VFENE))
+     (set (reg:CCRAW CC_REGNUM)
+	  (unspec:CCRAW [(match_dup 1)
+			 (match_dup 2)
+			 (match_dup 4)]
+			UNSPEC_VEC_VFENECC))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX"
+{
+  operands[4] = GEN_INT (VSTRING_FLAG_ZS | VSTRING_FLAG_CS);
+})
+
+; Vector isolate string
+
+; vistrb, vistrh, vistrf
+(define_insn "vistr<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "v")]
+			  UNSPEC_VEC_VISTR))]
+  "TARGET_VX"
+  "vistr<bhfgq>\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; vistrbs, vistrhs, vistrfs
+(define_insn "*vistrs<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "v")]
+			  UNSPEC_VEC_VISTR))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)] UNSPEC_VEC_VISTRCC))]
+  "TARGET_VX"
+  "vistr<bhfgq>s\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vistrs<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS                    0 "register_operand" "")
+	  (unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")]
+			    UNSPEC_VEC_VISTR))
+     (set (reg:CCRAW CC_REGNUM)
+	  (unspec:CCRAW [(match_dup 1)]
+			UNSPEC_VEC_VISTRCC))])
+   (set (match_operand:SI 2 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+
+; Vector compare range
+
+; vstrcb, vstrch, vstrcf
+; vstrczb, vstrczh, vstrczf
+(define_insn "vstrc<mode>"
+  [(set (match_operand:VI_HW_QHS                    0 "register_operand"  "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"   "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand"   "v")
+			   (match_operand:VI_HW_QHS 3 "register_operand"   "v")
+			   (match_operand:QI        4 "const_mask_operand" "C")]
+			  UNSPEC_VEC_VSTRC))]
+  "TARGET_VX"
+{
+  unsigned HOST_WIDE_INT flags = INTVAL (operands[4]);
+
+  if (flags & VSTRING_FLAG_ZS)
+    {
+      flags &= ~VSTRING_FLAG_ZS;
+      operands[4] = GEN_INT (flags);
+      return "vstrcz<bhfgq>\t%v0,%v1,%v2,%v3,%b4";
+    }
+  return "vstrc<bhfgq>\t%v0,%v1,%v2,%v3,%b4";
+}
+[(set_attr "op_type" "VRR")])
+
+; vstrcbs, vstrchs, vstrcfs
+; vstrczbs, vstrczhs, vstrczfs
+(define_insn "*vstrcs<mode>"
+  [(set (match_operand:VI_HW_QHS                    0 "register_operand"  "=v")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"   "v")
+			   (match_operand:VI_HW_QHS 2 "register_operand"   "v")
+			   (match_operand:VI_HW_QHS 3 "register_operand"   "v")
+			   (match_operand:QI        4 "const_mask_operand" "C")]
+			  UNSPEC_VEC_VSTRC))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)
+		       (match_dup 4)]
+		      UNSPEC_VEC_VSTRCCC))]
+  "TARGET_VX"
+{
+  unsigned HOST_WIDE_INT flags = INTVAL (operands[4]);
+
+  if (flags & VSTRING_FLAG_ZS)
+    {
+      flags &= ~VSTRING_FLAG_ZS;
+      operands[4] = GEN_INT (flags);
+      return "vstrcz<bhfgq>s\t%v0,%v1,%v2,%v3,%b4";
+    }
+  return "vstrc<bhfgq>s\t%v0,%v1,%v2,%v3,%b4";
+}
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vstrcz<mode>"
+  [(set (match_operand:VI_HW_QHS 0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand"   "")
+			   (match_operand:VI_HW_QHS 2 "register_operand"   "")
+			   (match_operand:VI_HW_QHS 3 "register_operand"   "")
+			   (match_operand:QI        4 "const_mask_operand" "")]
+			  UNSPEC_VEC_VSTRC))]
+  "TARGET_VX"
+{
+  operands[4] = GEN_INT (INTVAL (operands[4]) | VSTRING_FLAG_ZS);
+})
+
+(define_expand "vstrcs<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS 0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "")
+			   (match_operand:VI_HW_QHS 3 "register_operand" "")
+			   (match_operand:QI        4 "const_mask_operand" "")]
+			  UNSPEC_VEC_VSTRC))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)
+		       (match_dup 4)]
+		      UNSPEC_VEC_VSTRCCC))])
+   (set (match_operand:SI 5 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX"
+{
+  operands[4] = GEN_INT (INTVAL (operands[4]) | VSTRING_FLAG_CS);
+})
+
+(define_expand "vstrczs<mode>"
+  [(parallel
+    [(set (match_operand:VI_HW_QHS 0 "register_operand" "")
+	(unspec:VI_HW_QHS [(match_operand:VI_HW_QHS 1 "register_operand" "")
+			   (match_operand:VI_HW_QHS 2 "register_operand" "")
+			   (match_operand:VI_HW_QHS 3 "register_operand" "")
+			   (match_operand:QI        4 "const_mask_operand" "")]
+			  UNSPEC_VEC_VSTRC))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1)
+		       (match_dup 2)
+		       (match_dup 3)
+		       (match_dup 4)]
+		      UNSPEC_VEC_VSTRCCC))])
+   (set (match_operand:SI 5 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX"
+{
+  operands[4] = GEN_INT (INTVAL (operands[4]) | VSTRING_FLAG_CS | VSTRING_FLAG_ZS);
+})
+
+
+; Signed V2DI -> V2DF conversion - inexact exception disabled
+(define_insn "vec_di_to_df_s64"
+  [(set (match_operand:V2DF 0 "register_operand"                "=v")
+	(unspec:V2DF [(match_operand:V2DI 1 "register_operand"   "v")
+		      (match_operand:QI   2 "const_mask_operand" "C")]
+		     UNSPEC_VEC_VCDGB))]
+  "TARGET_VX && UINTVAL (operands[2]) != 2 && UINTVAL (operands[2]) <= 7"
+  "vcdgb\t%v0,%v1,4,%b2"
+  [(set_attr "op_type" "VRR")])
+
+; The result needs to be multiplied with 2**-op2
+(define_expand "vec_ctd_s64"
+  [(set (match_operand:V2DF               0 "register_operand" "")
+	(unspec:V2DF [(match_operand:V2DI 1 "register_operand" "")
+		      (const_int 0)] ; According to current BFP rounding mode
+		     UNSPEC_VEC_VCDGB))
+   (use (match_operand:QI 2 "const_int_operand" ""))
+   (set (match_dup 0) (mult:V2DF (match_dup 0) (match_dup 3)))]
+  "TARGET_VX"
+{
+  REAL_VALUE_TYPE f;
+  rtx c;
+
+  real_2expN (&f, -INTVAL (operands[2]), DFmode);
+  c = CONST_DOUBLE_FROM_REAL_VALUE (f, DFmode);
+
+  operands[3] = gen_rtx_CONST_VECTOR (V2DFmode, gen_rtvec (2, c, c));
+  operands[3] = force_reg (V2DFmode, operands[3]);
+})
+
+; Unsigned V2DI -> V2DF conversion - inexact exception disabled
+(define_insn "vec_di_to_df_u64"
+  [(set (match_operand:V2DF 0 "register_operand"               "=v")
+	(unspec:V2DF [(match_operand:V2DI 1 "register_operand"  "v")
+		      (match_operand:QI   2 "const_int_operand" "C")]
+		     UNSPEC_VEC_VCDLGB))]
+  "TARGET_VX"
+  "vcdlgb\t%v0,%v1,4,%b2"
+  [(set_attr "op_type" "VRR")])
+
+; The result needs to be multiplied with 2**-op2
+(define_expand "vec_ctd_u64"
+  [(set (match_operand:V2DF               0 "register_operand" "")
+	(unspec:V2DF [(match_operand:V2DI 1 "register_operand" "")
+		      (const_int 0)] ; According to current BFP rounding mode
+		     UNSPEC_VEC_VCDLGB))
+   (use (match_operand:QI 2 "const_int_operand" ""))
+   (set (match_dup 0) (mult:V2DF (match_dup 0) (match_dup 3)))]
+  "TARGET_VX"
+{
+  REAL_VALUE_TYPE f;
+  rtx c;
+
+  real_2expN (&f, -INTVAL (operands[2]), DFmode);
+  c = CONST_DOUBLE_FROM_REAL_VALUE (f, DFmode);
+
+  operands[3] = gen_rtx_CONST_VECTOR (V2DFmode, gen_rtvec (2, c, c));
+  operands[3] = force_reg (V2DFmode, operands[3]);
+})
+
+
+; Signed V2DF -> V2DI conversion - inexact exception disabled
+(define_insn "vec_df_to_di_s64"
+  [(set (match_operand:V2DI 0 "register_operand"               "=v")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand"  "v")
+		      (match_operand:QI   2 "const_int_operand" "C")]
+		     UNSPEC_VEC_VCGDB))]
+  "TARGET_VX"
+  "vcgdb\t%v0,%v1,4,%b2"
+  [(set_attr "op_type" "VRR")])
+
+; The input needs to be multiplied with 2**op2
+(define_expand "vec_ctsl"
+  [(use (match_operand:QI 2 "const_int_operand" ""))
+   (set (match_dup 4) (mult:V2DF (match_operand:V2DF 1 "register_operand" "")
+				 (match_dup 3)))
+   (set (match_operand:V2DI 0 "register_operand" "")
+	(unspec:V2DI [(match_dup 4) (const_int 0)] ; According to current BFP rounding mode
+		     UNSPEC_VEC_VCGDB))]
+  "TARGET_VX"
+{
+  REAL_VALUE_TYPE f;
+  rtx c;
+
+  real_2expN (&f, INTVAL (operands[2]), DFmode);
+  c = CONST_DOUBLE_FROM_REAL_VALUE (f, DFmode);
+
+  operands[3] = gen_rtx_CONST_VECTOR (V2DFmode, gen_rtvec (2, c, c));
+  operands[3] = force_reg (V2DFmode, operands[3]);
+  operands[4] = gen_reg_rtx (V2DFmode);
+})
+
+; Unsigned V2DF -> V2DI conversion - inexact exception disabled
+(define_insn "vec_df_to_di_u64"
+  [(set (match_operand:V2DI 0 "register_operand"               "=v")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand"  "v")
+		      (match_operand:QI   2 "const_mask_operand" "C")]
+		     UNSPEC_VEC_VCLGDB))]
+  "TARGET_VX && UINTVAL (operands[2]) <= 7"
+  "vclgdb\t%v0,%v1,4,%b2"
+  [(set_attr "op_type" "VRR")])
+
+; The input needs to be multiplied with 2**op2
+(define_expand "vec_ctul"
+  [(use (match_operand:QI 2 "const_int_operand" ""))
+   (set (match_dup 4) (mult:V2DF (match_operand:V2DF 1 "register_operand" "")
+				 (match_dup 3)))
+   (set (match_operand:V2DI 0 "register_operand" "")
+	(unspec:V2DI [(match_dup 4) (const_int 0)] ; According to current BFP rounding mode
+		     UNSPEC_VEC_VCLGDB))]
+  "TARGET_VX"
+{
+  REAL_VALUE_TYPE f;
+  rtx c;
+
+  real_2expN (&f, INTVAL (operands[2]), DFmode);
+  c = CONST_DOUBLE_FROM_REAL_VALUE (f, DFmode);
+
+  operands[3] = gen_rtx_CONST_VECTOR (V2DFmode, gen_rtvec (2, c, c));
+  operands[3] = force_reg (V2DFmode, operands[3]);
+  operands[4] = gen_reg_rtx (V2DFmode);
+})
+
+; Vector load fp integer - IEEE inexact exception is suppressed
+(define_insn "vfidb"
+  [(set (match_operand:V2DI               0 "register_operand"  "=v")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand"   "v")
+		      (match_operand:QI   2 "const_mask_operand" "C")
+		      (match_operand:QI   3 "const_mask_operand" "C")]
+		     UNSPEC_VEC_VFIDB))]
+  "TARGET_VX && !(UINTVAL (operands[2]) & 3) && UINTVAL (operands[3]) <= 7"
+  "vfidb\t%v0,%v1,%b2,%b3"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vec_ceil"
+  [(set (match_operand:V2DI               0 "register_operand" "")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand" "")
+		      (const_int VEC_RND_TO_INF)]
+		     UNSPEC_VEC_VFIDB))]
+  "TARGET_VX")
+
+(define_expand "vec_floor"
+  [(set (match_operand:V2DI               0 "register_operand" "")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand" "")
+		      (const_int VEC_RND_TO_MINF)]
+		     UNSPEC_VEC_VFIDB))]
+  "TARGET_VX")
+
+(define_expand "vec_trunc"
+  [(set (match_operand:V2DI               0 "register_operand" "")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand" "")
+		      (const_int VEC_RND_TO_ZERO)]
+		     UNSPEC_VEC_VFIDB))]
+  "TARGET_VX")
+
+(define_expand "vec_roundc"
+  [(set (match_operand:V2DI               0 "register_operand" "")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand" "")
+		      (const_int VEC_RND_CURRENT)]
+		     UNSPEC_VEC_VFIDB))]
+  "TARGET_VX")
+
+(define_expand "vec_round"
+  [(set (match_operand:V2DI               0 "register_operand" "")
+	(unspec:V2DI [(match_operand:V2DF 1 "register_operand" "")
+		      (const_int VEC_RND_NEAREST_TO_EVEN)]
+		     UNSPEC_VEC_VFIDB))]
+  "TARGET_VX")
+
+
+; Vector load lengthened - V4SF -> V2DF
+
+(define_insn "*vldeb"
+  [(set (match_operand:V2DF 0 "register_operand"               "=v")
+	(unspec:V2DF [(match_operand:V4SF 1 "register_operand"  "v")]
+		     UNSPEC_VEC_VLDEB))]
+  "TARGET_VX"
+  "vldeb\t%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vec_ld2f"
+  [; Initialize a vector to all zeroes.  FIXME: This should not be
+   ; necessary since all elements of the vector will be set anyway.
+   ; This is just to make it explicit to the data flow framework.
+   (set (match_dup 2) (match_dup 3))
+   (set (match_dup 2) (unspec:V4SF [(match_operand:SF 1 "memory_operand" "")
+				    (const_int 0)
+				    (match_dup 2)]
+				    UNSPEC_VEC_SET))
+   (set (match_dup 2) (unspec:V4SF [(match_dup 4)
+				    (const_int 2)
+				    (match_dup 2)]
+				    UNSPEC_VEC_SET))
+   (set (match_operand:V2DF 0 "register_operand" "")
+	(unspec:V2DF [(match_dup 2)] UNSPEC_VEC_VLDEB))]
+  "TARGET_VX"
+{
+  operands[2] = gen_reg_rtx (V4SFmode);
+  operands[3] = CONST0_RTX (V4SFmode);
+  operands[4] = adjust_address (operands[1], SFmode, 4);
+})
+
+
+; Vector load rounded - V2DF -> V4SF
+
+(define_insn "*vledb"
+  [(set (match_operand:V4SF 0 "register_operand"               "=v")
+	(unspec:V4SF [(match_operand:V2DF 1 "register_operand"  "v")]
+		     UNSPEC_VEC_VLEDB))]
+  "TARGET_VX"
+  "vledb\t%v0,%v1,0,0"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vec_st2f"
+  [(set (match_dup 2)
+	(unspec:V4SF [(match_operand:V2DF 0 "register_operand" "")]
+		     UNSPEC_VEC_VLEDB))
+   (set (match_operand:SF 1 "memory_operand" "")
+	(unspec:SF [(match_dup 2) (const_int 0)] UNSPEC_VEC_EXTRACT))
+   (set (match_dup 3)
+	(unspec:SF [(match_dup 2) (const_int 2)] UNSPEC_VEC_EXTRACT))]
+  "TARGET_VX"
+{
+  operands[2] = gen_reg_rtx (V4SFmode);
+  operands[3] = adjust_address (operands[1], SFmode, 4);
+})
+
+
+; Vector load negated fp
+
+(define_expand "vec_nabs"
+  [(set (match_operand:V2DF 0 "register_operand" "")
+	(neg:V2DF (abs:V2DF (match_operand:V2DF 1 "register_operand" ""))))]
+  "TARGET_VX")
+
+; Vector square root fp vec_sqrt -> sqrt rtx standard name
+
+; Vector FP test data class immediate
+
+(define_insn "*vftcidb"
+  [(set (match_operand:V2DF 0 "register_operand"  "=v")
+	(unspec:V2DF [(match_operand:V2DF 1 "register_operand"  "v")
+		      (match_operand:HI   2 "const_int_operand" "J")]
+		     UNSPEC_VEC_VFTCIDB))
+   (set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_dup 1) (match_dup 2)] UNSPEC_VEC_VFTCIDBCC))]
+  "TARGET_VX && CONST_OK_FOR_CONSTRAINT_P (INTVAL (operands[2]), 'J', \"J\")"
+  "vftcidb\t%v0,%v1,%x2"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "*vftcidb_cconly"
+  [(set (reg:CCRAW CC_REGNUM)
+	(unspec:CCRAW [(match_operand:V2DF 1 "register_operand"  "v")
+		       (match_operand:HI   2 "const_int_operand" "J")]
+		      UNSPEC_VEC_VFTCIDBCC))
+   (clobber (match_scratch:V2DI 0 "=v"))]
+  "TARGET_VX && CONST_OK_FOR_CONSTRAINT_P (INTVAL (operands[2]), 'J', \"J\")"
+  "vftcidb\t%v0,%v1,%x2"
+  [(set_attr "op_type" "VRR")])
+
+(define_expand "vftcidb"
+  [(parallel
+    [(set (match_operand:V2DF               0 "register_operand"  "")
+	  (unspec:V2DF [(match_operand:V2DF 1 "register_operand"  "")
+			(match_operand:HI   2 "const_int_operand" "")]
+		       UNSPEC_VEC_VFTCIDB))
+     (set (reg:CCRAW CC_REGNUM)
+	  (unspec:CCRAW [(match_dup 1) (match_dup 2)] UNSPEC_VEC_VFTCIDBCC))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCRAW CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX && CONST_OK_FOR_CONSTRAINT_P (INTVAL (operands[2]), 'J', \"J\")")
+
+;;
+;; Integer compares
+;;
+
+; All comparisons which produce a CC need fully populated (VI_HW)
+; vector arguments.  Otherwise the any/all CCs would be just bogus.
+
+(define_insn "*vec_cmp<VICMP:insn_cmp><VI_HW:mode>_cconly"
+  [(set (reg:VICMP CC_REGNUM)
+	(compare:VICMP (match_operand:VI_HW 0 "register_operand" "v")
+		       (match_operand:VI_HW 1 "register_operand" "v")))
+   (clobber (match_scratch:VI_HW 2 "=v"))]
+  "TARGET_VX"
+  "vc<VICMP:insn_cmp><VI_HW:bhfgq>s\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; FIXME: The following 2x3 definitions should be merged into 2 with
+; VICMP like above but I could not find a way to set the comparison
+; operator (eq) depending on the mode CCVEQ (mode_iterator). Or the
+; other way around - setting the mode depending on the code
+; (code_iterator).
+(define_expand "vec_cmpeq<VI_HW:mode>_cc"
+  [(parallel
+    [(set (reg:CCVEQ CC_REGNUM)
+	(compare:CCVEQ (match_operand:VI_HW 1 "register_operand" "v")
+		       (match_operand:VI_HW 2 "register_operand" "v")))
+     (set (match_operand:VI_HW 0 "register_operand" "=v")
+	  (eq:VI_HW (match_dup 1) (match_dup 2)))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCVEQ CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+(define_expand "vec_cmph<VI_HW:mode>_cc"
+  [(parallel
+    [(set (reg:CCVH CC_REGNUM)
+	  (compare:CCVH (match_operand:VI_HW 1 "register_operand" "v")
+			(match_operand:VI_HW 2 "register_operand" "v")))
+     (set (match_operand:VI_HW 0 "register_operand" "=v")
+	  (gt:VI_HW (match_dup 1) (match_dup 2)))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCVH CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+(define_expand "vec_cmphl<VI_HW:mode>_cc"
+  [(parallel
+    [(set (reg:CCVHU CC_REGNUM)
+	  (compare:CCVHU (match_operand:VI_HW 1 "register_operand" "v")
+			 (match_operand:VI_HW 2 "register_operand" "v")))
+     (set (match_operand:VI_HW 0 "register_operand" "=v")
+	  (gtu:VI_HW (match_dup 1) (match_dup 2)))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCVHU CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+
+(define_insn "*vec_cmpeq<VI_HW:mode>_cc"
+  [(set (reg:CCVEQ CC_REGNUM)
+	(compare:CCVEQ (match_operand:VI_HW 0 "register_operand"  "v")
+		       (match_operand:VI_HW 1 "register_operand"  "v")))
+   (set (match_operand:VI_HW                2 "register_operand" "=v")
+	(eq:VI_HW (match_dup 0) (match_dup 1)))]
+  "TARGET_VX"
+  "vceq<VI_HW:bhfgq>s\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "*vec_cmph<VI_HW:mode>_cc"
+  [(set (reg:CCVH CC_REGNUM)
+	(compare:CCVH (match_operand:VI_HW 0 "register_operand"  "v")
+		      (match_operand:VI_HW 1 "register_operand"  "v")))
+   (set (match_operand:VI_HW               2 "register_operand" "=v")
+	(gt:VI_HW (match_dup 0) (match_dup 1)))]
+  "TARGET_VX"
+  "vch<VI_HW:bhfgq>s\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "*vec_cmphl<VI_HW:mode>_cc"
+  [(set (reg:CCVHU CC_REGNUM)
+	(compare:CCVHU (match_operand:VI_HW 0 "register_operand"  "v")
+		       (match_operand:VI_HW 1 "register_operand"  "v")))
+   (set (match_operand:VI_HW                2 "register_operand" "=v")
+	(gtu:VI_HW (match_dup 0) (match_dup 1)))]
+  "TARGET_VX"
+  "vchl<VI_HW:bhfgq>s\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+;;
+;; Floating point comparesg
+;;
+
+(define_insn "*vec_cmp<insn_cmp>v2df_cconly"
+  [(set (reg:VFCMP CC_REGNUM)
+	(compare:VFCMP (match_operand:V2DF 0 "register_operand" "v")
+		       (match_operand:V2DF 1 "register_operand" "v")))
+   (clobber (match_scratch:V2DI 2 "=v"))]
+  "TARGET_VX"
+  "vfc<asm_fcmp>dbs\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+; FIXME: Merge the following 2x3 patterns with VFCMP
+(define_expand "vec_cmpeqv2df_cc"
+  [(parallel
+    [(set (reg:CCVEQ CC_REGNUM)
+	  (compare:CCVEQ (match_operand:V2DF 1 "register_operand"  "v")
+			 (match_operand:V2DF 2 "register_operand"  "v")))
+     (set (match_operand:V2DI 0 "register_operand" "=v")
+	  (eq:V2DI (match_dup 1) (match_dup 2)))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCVEQ CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+(define_expand "vec_cmphv2df_cc"
+  [(parallel
+    [(set (reg:CCVH CC_REGNUM)
+	  (compare:CCVH (match_operand:V2DF 1 "register_operand"  "v")
+			(match_operand:V2DF 2 "register_operand"  "v")))
+     (set (match_operand:V2DI 0 "register_operand" "=v")
+	  (gt:V2DI (match_dup 1) (match_dup 2)))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCVH CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+(define_expand "vec_cmphev2df_cc"
+  [(parallel
+    [(set (reg:CCVFHE CC_REGNUM)
+	  (compare:CCVFHE (match_operand:V2DF 1 "register_operand"  "v")
+			  (match_operand:V2DF 2 "register_operand"  "v")))
+     (set (match_operand:V2DI 0 "register_operand" "=v")
+	  (ge:V2DI (match_dup 1) (match_dup 2)))])
+   (set (match_operand:SI 3 "memory_operand" "")
+	(unspec:SI [(reg:CCVFHE CC_REGNUM)] UNSPEC_CC_TO_INT))]
+  "TARGET_VX")
+
+
+(define_insn "*vec_cmpeqv2df_cc"
+  [(set (reg:CCVEQ CC_REGNUM)
+	(compare:CCVEQ (match_operand:V2DF 0 "register_operand"  "v")
+		       (match_operand:V2DF 1 "register_operand"  "v")))
+   (set (match_operand:V2DI                2 "register_operand" "=v")
+	(eq:V2DI (match_dup 0) (match_dup 1)))]
+  "TARGET_VX"
+  "vfcedbs\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "*vec_cmphv2df_cc"
+  [(set (reg:CCVH CC_REGNUM)
+	(compare:CCVH (match_operand:V2DF 0 "register_operand"  "v")
+		      (match_operand:V2DF 1 "register_operand"  "v")))
+   (set (match_operand:V2DI               2 "register_operand" "=v")
+	(gt:V2DI (match_dup 0) (match_dup 1)))]
+  "TARGET_VX"
+  "vfchdbs\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
+
+(define_insn "*vec_cmphev2df_cc"
+  [(set (reg:CCVFHE CC_REGNUM)
+	(compare:CCVFHE (match_operand:V2DF 0 "register_operand"  "v")
+			(match_operand:V2DF 1 "register_operand"  "v")))
+   (set (match_operand:V2DI                 2 "register_operand" "=v")
+	(ge:V2DI (match_dup 0) (match_dup 1)))]
+  "TARGET_VX"
+  "vfchedbs\t%v2,%v0,%v1"
+  [(set_attr "op_type" "VRR")])
--- gcc/config.gcc	2016-05-11 14:46:08.298981685 +0200
+++ gcc/config.gcc	2016-05-11 17:17:32.000000000 +0200
@@ -452,7 +452,7 @@ s390*-*-*)
 	cpu_type=s390
 	need_64bit_hwint=yes
 	extra_options="${extra_options} fused-madd.opt"
-	extra_headers="s390intrin.h htmintrin.h htmxlintrin.h"
+	extra_headers="s390intrin.h htmintrin.h htmxlintrin.h vecintrin.h"
 	;;
 # Note the 'l'; we need to be able to match e.g. "shle" or "shl".
 sh[123456789lbe]*-*-* | sh-*-*)
@@ -2249,27 +2249,35 @@ rx-*-elf*)
 s390-*-linux*)
 	default_gnu_indirect_function=yes
 	tm_file="s390/s390.h dbxelf.h elfos.h gnu-user.h linux.h glibc-stdint.h s390/linux.h"
+	c_target_objs="${c_target_objs} s390-c.o"
+	cxx_target_objs="${cxx_target_objs} s390-c.o"
 	if test x$enable_targets = xall; then
 		tmake_file="${tmake_file} s390/t-linux64"
 	fi
+	tmake_file="${tmake_file} s390/t-s390"
 	;;
 s390x-*-linux*)
 	default_gnu_indirect_function=yes
 	tm_file="s390/s390x.h s390/s390.h dbxelf.h elfos.h gnu-user.h linux.h glibc-stdint.h s390/linux.h"
 	tm_p_file=s390/s390-protos.h
+	c_target_objs="${c_target_objs} s390-c.o"
+	cxx_target_objs="${cxx_target_objs} s390-c.o"
 	md_file=s390/s390.md
 	extra_modes=s390/s390-modes.def
 	out_file=s390/s390.c
-	tmake_file="${tmake_file} s390/t-linux64"
+	tmake_file="${tmake_file} s390/t-linux64 s390/t-s390"
 	;;
 s390x-ibm-tpf*)
-        tm_file="s390/s390x.h s390/s390.h dbxelf.h elfos.h s390/tpf.h"
-        tm_p_file=s390/s390-protos.h
-        md_file=s390/s390.md
-        extra_modes=s390/s390-modes.def
-        out_file=s390/s390.c
-        thread_file='tpf'
+	tm_file="s390/s390x.h s390/s390.h dbxelf.h elfos.h s390/tpf.h"
+	tm_p_file=s390/s390-protos.h
+	c_target_objs="${c_target_objs} s390-c.o"
+	cxx_target_objs="${cxx_target_objs} s390-c.o"
+	md_file=s390/s390.md
+	extra_modes=s390/s390-modes.def
+	out_file=s390/s390.c
+	thread_file='tpf'
 	extra_options="${extra_options} s390/tpf.opt"
+	tmake_file="${tmake_file} s390/t-s390"
 	;;
 score-*-elf)
 	gas=yes
@@ -3603,7 +3611,7 @@ case "${target}" in
 		for which in arch tune; do
 			eval "val=\$with_$which"
 			case ${val} in
-			"" | g5 | g6 | z900 | z990 | z9-109 | z9-ec | z10 | z196 | zEC12)
+			"" | g5 | g6 | z900 | z990 | z9-109 | z9-ec | z10 | z196 | zEC12 | z13)
 				# OK
 				;;
 			*)
--- gcc/configure	2016-05-11 14:46:08.719976035 +0200
+++ gcc/configure	2016-05-11 19:41:14.975813805 +0200
@@ -26000,6 +26000,42 @@ $as_echo "#define HAVE_LD_PERSONALITY_RE
 
     fi
     ;;
+  s390*-*-*)
+    { $as_echo "$as_me:${as_lineno-$LINENO}: checking assembler for .gnu_attribute support" >&5
+$as_echo_n "checking assembler for .gnu_attribute support... " >&6; }
+if test "${gcc_cv_as_s390_gnu_attribute+set}" = set; then :
+  $as_echo_n "(cached) " >&6
+else
+  gcc_cv_as_s390_gnu_attribute=no
+    if test $in_tree_gas = yes; then
+    if test $gcc_cv_gas_vers -ge `expr \( \( 2 \* 1000 \) + 18 \) \* 1000 + 0`
+  then gcc_cv_as_s390_gnu_attribute=yes
+fi
+  elif test x$gcc_cv_as != x; then
+    $as_echo '.gnu_attribute 8,1' > conftest.s
+    if { ac_try='$gcc_cv_as $gcc_cv_as_flags  -o conftest.o conftest.s >&5'
+  { { eval echo "\"\$as_me\":${as_lineno-$LINENO}: \"$ac_try\""; } >&5
+  (eval $ac_try) 2>&5
+  ac_status=$?
+  $as_echo "$as_me:${as_lineno-$LINENO}: \$? = $ac_status" >&5
+  test $ac_status = 0; }; }
+    then
+	gcc_cv_as_s390_gnu_attribute=yes
+    else
+      echo "configure: failed program was" >&5
+      cat conftest.s >&5
+    fi
+    rm -f conftest.o conftest.s
+  fi
+fi
+{ $as_echo "$as_me:${as_lineno-$LINENO}: result: $gcc_cv_as_s390_gnu_attribute" >&5
+$as_echo "$gcc_cv_as_s390_gnu_attribute" >&6; }
+if test $gcc_cv_as_s390_gnu_attribute = yes; then
+
+$as_echo "#define HAVE_AS_GNU_ATTRIBUTE 1" >>confdefs.h
+
+fi
+    ;;
 esac
 
 # Mips and HP-UX need the GNU assembler.
--- gcc/configure.ac	2015-06-18 16:32:50.000000000 +0200
+++ gcc/configure.ac	2016-05-11 19:34:04.507631160 +0200
@@ -4207,6 +4207,13 @@ EOF
 pointers into PC-relative form.])
     fi
     ;;
+  s390*-*-*)
+    gcc_GAS_CHECK_FEATURE([.gnu_attribute support],
+      gcc_cv_as_s390_gnu_attribute, [2,18,0],,
+      [.gnu_attribute 8,1],,
+      [AC_DEFINE(HAVE_AS_GNU_ATTRIBUTE, 1,
+	  [Define if your assembler supports .gnu_attribute.])])
+    ;;
 esac
 
 # Mips and HP-UX need the GNU assembler.
--- gcc/doc/invoke.texi	2016-05-11 14:46:08.615977431 +0200
+++ gcc/doc/invoke.texi	2016-05-11 19:27:23.065121001 +0200
@@ -885,6 +885,7 @@ See RS/6000 and PowerPC Options.
 -mbackchain  -mno-backchain -mpacked-stack  -mno-packed-stack @gol
 -msmall-exec  -mno-small-exec  -mmvcle -mno-mvcle @gol
 -m64  -m31  -mdebug  -mno-debug  -mesa  -mzarch @gol
+-mhtm -mvx -mzvector @gol
 -mtpf-trace -mno-tpf-trace  -mfused-madd  -mno-fused-madd @gol
 -mwarn-framesize  -mwarn-dynamicstack  -mstack-size -mstack-guard @gol
 -mhotpatch=@var{halfwords},@var{halfwords}}
@@ -18596,6 +18597,46 @@ When generating code compliant to the GN
 the default is @option{-mesa}.  When generating code compliant
 to the GNU/Linux for zSeries ABI, the default is @option{-mzarch}.
 
+@item -mhtm
+@itemx -mno-htm
+@opindex mhtm
+@opindex mno-htm
+The @option{-mhtm} option enables a set of builtins making use of
+instructions available with the transactional execution facility
+introduced with the IBM zEnterprise EC12 machine generation
+@ref{S/390 System z Built-in Functions}.
+@option{-mhtm} is enabled by default when using @option{-march=zEC12}.
+
+@item -mvx
+@itemx -mno-vx
+@opindex mvx
+@opindex mno-vx
+When @option{-mvx} is specified, generate code using the instructions
+available with the vector extension facility introduced with the IBM
+z13 machine generation.
+This option changes the ABI for some vector type values with regard to
+alignment and calling conventions.  In case vector type values are
+being used in an ABI-relevant context a GAS @samp{.gnu_attribute}
+command will be added to mark the resulting binary with the ABI used.
+@option{-mvx} is enabled by default when using @option{-march=z13}.
+
+@item -mzvector
+@itemx -mno-zvector
+@opindex mzvector
+@opindex mno-zvector
+The @option{-mzvector} option enables vector language extensions and
+builtins using instructions available with the vector extension
+facility introduced with the IBM z13 machine generation.
+This option adds support for @samp{vector} to be used as a keyword to
+define vector type variables and arguments.  @samp{vector} is only
+available when GNU extensions are enabled.  It will not be expanded
+when requesting strict standard compliance e.g. with @option{-std=c99}.
+In addition to the GCC low-level builtins @option{-mzvector} enables
+a set of builtins added for compatibility with Altivec-style
+implementations like Power and Cell.  In order to make use of these
+builtins the header file @file{vecintrin.h} needs to be included.
+@option{-mzvector} is disabled by default.
+
 @item -mmvcle
 @itemx -mno-mvcle
 @opindex mmvcle
@@ -18617,7 +18658,8 @@ The default is to not print debug inform
 Generate code that runs on @var{cpu-type}, which is the name of a system
 representing a certain processor type.  Possible values for
 @var{cpu-type} are @samp{g5}, @samp{g6}, @samp{z900}, @samp{z990},
-@samp{z9-109}, @samp{z9-ec}, @samp{z10}, @samp{z196}, and @samp{zEC12}.
+@samp{z9-109}, @samp{z9-ec}, @samp{z10}, @samp{z196}, @samp{zEC12},
+and @samp{z13}.
 When generating code using the instructions available on z/Architecture,
 the default is @option{-march=z900}.  Otherwise, the default is
 @option{-march=g5}.
--- gcc/doc/tm.texi	2016-05-11 14:46:08.216982786 +0200
+++ gcc/doc/tm.texi	2016-05-11 15:41:36.000000000 +0200
@@ -8983,6 +8983,13 @@ register in Dwarf.  Otherwise, this hook
 If not defined, the default is to return @code{NULL_RTX}.
 @end deftypefn
 
+@deftypefn {Target Hook} {enum machine_mode} TARGET_DWARF_FRAME_REG_MODE (int @var{regno})
+Given a register, this hook should return the mode which the
+corresponding Dwarf frame register should have.  This is normally
+used to return a smaller mode than the raw mode to prevent call
+clobbered parts of a register altering the frame register size
+@end deftypefn
+
 @deftypefn {Target Hook} void TARGET_INIT_DWARF_REG_SIZES_EXTRA (tree @var{address})
 If some registers are represented in Dwarf-2 unwind information in
 multiple pieces, define this hook to fill in information about the
--- gcc/doc/tm.texi.in	2016-05-11 14:46:08.213982826 +0200
+++ gcc/doc/tm.texi.in	2016-05-11 15:41:36.000000000 +0200
@@ -8854,6 +8854,8 @@ register in Dwarf.  Otherwise, this hook
 If not defined, the default is to return @code{NULL_RTX}.
 @end deftypefn
 
+@hook TARGET_DWARF_FRAME_REG_MODE
+
 @hook TARGET_INIT_DWARF_REG_SIZES_EXTRA
 If some registers are represented in Dwarf-2 unwind information in
 multiple pieces, define this hook to fill in information about the
--- gcc/dwarf2cfi.c	2013-01-21 16:10:46.000000000 +0100
+++ gcc/dwarf2cfi.c	2016-05-11 15:41:36.000000000 +0200
@@ -244,11 +244,9 @@ expand_builtin_init_dwarf_reg_sizes (tre
       if (rnum < DWARF_FRAME_REGISTERS)
 	{
 	  HOST_WIDE_INT offset = rnum * GET_MODE_SIZE (mode);
-	  enum machine_mode save_mode = reg_raw_mode[i];
 	  HOST_WIDE_INT size;
+	  enum machine_mode save_mode = targetm.dwarf_frame_reg_mode (i);
 
-	  if (HARD_REGNO_CALL_PART_CLOBBERED (i, save_mode))
-	    save_mode = choose_hard_reg_mode (i, 1, true);
 	  if (dnum == DWARF_FRAME_RETURN_COLUMN)
 	    {
 	      if (save_mode == VOIDmode)
--- gcc/genattrtab.c	2013-01-21 16:08:23.000000000 +0100
+++ gcc/genattrtab.c	2016-05-11 17:32:29.000000000 +0200
@@ -229,7 +229,7 @@ static int *insn_n_alternatives;
 /* Stores, for each insn code, a bitmap that has bits on for each possible
    alternative.  */
 
-static int *insn_alternatives;
+static uint64_t *insn_alternatives;
 
 /* Used to simplify expressions.  */
 
@@ -257,7 +257,7 @@ static char *attr_printf           (unsi
   ATTRIBUTE_PRINTF_2;
 static rtx make_numeric_value      (int);
 static struct attr_desc *find_attr (const char **, int);
-static rtx mk_attr_alt             (int);
+static rtx mk_attr_alt             (uint64_t);
 static char *next_comma_elt	   (const char **);
 static rtx insert_right_side	   (enum rtx_code, rtx, rtx, int, int);
 static rtx copy_boolean		   (rtx);
@@ -771,7 +771,7 @@ check_attr_test (rtx exp, int is_const,
 	  if (attr == NULL)
 	    {
 	      if (! strcmp (XSTR (exp, 0), "alternative"))
-		return mk_attr_alt (1 << atoi (XSTR (exp, 1)));
+		return mk_attr_alt (((uint64_t) 1) << atoi (XSTR (exp, 1)));
 	      else
 		fatal ("unknown attribute `%s' in EQ_ATTR", XSTR (exp, 0));
 	    }
@@ -817,7 +817,7 @@ check_attr_test (rtx exp, int is_const,
 
 	      name_ptr = XSTR (exp, 1);
 	      while ((p = next_comma_elt (&name_ptr)) != NULL)
-		set |= 1 << atoi (p);
+		set |= ((uint64_t) 1) << atoi (p);
 
 	      return mk_attr_alt (set);
 	    }
@@ -1292,7 +1292,7 @@ static struct attr_value *
 get_attr_value (rtx value, struct attr_desc *attr, int insn_code)
 {
   struct attr_value *av;
-  int num_alt = 0;
+  uint64_t num_alt = 0;
 
   value = make_canonical (attr, value);
   if (compares_alternatives_p (value))
@@ -1934,7 +1934,7 @@ insert_right_side (enum rtx_code code, r
    This routine is passed an expression and either AND or IOR.  It returns a
    bitmask indicating which alternatives are mentioned within EXP.  */
 
-static int
+static uint64_t
 compute_alternative_mask (rtx exp, enum rtx_code code)
 {
   const char *string;
@@ -1965,15 +1965,15 @@ compute_alternative_mask (rtx exp, enum
     return 0;
 
   if (string[1] == 0)
-    return 1 << (string[0] - '0');
-  return 1 << atoi (string);
+    return ((uint64_t) 1) << (string[0] - '0');
+  return ((uint64_t) 1) << atoi (string);
 }
 
 /* Given I, a single-bit mask, return RTX to compare the `alternative'
    attribute with the value represented by that bit.  */
 
 static rtx
-make_alternative_compare (int mask)
+make_alternative_compare (uint64_t mask)
 {
   return mk_attr_alt (mask);
 }
@@ -2472,7 +2472,7 @@ attr_alt_complement (rtx s)
    in E.  */
 
 static rtx
-mk_attr_alt (int e)
+mk_attr_alt (uint64_t e)
 {
   rtx result = rtx_alloc (EQ_ATTR_ALT);
 
@@ -2499,7 +2499,7 @@ simplify_test_exp (rtx exp, int insn_cod
   struct attr_value *av;
   struct insn_ent *ie;
   struct attr_value_list *iv;
-  int i;
+  uint64_t i;
   rtx newexp = exp;
   bool left_alt, right_alt;
 
@@ -2779,7 +2779,7 @@ simplify_test_exp (rtx exp, int insn_cod
     case EQ_ATTR:
       if (XSTR (exp, 0) == alternative_name)
 	{
-	  newexp = mk_attr_alt (1 << atoi (XSTR (exp, 1)));
+	  newexp = mk_attr_alt (((uint64_t) 1) << atoi (XSTR (exp, 1)));
 	  break;
 	}
 
@@ -5240,10 +5240,11 @@ main (int argc, char **argv)
     expand_delays ();
 
   /* Make `insn_alternatives'.  */
-  insn_alternatives = oballocvec (int, insn_code_number);
+  insn_alternatives = oballocvec (uint64_t, insn_code_number);
   for (id = defs; id; id = id->next)
     if (id->insn_code >= 0)
-      insn_alternatives[id->insn_code] = (1 << id->num_alternatives) - 1;
+      insn_alternatives[id->insn_code]
+	= (((uint64_t) 1) << id->num_alternatives) - 1;
 
   /* Make `insn_n_alternatives'.  */
   insn_n_alternatives = oballocvec (int, insn_code_number);
--- gcc/optabs.c	2014-05-15 10:46:12.000000000 +0200
+++ gcc/optabs.c	2016-05-11 15:53:11.000000000 +0200
@@ -6659,11 +6659,11 @@ expand_vec_perm (enum machine_mode mode,
       enum machine_mode selmode = GET_MODE (sel);
       if (u == 2)
 	sel = expand_simple_binop (selmode, PLUS, sel, sel,
-				   sel, 0, OPTAB_DIRECT);
+				   NULL, 0, OPTAB_DIRECT);
       else
 	sel = expand_simple_binop (selmode, ASHIFT, sel,
 				   GEN_INT (exact_log2 (u)),
-				   sel, 0, OPTAB_DIRECT);
+				   NULL, 0, OPTAB_DIRECT);
       gcc_assert (sel != NULL);
 
       /* Broadcast the low byte each element into each of its bytes.  */
--- gcc/recog.h	2013-09-09 19:16:08.000000000 +0200
+++ gcc/recog.h	2016-05-11 15:52:48.000000000 +0200
@@ -21,7 +21,7 @@ along with GCC; see the file COPYING3.
 #define GCC_RECOG_H
 
 /* Random number that should be large enough for all purposes.  */
-#define MAX_RECOG_ALTERNATIVES 30
+#define MAX_RECOG_ALTERNATIVES 35
 
 /* Types of operands.  */
 enum op_type {
--- gcc/target.def	2013-03-04 12:46:23.000000000 +0100
+++ gcc/target.def	2016-05-11 15:41:36.000000000 +0200
@@ -1834,6 +1834,17 @@ DEFHOOK
  rtx, (rtx reg),
  hook_rtx_rtx_null)
 
+/* Given a register return the mode of the corresponding DWARF frame
+   register.  */
+DEFHOOK
+(dwarf_frame_reg_mode,
+ "Given a register, this hook should return the mode which the\n\
+corresponding Dwarf frame register should have.  This is normally\n\
+used to return a smaller mode than the raw mode to prevent call\n\
+clobbered parts of a register altering the frame register size",
+ enum machine_mode, (int regno),
+ default_dwarf_frame_reg_mode)
+
 /* If expand_builtin_init_dwarf_reg_sizes needs to fill in table
    entries not corresponding directly to registers below
    FIRST_PSEUDO_REGISTER, this hook should generate the necessary
--- gcc/targhooks.c	2013-01-21 16:02:59.000000000 +0100
+++ gcc/targhooks.c	2016-05-11 15:41:36.000000000 +0200
@@ -1411,6 +1411,19 @@ default_debug_unwind_info (void)
   return UI_NONE;
 }
 
+/* Determine the correct mode for a Dwarf frame register that represents
+   register REGNO.  */
+
+enum machine_mode
+default_dwarf_frame_reg_mode (int regno)
+{
+  enum machine_mode save_mode = reg_raw_mode[regno];
+
+  if (HARD_REGNO_CALL_PART_CLOBBERED (regno, save_mode))
+    save_mode = choose_hard_reg_mode (regno, 1, true);
+  return save_mode;
+}
+
 /* To be used by targets where reg_raw_mode doesn't return the right
    mode for registers used in apply_builtin_return and apply_builtin_arg.  */
 
--- gcc/targhooks.h	2013-01-21 16:03:00.000000000 +0100
+++ gcc/targhooks.h	2016-05-11 15:42:21.000000000 +0200
@@ -186,6 +186,7 @@ extern int default_label_align_max_skip
 extern int default_jump_align_max_skip (rtx);
 extern section * default_function_section(tree decl, enum node_frequency freq,
 					  bool startup, bool exit);
+extern enum machine_mode default_dwarf_frame_reg_mode (int);
 extern enum machine_mode default_get_reg_raw_mode(int);
 
 extern void *default_get_pch_validity (size_t *);
--- gcc/testsuite/gcc.dg/tree-ssa/gen-vect-11b.c	2012-12-13 11:28:46.000000000 +0100
+++ gcc/testsuite/gcc.dg/tree-ssa/gen-vect-11b.c	2016-05-11 17:30:16.000000000 +0200
@@ -1,5 +1,6 @@
 /* { dg-do run { target vect_cmdline_needed } } */
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details" } */
+/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -mno-vx" { target { s390*-*-* } } } */
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -mno-sse" { target { i?86-*-* x86_64-*-* } } } */
 
 #include <stdlib.h>
--- gcc/testsuite/gcc.dg/tree-ssa/gen-vect-11c.c	2012-12-13 11:28:46.000000000 +0100
+++ gcc/testsuite/gcc.dg/tree-ssa/gen-vect-11c.c	2016-05-11 17:30:16.000000000 +0200
@@ -1,5 +1,6 @@
 /* { dg-do run { target vect_cmdline_needed } } */
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details" } */
+/* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -mno-vx" { target { s390*-*-* } } } */
 /* { dg-options "-O2 -ftree-vectorize -fdump-tree-vect-details -mno-sse" { target { i?86-*-* x86_64-*-* } } } */
 
 #include <stdlib.h>
--- gcc/testsuite/gcc.target/s390/htm-builtins-z13-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/htm-builtins-z13-1.c	2016-05-11 17:34:03.000000000 +0200
@@ -0,0 +1,34 @@
+/* Verify if VRs are saved and restored.  */
+
+/* { dg-do run } */
+/* { dg-require-effective-target vector } */
+/* { dg-options "-O3 -march=z13 -mzarch" } */
+
+typedef int __attribute__((vector_size(16))) v4si;
+
+v4si __attribute__((noinline))
+foo (v4si a)
+{
+  a += (v4si){ 1, 1, 1, 1 };
+  if (__builtin_tbegin (0) == 0)
+    {
+      a += (v4si){ 1, 1, 1, 1 };
+      __builtin_tabort (256);
+      __builtin_tend ();
+    }
+  else
+    a -= (v4si){ 1, 1, 1, 1 };
+
+  return a;
+}
+
+int
+main ()
+{
+  v4si a = (v4si){ 0, 0, 0, 0 };
+
+  a = foo (a);
+
+  if (a[0] != 0)
+    __builtin_abort ();
+}
--- gcc/testsuite/gcc.target/s390/s390.exp	2015-06-18 16:32:12.000000000 +0200
+++ gcc/testsuite/gcc.target/s390/s390.exp	2016-05-11 17:12:20.000000000 +0200
@@ -37,6 +37,21 @@ proc check_effective_target_htm { } {
     }] "-march=zEC12 -mzarch" ] } { return 0 } else { return 1 }
 }
 
+# Return 1 if vector (va - vector add) instructions are understood by
+# the assembler and can be executed.  This also covers checking for
+# the VX kernel feature.  A kernel without that feature does not
+# enable the vector facility and the following check will die with a
+# signal.
+proc check_effective_target_vector { } {
+    if { ![check_runtime s390_check_vector [subst {
+	int main (void)
+	{
+	    asm ("va %%v24, %%v26, %%v28, 3" : : : "v24", "v26", "v28");
+	    return 0;
+	}
+    }] "-march=z13 -mzarch" ] } { return 0 } else { return 1 }
+}
+
 # If a testcase doesn't have special options, use these.
 global DEFAULT_CFLAGS
 if ![info exists DEFAULT_CFLAGS] then {
@@ -59,5 +74,8 @@ set-torture-options $HOTPATCH_TEST_OPTS
 gcc-dg-runtest [lsort [glob -nocomplain $hotpatch_tests]] $DEFAULT_CFLAGS
 torture-finish
 
+dg-runtest [lsort [glob -nocomplain $srcdir/$subdir/*vector*/*.\[cS\]]] \
+	"" $DEFAULT_CFLAGS
+
 # All done.
 dg-finish
--- gcc/testsuite/gcc.target/s390/vector/int128-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/int128-1.c	2016-05-11 18:10:56.000000000 +0200
@@ -0,0 +1,47 @@
+/* Check that vaq/vsq are used for int128 operations.  */
+
+/* { dg-do compile { target { lp64 } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+
+const __int128 c = (__int128)0x0123456789abcd55 + ((__int128)7 << 64);
+
+
+__int128
+addreg(__int128 a, __int128 b)
+{
+  return a + b;
+}
+
+__int128
+addconst(__int128 a)
+{
+  return a + c;
+}
+
+__int128
+addmem(__int128 *a, __int128_t *b)
+{
+  return *a + *b;
+}
+
+__int128
+subreg(__int128 a, __int128 b)
+{
+  return a - b;
+}
+
+__int128
+subconst(__int128 a)
+{
+  return a - c; /* This becomes vaq as well.  */
+}
+
+__int128
+submem(__int128 *a, __int128_t *b)
+{
+  return *a - *b;
+}
+
+/* { dg-final { scan-assembler-times "vaq" 4 } } */
+/* { dg-final { scan-assembler-times "vsq" 2 } } */
--- gcc/testsuite/gcc.target/s390/vector/stpcpy-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/stpcpy-1.c	2016-05-11 18:11:22.000000000 +0200
@@ -0,0 +1,100 @@
+/* The z13 stpcpy implementation plays some alignment tricks for good
+   performance.  This test tries to make sure it works correctly and
+   does not access bytes beyond the source and destination
+   strings.  */
+
+/* { dg-do run } */
+/* { dg-require-effective-target vector } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+#include <stdio.h>
+#include <sys/mman.h>
+
+#define PAGE_SIZE 4096
+
+struct {
+  char unused[PAGE_SIZE - 32];
+  char m32[15]; /* page bndry - 32 */
+  char m17[1];
+  char m16[1];
+  char m15[14];
+  char m1[1];
+  char next_page[PAGE_SIZE];
+} s, d __attribute__((aligned(PAGE_SIZE)));
+
+char *__attribute__((noinline))
+my_stpcpy(char *dest, const char *src)
+{
+  return __builtin_stpcpy (dest, src);
+}
+
+void __attribute__ ((noinline))
+check (char *dest, char *src, size_t len)
+{
+  char *result;
+
+  result = my_stpcpy (dest, src);
+  if (result != dest + len)
+    __builtin_abort ();
+  if (__builtin_memcmp (src, dest, len) != 0)
+    __builtin_abort ();
+}
+
+int
+main ()
+{
+  char *src[5] = { s.m32, s.m17, s.m16, s.m15, s.m1 };
+  char *dst[5] = { d.m32, d.m17, d.m16, d.m15, d.m1 };
+  int len[8] = { 33, 32, 31, 17, 16, 15, 1, 0 };
+  int i, j, k;
+  char backup;
+
+  for (i = 0; i < sizeof (s); i++)
+    ((char*)&s)[i] = i % 26 + 97;
+
+  for (i = 0; i < 5; i++)
+    for (j = 0; j < 5; j++)
+      for (k = 0; k < 8; k++)
+	{
+	  backup = src[j][len[k]];
+	  src[j][len[k]] = 0;
+	  __builtin_memset (&d, 0, sizeof (d));
+	  check (dst[i], src[j], len[k]);
+	  src[j][len[k]] = backup;
+	}
+
+  /* Make all source strings end before the page boundary.  */
+  backup = s.m1[0];
+  s.m1[0] = 0;
+
+  if (mprotect (&s.next_page, PAGE_SIZE, PROT_NONE) == -1)
+    perror ("mprotect src");
+
+  for (i = 0; i < 5; i++)
+    for (j = 0; j < 5; j++)
+      check (dst[i], src[j],
+	     PAGE_SIZE - ((unsigned long)src[j] & ((1UL << 12) - 1)) - 1);
+
+  if (mprotect (&s.next_page, PAGE_SIZE, PROT_READ | PROT_WRITE) == -1)
+    perror ("mprotect src");
+
+  s.m1[0] = backup;
+
+  if (mprotect (&d.next_page, PAGE_SIZE, PROT_NONE) == -1)
+    perror ("mprotect dst");
+
+  for (i = 0; i < 5; i++)
+    for (j = 0; j < 5; j++)
+      {
+	int len = PAGE_SIZE - ((unsigned long)dst[i] & ((1UL << 12) - 1)) - 1;
+	char backup = src[j][len];
+
+	src[j][len] = 0;
+	__builtin_memset (&d, 0,
+			  (unsigned long)&d.next_page - (unsigned long)&d);
+	check (dst[i], src[j], len);
+	src[j][len] = backup;
+      }
+
+  return 0;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-1.c	2016-05-11 17:32:39.000000000 +0200
@@ -0,0 +1,18 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* Make sure the last argument is fetched from the argument overflow area.  */
+/* { dg-final { scan-assembler "vl\t%v\[0-9\]*,160\\(%r15\\)" { target lp64 } } } */
+/* { dg-final { scan-assembler "vl\t%v\[0-9\]*,96\\(%r15\\)" { target ilp32 } } } */
+/* { dg-final { scan-assembler "gnu_attribute 8, 2" } } */
+
+typedef double v2df __attribute__((vector_size(16)));
+
+v2df
+add (v2df a, v2df b, v2df c, v2df d,
+     v2df e, v2df f, v2df g, v2df h, v2df i)
+{
+  return a + b + c + d + e + f + g + h + i;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-2.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,15 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* This needs to be v24 = v24 * v26 + v28 */
+/* { dg-final { scan-assembler "vfmadb\t%v24,%v24,%v26,%v28" } } */
+
+typedef double v2df __attribute__((vector_size(16)));
+
+v2df
+madd (v2df a, v2df b, v2df c)
+{
+  return a * b + c;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-3.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-3.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,101 @@
+/* Check calling convention in the vector ABI regarding vector like structs.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* addA */
+/* { dg-final { scan-assembler-times "vfadb\t%v24,%v24,%v26" 1 } } */
+
+/* addB and addE*/
+/* { dg-final { scan-assembler-times "vah\t%v24,%v\[0-9\]*,%v\[0-9\]*" 2 } } */
+
+/* addC */
+/* { dg-final { scan-assembler-times "vag\t%v24,%v\[0-9\]*,%v\[0-9\]*" 1 } } */
+
+/* addB and addC are expected to read the arguments via pointers in r2 and r3 */
+/* { dg-final { scan-assembler-times "vl\t%v\[0-9\]*,0\\(%r2\\)" 2 } } */
+/* { dg-final { scan-assembler-times "vl\t%v\[0-9\]*,0\\(%r3\\)" 2 } } */
+
+/* addD */
+/* { dg-final { scan-assembler-times "vaf\t%v24,%v24,%v26" 1 } } */
+
+/* addE */
+/* { dg-final { scan-assembler-times "vah\t%v24,%v24,%v26" 1 } } */
+
+/* addF */
+/* { dg-final { scan-assembler-times "vab\t%v24,%v\[0-9\]*,%v\[0-9\]*" 1 } } */
+/* { dg-final { scan-assembler-times "srlg\t%r\[0-9\]*,%r2,32" 1 { target lp64 } } } */
+/* { dg-final { scan-assembler-times "srlg\t%r\[0-9\]*,%r3,32" 1 { target lp64 } } } */
+/* { dg-final { scan-assembler-times "llgfr\t%.*,%r2" 1 { target { ! lp64 } } } } */
+/* { dg-final { scan-assembler-times "llgfr\t%.*,%r4" 1 { target { ! lp64 } } } } */
+
+
+typedef double v2df __attribute__((vector_size(16)));
+typedef long long v2di __attribute__((vector_size(16)));
+typedef int v4si __attribute__((vector_size(16)));
+typedef short v8hi __attribute__((vector_size(16)));
+
+typedef short v2hi __attribute__((vector_size(4)));
+typedef char v4qi __attribute__((vector_size(4)));
+
+/* Vector like structs are passed in VRs.  */
+struct A { v2df a; };
+
+v2df
+addA (struct A a, struct A b)
+{
+  return a.a + b.a;
+}
+
+/* Only single element vectors qualify as vector type parms.  This one
+   is passed as a struct. Since it is bigger than 8 bytes it is passed
+   on the stack with the reference being put into r2/r3.  */
+struct B { v8hi a; char b;};
+
+v8hi
+addB (struct B a, struct B b)
+{
+  return a.a + b.a;
+}
+
+/* The resulting struct is bigger than 16 bytes and therefore passed
+   on the stack with the references residing in r2/r3.  */
+struct C { v2di __attribute__((aligned(32))) a; };
+
+v2di
+addC (struct C a, struct C b)
+{
+  return a.a + b.a;
+}
+
+/* The attribute here does not have any effect. So this struct stays
+   vector like and hence is passed in a VR.  */
+struct D { v4si __attribute__((aligned(16))) a; };
+
+v4si
+addD (struct D a, struct D b)
+{
+  return a.a + b.a;
+}
+
+
+/* Smaller vectors are passed in vector registers. This also applies
+   for vector like structs.  */
+struct E { v2hi a; };
+
+v2hi
+addE (struct E a, struct E b)
+{
+  return a.a + b.a;
+}
+
+/* This struct is not passed in VRs because of padding.  But since it
+   fits in a GPR and has a power of two size. It is passed in
+   GPRs.  */
+struct F { v4qi __attribute__((aligned(8))) a; };
+
+v4qi
+addF (struct F a, struct F b)
+{
+  return a.a + b.a;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-4.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-4.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,19 @@
+/* Check calling convention in the vector ABI.  Smaller vector need to
+   be placed left-justified in the stack slot.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "lde\t%.*,160\\\(%r15\\\)" 1 { target lp64 } } } */
+/* { dg-final { scan-assembler-times "lde\t%.*,168\\\(%r15\\\)" 1 { target lp64 } } } */
+/* { dg-final { scan-assembler-times "lde\t%.*,96\\\(%r15\\\)" 1 { target { ! lp64 } } } } */
+/* { dg-final { scan-assembler-times "lde\t%.*,100\\\(%r15\\\)" 1 { target { ! lp64 } } } } */
+
+typedef char __attribute__((vector_size(4))) v4qi;
+
+v4qi
+foo (v4qi a, v4qi b, v4qi c, v4qi d, v4qi e,
+     v4qi f, v4qi g, v4qi h, v4qi i, v4qi j)
+{
+  return (a + b + c + d + e + f + g + h + i + j);
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-align-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-align-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,48 @@
+/* Check alignment convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+#include <stddef.h>
+
+/* Vector types get an 8 byte alignment.  */
+typedef double v2df __attribute__((vector_size(16)));
+typedef struct
+{
+  char a;
+  v2df b;
+} A;
+char c1[offsetof (A, b) == 8 ? 0 : -1];
+
+/* Smaller vector allow for smaller alignments.  */
+typedef char v4qi __attribute__((vector_size(4)));
+typedef struct
+{
+  char a;
+  v4qi b;
+} B;
+char c2[offsetof (B, b) == 4 ? 0 : -1];
+
+
+typedef double v4df __attribute__((vector_size(32)));
+typedef struct
+{
+  char a;
+  v4df b;
+} C;
+char c3[offsetof (C, b) == 8 ? 0 : -1];
+
+/* However, we allow the programmer to chose a bigger alignment.  */
+typedef struct
+{
+  char a;
+  v2df b __attribute__((aligned(16)));
+} D;
+char c4[offsetof (D, b) == 16 ? 0 : -1];
+
+typedef struct
+{
+  char a;
+  v2df b;
+} __attribute__((packed)) E;
+char c5[offsetof (E, b) == 1 ? 0 : -1];
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-1.c	2016-05-11 17:32:39.000000000 +0200
@@ -0,0 +1,18 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13 -mno-vx" } */
+
+/* The function passes arguments whose calling conventions change with
+   -mvx/-mno-vx.  In that case GCC has to emit the ABI attribute to
+   allow GDB and Binutils to detect this.  */
+/* { dg-final { scan-assembler "gnu_attribute 8, 1" } } */
+
+typedef double v2df __attribute__((vector_size(16)));
+
+v2df
+add (v2df a, v2df b, v2df c, v2df d,
+     v2df e, v2df f, v2df g, v2df h, v2df i)
+{
+  return a + b + c + d + e + f + g + h + i;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-2.c	2016-05-11 17:32:39.000000000 +0200
@@ -0,0 +1,53 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* No abi attribute should be emitted when nothing relevant happened.  */
+/* { dg-final { scan-assembler-not "gnu_attribute" } } */
+
+#include <stdarg.h>
+
+/* Local use is ok.  */
+
+typedef int v4si __attribute__((vector_size(16)));
+
+static
+v4si __attribute__((__noinline__))
+foo (v4si a)
+{
+  return a + (v4si){ 1, 2, 3, 4 };
+}
+
+int
+bar (int a)
+{
+  return foo ((v4si){ 1, 1, 1, 1 })[1];
+}
+
+/* Big vector type only used as function argument and return value
+   without being a struct/union member.  The alignment change is not
+   relevant here.  */
+
+typedef double v4df __attribute__((vector_size(32)));
+
+v4df
+add (v4df a, v4df b, v4df c, v4df d,
+     v4df e, v4df f, v4df g, v4df h, v4df i)
+{
+  return a + b + c + d + e + f + g + h + i;
+}
+
+double
+bar2 (int n, ...)
+{
+  double ret;
+  v4df a;
+  va_list va;
+
+  va_start (va, n);
+  ret = va_arg (va, v4df)[2];
+  va_end (va);
+
+  return ret;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-3.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-3.c	2016-05-11 17:32:39.000000000 +0200
@@ -0,0 +1,18 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler "gnu_attribute 8, 2" } } */
+
+typedef double v4df __attribute__((vector_size(32)));
+typedef struct { v4df a; } s;
+
+s
+add (v4df a, v4df b, v4df c, v4df d,
+     v4df e, v4df f, v4df g, v4df h, v4df i)
+{
+  s t;
+  t.a = a + b + c + d + e + f + g + h + i;
+  return t;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-4.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-4.c	2016-05-11 17:32:39.000000000 +0200
@@ -0,0 +1,17 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler "gnu_attribute 8, 2" } } */
+
+typedef int __attribute__((vector_size(16))) v4si;
+
+extern void bar (v4si);
+
+void
+foo (int a)
+{
+  v4si b = (v4si){ a, a, a, a };
+  bar (b);
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-5.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-5.c	2016-05-11 17:32:39.000000000 +0200
@@ -0,0 +1,19 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler "gnu_attribute 8, 2" } } */
+
+#include <stdarg.h>
+
+typedef int __attribute__((vector_size(16))) v4si;
+
+extern void bar (int, ...);
+
+void
+foo (int a)
+{
+  v4si b = (v4si){ a, a, a, a };
+  bar (1, b);
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-6.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-attr-6.c	2016-05-11 17:32:39.000000000 +0200
@@ -0,0 +1,24 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler "gnu_attribute 8, 2" } } */
+
+#include <stdarg.h>
+
+typedef int __attribute__((vector_size(16))) v4si;
+
+int
+bar (int n, ...)
+{
+  int ret;
+  v4si a;
+  va_list va;
+
+  va_start (va, n);
+  ret = va_arg (va, v4si)[2];
+  va_end (va);
+
+  return ret;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-single-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-single-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,24 @@
+/* Check calling convention in the vector ABI for single element vectors.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "vlr\t%v24,%v26" 7 } } */
+
+typedef int  __attribute__((vector_size(16))) v4si;
+
+typedef char __attribute__((vector_size(1))) v1qi;
+typedef short int __attribute__((vector_size(2))) v1hi;
+typedef int __attribute__((vector_size(4))) v1si;
+typedef long long __attribute__((vector_size(8))) v1di;
+typedef float __attribute__((vector_size(4))) v1sf;
+typedef double __attribute__((vector_size(8))) v1df;
+typedef long double __attribute__((vector_size(16))) v1tf;
+
+v1qi foo1 (v4si a, v1qi b) { return b; }
+v1hi foo2 (v4si a, v1hi b) { return b; }
+v1si foo3 (v4si a, v1si b) { return b; }
+v1di foo4 (v4si a, v1di b) { return b; }
+v1sf foo5 (v4si a, v1sf b) { return b; }
+v1df foo6 (v4si a, v1df b) { return b; }
+v1tf foo7 (v4si a, v1tf b) { return b; }
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-single-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-single-2.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,12 @@
+/* Check calling convention in the vector ABI for single element vectors.  */
+
+/* { dg-do compile { target { lp64 } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "vlr\t%v24,%v26" 1 } } */
+
+typedef int  __attribute__((vector_size(16))) v4si;
+
+typedef __int128_t __attribute__((vector_size(16))) v1ti;
+
+v1ti foo (v4si a, v1ti b) { return b; }
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-struct-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-struct-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,37 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* c.i and c.j are passed by reference since a struct with two
+   elements is no vector type argument.  */
+/* { dg-final { scan-assembler "ld\t%v\[0-9\]*,0\\(%r3\\)" } } */
+/* { dg-final { scan-assembler "ld\t%v\[0-9\]*,8\\(%r3\\)" } } */
+
+/* just_v2si is passed in a vector reg if it as an incoming arg.
+   However, as return value it is passed via hidden first pointer
+   argument.  */
+/* { dg-final { scan-assembler ".*st.*\t%v\[0-9\]*,0\\(%r2\\)" } } */
+
+/* { dg-final { scan-assembler "gnu_attribute 8, 2" } } */
+
+typedef int __attribute__ ((vector_size(8))) v2si;
+
+struct just_v2si
+{
+  v2si i;
+};
+
+struct two_v2si
+{
+  v2si i, j;
+};
+
+struct just_v2si
+add_structvecs (v2si a, struct just_v2si b, struct two_v2si c)
+{
+  struct just_v2si res;
+
+  res.i = a + b.i + c.i + c.j;
+  return res;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-vararg-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-vararg-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,60 @@
+/* Check calling convention with variable argument lists in the vector
+   ABI.  */
+
+/* { dg-do run { target { s390*-*-* } } } */
+/* { dg-require-effective-target vector } */
+/* { dg-options "-O3 -mzarch -march=z13 --save-temps" } */
+
+/* Make sure arguments are fetched from the argument overflow area.  */
+/* { dg-final { scan-assembler "vl\t%v\[0-9\]*,352\\(%r15\\)" { target lp64 } } } */
+/* { dg-final { scan-assembler "ld\t%v\[0-9\]*,368\\(%r15\\)" { target lp64 } } } */
+/* { dg-final { scan-assembler "vl\t%v\[0-9\]*,376\\(%r15\\)" { target lp64 } } } */
+/* { dg-final { scan-assembler "ld\t%v\[0-9\]*,392\\(%r15\\)" { target lp64 } } } */
+
+/* { dg-final { scan-assembler "vl\t%v\[0-9\]*,208\\(%r15\\)" { target ilp32 } } } */
+/* { dg-final { scan-assembler "ld\t%v\[0-9\]*,224\\(%r15\\)" { target ilp32 } } } */
+/* { dg-final { scan-assembler "vl\t%v\[0-9\]*,232\\(%r15\\)" { target ilp32 } } } */
+/* { dg-final { scan-assembler "ld\t%v\[0-9\]*,248\\(%r15\\)" { target ilp32 } } } */
+
+/* { dg-final { cleanup-saved-temps } } */
+
+#include <stdarg.h>
+
+extern void abort (void);
+
+typedef long long v2di __attribute__((vector_size(16)));
+typedef int v2si __attribute__((vector_size(8)));
+
+v2di __attribute__((noinline))
+add (int a, ...)
+{
+  int i;
+  va_list va;
+  v2di di_result = { 0, 0 };
+  v2si si_result = (v2si){ 0, 0 };
+
+  va_start (va, a);
+
+  di_result += va_arg (va, v2di);
+  si_result += va_arg (va, v2si);
+  di_result += va_arg (va, v2di);
+  si_result += va_arg (va, v2si);
+
+  va_end (va);
+
+  di_result[0] += si_result[0];
+  di_result[1] += si_result[1];
+
+  return di_result;
+}
+
+int
+main ()
+{
+  v2di r = add (4, (v2di){ 11, 21 }, (v2si){ 12, 22 }, (v2di){ 13, 23 }, (v2si){ 14, 24 });
+
+  if (r[0] != 50 || r[1] != 90)
+    abort ();
+
+  return 0;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-abi-vararg-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-abi-vararg-2.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,18 @@
+/* Check calling convention in the vector ABI.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13 -Wno-implicit-function-declaration" } */
+
+
+typedef long v2di __attribute__((vector_size(16)));
+extern v2di foo1 (int, v2di);
+extern v2di foo2 (int, int);
+extern v2di foo3 (int, ...);
+
+v2di bar1 (int a)  { return foo2 (1, a); }
+v2di bar2 (int a)  { return foo3 (1, a); }
+v2di bar3 (v2di a) { return foo1 (1, a); }
+v2di bar4 (v2di a) { return foo3 (1, a); }
+
+int bar5 (int a)  { return foo4 (1, a); }
+int bar6 (v2di a) { return foo4 (1, a); } /* { dg-error "Vector argument passed to unprototyped function" } */
--- gcc/testsuite/gcc.target/s390/vector/vec-clobber-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-clobber-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,38 @@
+/* { dg-do run { target { s390*-*-* } } } */
+/* { dg-require-effective-target vector } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* For FP zero checks we use the ltdbr instruction.  Since this is an
+   load and test it actually writes the FPR.  Whenever an FPR gets
+   written the rest of the overlapping VR is clobbered.  */
+typedef double __attribute__((vector_size(16))) v2df;
+
+v2df a = { 1.0, 2.0 };
+
+extern void abort (void);
+
+void __attribute__((noinline))
+foo (v2df a)
+{
+  v2df b = { 1.0, 3.0 };
+
+  b -= a;
+
+  /* Take away all the VRs not overlapping with FPRs.  */
+  asm volatile ("" : : :
+		"v16","v17","v18","v19",
+		"v20","v21","v22","v23",
+		"v24","v25","v26","v27",
+		"v28","v29","v30","v31");
+  if (b[0] != 0.0) /* ltdbr */
+    abort ();
+  if (b[1] != 1.0)
+    abort ();
+}
+
+int
+main ()
+{
+  foo (a);
+  return 0;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-cmp-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-cmp-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,45 @@
+/* Check that the proper unsigned compare instructions are being generated.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "vchlb" 1 } } */
+/* { dg-final { scan-assembler-times "vchlh" 1 } } */
+/* { dg-final { scan-assembler-times "vchlf" 1 } } */
+/* { dg-final { scan-assembler-times "vchlg" 1 } } */
+
+typedef __attribute__((vector_size(16))) signed char v16qi;
+typedef __attribute__((vector_size(16))) unsigned char uv16qi;
+
+typedef __attribute__((vector_size(16))) signed short v8hi;
+typedef __attribute__((vector_size(16))) unsigned short uv8hi;
+
+typedef __attribute__((vector_size(16))) signed int v4si;
+typedef __attribute__((vector_size(16))) unsigned int uv4si;
+
+typedef __attribute__((vector_size(16))) signed long long v2di;
+typedef __attribute__((vector_size(16))) unsigned long long uv2di;
+
+v16qi
+f (uv16qi a, uv16qi b)
+{
+  return a > b;
+}
+
+v8hi
+g (uv8hi a, uv8hi b)
+{
+  return a > b;
+}
+
+v4si
+h (uv4si a, uv4si b)
+{
+  return a > b;
+}
+
+v2di
+i (uv2di a, uv2di b)
+{
+  return a > b;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-cmp-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-cmp-2.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,38 @@
+/* Check that the proper signed compare instructions are being generated.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "vchb" 1 } } */
+/* { dg-final { scan-assembler-times "vchh" 1 } } */
+/* { dg-final { scan-assembler-times "vchf" 1 } } */
+/* { dg-final { scan-assembler-times "vchg" 1 } } */
+
+typedef __attribute__((vector_size(16))) signed char v16qi;
+typedef __attribute__((vector_size(16))) signed short v8hi;
+typedef __attribute__((vector_size(16))) signed int v4si;
+typedef __attribute__((vector_size(16))) signed long long v2di;
+
+v16qi
+f (v16qi a, v16qi b)
+{
+  return a > b;
+}
+
+v8hi
+g (v8hi a, v8hi b)
+{
+  return a > b;
+}
+
+v4si
+h (v4si a, v4si b)
+{
+  return a > b;
+}
+
+v2di
+i (v2di a, v2di b)
+{
+  return a > b;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-dbl-math-compile-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-dbl-math-compile-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,48 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13 --save-temps" } */
+
+typedef __attribute__((vector_size(16))) double v2df;
+
+v2df
+adddbl (v2df a, v2df b)
+{
+  return a + b;
+}
+/* { dg-final { scan-assembler-times "vfadb" 1 } } */
+
+v2df
+subdbl (v2df a, v2df b)
+{
+  return a - b;
+}
+/* { dg-final { scan-assembler-times "vfsdb" 1 } } */
+
+v2df
+muldbl (v2df a, v2df b)
+{
+  return a * b;
+}
+/* { dg-final { scan-assembler-times "vfmdb" 1 } } */
+
+v2df
+divdbl (v2df a, v2df b)
+{
+  return a / b;
+}
+/* { dg-final { scan-assembler-times "vfd" 1 } } */
+
+v2df
+fmadbl (v2df a, v2df b, v2df c)
+{
+  return a * b + c;
+}
+/* { dg-final { scan-assembler-times "vfma" 1 } } */
+
+v2df
+fmsdbl (v2df a, v2df b, v2df c)
+{
+  return a * b - c;
+}
+/* { dg-final { scan-assembler-times "vfms" 1 } } */
+
+/* { dg-final { cleanup-saved-temps } } */
--- gcc/testsuite/gcc.target/s390/vector/vec-genbytemask-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-genbytemask-1.c	2016-05-11 17:38:00.000000000 +0200
@@ -0,0 +1,83 @@
+/* { dg-do run } */
+/* { dg-options "-O3 -mzarch -march=z13 --save-temps" } */
+/* { dg-require-effective-target vector } */
+/* { dg-require-effective-target int128 } */
+
+typedef unsigned char     uv16qi __attribute__((vector_size(16)));
+typedef unsigned short     uv8hi __attribute__((vector_size(16)));
+typedef unsigned int       uv4si __attribute__((vector_size(16)));
+typedef unsigned long long uv2di __attribute__((vector_size(16)));
+typedef unsigned __int128  uv1ti __attribute__((vector_size(16)));
+
+uv2di __attribute__((noinline))
+foo1 ()
+{
+  return (uv2di){ 0xff00ff00ff00ff00, 0x00ff00ff00ff00ff };
+}
+/* { dg-final { scan-assembler-times "vgbm\t%v24,43605" 1 } } */
+
+uv4si __attribute__((noinline))
+foo2 ()
+{
+  return (uv4si){ 0xff0000ff, 0x0000ffff, 0xffff0000, 0x00ffff00 };
+}
+/* { dg-final { scan-assembler-times "vgbm\t%v24,37830" 1 } } */
+
+uv8hi __attribute__((noinline))
+foo3a ()
+{
+  return (uv8hi){ 0xff00, 0xff00, 0xff00, 0xff00,
+      0xff00, 0xff00, 0xff00, 0xff00 };
+}
+/* { dg-final { scan-assembler-times "vgbm\t%v24,43690" 1 } } */
+
+uv8hi __attribute__((noinline))
+foo3b ()
+{
+  return (uv8hi){ 0x00ff, 0x00ff, 0x00ff, 0x00ff,
+      0x00ff, 0x00ff, 0x00ff, 0x00ff };
+}
+/* { dg-final { scan-assembler-times "vgbm\t%v24,21845" 1 } } */
+
+uv16qi __attribute__((noinline))
+foo4 ()
+{
+  return (uv16qi){ 0xff, 0xff, 0xff, 0xff,
+      0, 0, 0, 0,
+      0xff, 0, 0xff, 0,
+      0, 0xff, 0, 0xff };
+}
+
+uv1ti __attribute__((noinline))
+foo5 ()
+{
+  return (uv1ti){ 0xff00ff00ff00ff00ULL };
+}
+
+/* { dg-final { scan-assembler-times "vgbm\t%v24,61605" 1 } } */
+
+int
+main ()
+{
+  if (foo1()[1] != 0x00ff00ff00ff00ffULL)
+    __builtin_abort ();
+
+  if (foo2()[1] != 0x0000ffff)
+    __builtin_abort ();
+
+  if (foo3a()[1] != 0xff00)
+    __builtin_abort ();
+
+  if (foo3b()[1] != 0x00ff)
+    __builtin_abort ();
+
+  if (foo4()[1] != 0xff)
+    __builtin_abort ();
+
+  if (foo5()[0] != 0xff00ff00ff00ff00ULL)
+    __builtin_abort ();
+
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
--- gcc/testsuite/gcc.target/s390/vector/vec-genbytemask-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-genbytemask-2.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,46 @@
+/* { dg-do compile } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+typedef unsigned char     uv16qi __attribute__((vector_size(16)));
+typedef unsigned short     uv8hi __attribute__((vector_size(16)));
+typedef unsigned int       uv4si __attribute__((vector_size(16)));
+typedef unsigned long long uv2di __attribute__((vector_size(16)));
+
+/* The elements differ.  */
+uv2di __attribute__((noinline))
+foo1 ()
+{
+  return (uv2di){ 0x001fffffffffff00, 0x0000ffffffffff00 };
+}
+
+/* Non-contiguous bitmasks */
+
+uv4si __attribute__((noinline))
+foo2 ()
+{
+  return (uv4si){ 0xff00100f, 0xff00100f, 0xff00100f, 0xff00100f };
+}
+
+uv8hi __attribute__((noinline))
+foo3a ()
+{
+  return (uv8hi){ 0xf700, 0xf700, 0xf700, 0xf700,
+      0xf700, 0xf700, 0xf700, 0xf700 };
+}
+
+uv8hi __attribute__((noinline))
+foo3b ()
+{
+  return (uv8hi){ 0x10ff, 0x10ff, 0x10ff, 0x10ff,
+      0x10ff, 0x10ff, 0x10ff, 0x10ff };
+}
+
+uv16qi __attribute__((noinline))
+foo4 ()
+{
+  return (uv16qi){ 0x82, 0x82, 0x82, 0x82,
+      0x82, 0x82, 0x82, 0x82,
+      0x82, 0x82, 0x82, 0x82,
+      0x82, 0x82, 0x82, 0x82 };
+}
+/* { dg-final { scan-assembler-not "vgbm" } } */
--- gcc/testsuite/gcc.target/s390/vector/vec-genmask-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-genmask-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,70 @@
+/* { dg-do run } */
+/* { dg-options "-O3 -mzarch -march=z13 --save-temps" } */
+/* { dg-require-effective-target vector } */
+
+typedef unsigned char     uv16qi __attribute__((vector_size(16)));
+typedef unsigned short     uv8hi __attribute__((vector_size(16)));
+typedef unsigned int       uv4si __attribute__((vector_size(16)));
+typedef unsigned long long uv2di __attribute__((vector_size(16)));
+
+uv2di __attribute__((noinline))
+foo1 ()
+{
+  return (uv2di){ 0x000fffffffffff00, 0x000fffffffffff00 };
+}
+/* { dg-final { scan-assembler-times "vgmg\t%v24,12,55" 1 } } */
+
+uv4si __attribute__((noinline))
+foo2 ()
+{
+  return (uv4si){ 0xff00000f, 0xff00000f, 0xff00000f, 0xff00000f };
+}
+/* { dg-final { scan-assembler-times "vgmf\t%v24,28,7" 1 } } */
+
+uv8hi __attribute__((noinline))
+foo3a ()
+{
+  return (uv8hi){ 0xfff0, 0xfff0, 0xfff0, 0xfff0,
+      0xfff0, 0xfff0, 0xfff0, 0xfff0 };
+}
+/* { dg-final { scan-assembler-times "vgmh\t%v24,0,11" 1 } } */
+
+uv8hi __attribute__((noinline))
+foo3b ()
+{
+  return (uv8hi){ 0x0fff, 0x0fff, 0x0fff, 0x0fff,
+      0x0fff, 0x0fff, 0x0fff, 0x0fff };
+}
+/* { dg-final { scan-assembler-times "vgmh\t%v24,4,15" 1 } } */
+
+uv16qi __attribute__((noinline))
+foo4 ()
+{
+  return (uv16qi){ 0x8, 0x8, 0x8, 0x8,
+      0x8, 0x8, 0x8, 0x8,
+      0x8, 0x8, 0x8, 0x8,
+      0x8, 0x8, 0x8, 0x8 };
+}
+/* { dg-final { scan-assembler-times "vgmb\t%v24,4,4" 1 } } */
+
+int
+main ()
+{
+  if (foo1()[1] != 0x000fffffffffff00ULL)
+    __builtin_abort ();
+
+  if (foo2()[1] != 0xff00000f)
+    __builtin_abort ();
+
+  if (foo3a()[1] != 0xfff0)
+    __builtin_abort ();
+
+  if (foo3b()[1] != 0x0fff)
+    __builtin_abort ();
+
+  if (foo4()[1] != 0x8)
+    __builtin_abort ();
+  return 0;
+}
+
+/* { dg-final { cleanup-saved-temps } } */
--- gcc/testsuite/gcc.target/s390/vector/vec-genmask-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-genmask-2.c	2016-05-11 17:38:00.000000000 +0200
@@ -0,0 +1,55 @@
+/* { dg-do compile } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+/* { dg-require-effective-target int128 } */
+
+typedef unsigned char     uv16qi __attribute__((vector_size(16)));
+typedef unsigned short     uv8hi __attribute__((vector_size(16)));
+typedef unsigned int       uv4si __attribute__((vector_size(16)));
+typedef unsigned long long uv2di __attribute__((vector_size(16)));
+typedef unsigned __int128  uv1ti __attribute__((vector_size(16)));
+
+/* The elements differ.  */
+uv2di __attribute__((noinline))
+foo1 ()
+{
+  return (uv2di){ 0x000fffffffffff00, 0x0000ffffffffff00 };
+}
+
+/* Non-contiguous bitmasks */
+
+uv4si __attribute__((noinline))
+foo2 ()
+{
+  return (uv4si){ 0xff00100f, 0xff00100f, 0xff00100f, 0xff00100f };
+}
+
+uv8hi __attribute__((noinline))
+foo3a ()
+{
+  return (uv8hi){ 0xf700, 0xf700, 0xf700, 0xf700,
+      0xf700, 0xf700, 0xf700, 0xf700 };
+}
+
+uv8hi __attribute__((noinline))
+foo3b ()
+{
+  return (uv8hi){ 0x10ff, 0x10ff, 0x10ff, 0x10ff,
+      0x10ff, 0x10ff, 0x10ff, 0x10ff };
+}
+
+uv16qi __attribute__((noinline))
+foo4 ()
+{
+  return (uv16qi){ 0x82, 0x82, 0x82, 0x82,
+      0x82, 0x82, 0x82, 0x82,
+      0x82, 0x82, 0x82, 0x82,
+      0x82, 0x82, 0x82, 0x82 };
+}
+
+/* We do not have vgmq.  */
+uv1ti
+foo5()
+{
+  return (uv1ti){ ((unsigned __int128)1 << 53) - 1 };
+}
+/* { dg-final { scan-assembler-not "vgm" } } */
--- gcc/testsuite/gcc.target/s390/vector/vec-init-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-init-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,68 @@
+/* Check that the vec_init expander does its job.  */
+
+/* { dg-do compile } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+
+
+
+
+typedef __attribute__((vector_size(16))) signed int v4si;
+
+extern v4si G;
+
+v4si
+f (signed int a)
+{
+  return G == a;
+}
+/* { dg-final { scan-assembler-times "vrepf" 1 } } */
+
+v4si
+g (signed int *a)
+{
+  return G == *a;
+}
+/* { dg-final { scan-assembler-times "vlrepf" 1 } } */
+
+v4si
+h ()
+{
+  return G == 1;
+}
+/* { dg-final { scan-assembler-times "vgmf\t%v.*,31,31" 1 } } */
+
+v4si
+i ()
+{
+  return G == -1;
+}
+/* { dg-final { scan-assembler-times "vone" 1 } } */
+
+v4si
+j ()
+{
+  return G == 0;
+}
+/* { dg-final { scan-assembler-times "vzero" 1 } } */
+
+v4si
+k ()
+{
+  return G == (v4si){ 0xff80, 0xff80, 0xff80, 0xff80 };
+}
+/* { dg-final { scan-assembler-times "vgmf\t%v.*,16,24" 1 } } */
+
+v4si
+l ()
+{
+  return G == (v4si){ 0xf000000f, 0xf000000f, 0xf000000f, 0xf000000f };
+}
+/* { dg-final { scan-assembler-times "vgmf\t%v.*,28,3" 1 } } */
+
+v4si
+m ()
+{
+  return G == (v4si){ 0x00ff00ff, 0x0000ffff, 0xffff0000, 0xff00ff00 };
+}
+/* { dg-final { scan-assembler-times "vgbm\t%v.*,21450" 1 } } */
--- gcc/testsuite/gcc.target/s390/vector/vec-int-math-compile-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-int-math-compile-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,40 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+typedef __attribute__((vector_size(16))) signed int v4si;
+
+v4si
+adddbl (v4si a, v4si b)
+{
+  return a + b;
+}
+
+v4si
+subdbl (v4si a, v4si b)
+{
+  return a - b;
+}
+
+v4si
+muldbl (v4si a, v4si b)
+{
+  return a * b;
+}
+
+v4si
+divdbl (v4si a, v4si b)
+{
+  return a / b;
+}
+
+v4si
+fmadbl (v4si a, v4si b, v4si c)
+{
+  return a * b + c;
+}
+
+v4si
+fmsdbl (v4si a, v4si b, v4si c)
+{
+  return a * b - c;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-scalar-cmp-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-scalar-cmp-1.c	2016-05-11 17:12:28.000000000 +0200
@@ -0,0 +1,49 @@
+/* Check that we use the scalar variants of vector compares.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "wfcedbs\t%v\[0-9\]*,%v0,%v2" 2 } } */
+/* { dg-final { scan-assembler-times "wfchdbs\t%v\[0-9\]*,%v0,%v2" 1 } } */
+/* { dg-final { scan-assembler-times "wfchedbs\t%v\[0-9\]*,%v2,%v0" 1 } } */
+/* { dg-final { scan-assembler-times "wfchdbs\t%v\[0-9\]*,%v2,%v0" 1 } } */
+/* { dg-final { scan-assembler-times "wfchedbs\t%v\[0-9\]*,%v2,%v0" 1 } } */
+/* { dg-final { scan-assembler-times "locrne" 5 } } */
+/* { dg-final { scan-assembler-times "locrno" 1 } } */
+
+
+int
+eq (double a, double b)
+{
+  return a == b;
+}
+
+int
+ne (double a, double b)
+{
+  return a != b;
+}
+
+int
+gt (double a, double b)
+{
+  return a > b;
+}
+
+int
+ge (double a, double b)
+{
+  return a >= b;
+}
+
+int
+lt (double a, double b)
+{
+  return a < b;
+}
+
+int
+le (double a, double b)
+{
+  return a <= b;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-shift-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-shift-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,108 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "veslb" 2 } } */
+/* { dg-final { scan-assembler-times "veslh" 2 } } */
+/* { dg-final { scan-assembler-times "veslf" 2 } } */
+/* { dg-final { scan-assembler-times "veslg" 2 } } */
+
+/* { dg-final { scan-assembler-times "vesrab" 1 } } */
+/* { dg-final { scan-assembler-times "vesrah" 1 } } */
+/* { dg-final { scan-assembler-times "vesraf" 1 } } */
+/* { dg-final { scan-assembler-times "vesrag" 1 } } */
+
+/* { dg-final { scan-assembler-times "vesrlb" 1 } } */
+/* { dg-final { scan-assembler-times "vesrlh" 1 } } */
+/* { dg-final { scan-assembler-times "vesrlf" 1 } } */
+/* { dg-final { scan-assembler-times "vesrlg" 1 } } */
+
+/* { dg-final { scan-assembler-times "veslvb" 2 } } */
+/* { dg-final { scan-assembler-times "veslvh" 2 } } */
+/* { dg-final { scan-assembler-times "veslvf" 2 } } */
+/* { dg-final { scan-assembler-times "veslvg" 2 } } */
+
+/* { dg-final { scan-assembler-times "vesravb" 1 } } */
+/* { dg-final { scan-assembler-times "vesravh" 1 } } */
+/* { dg-final { scan-assembler-times "vesravf" 1 } } */
+/* { dg-final { scan-assembler-times "vesravg" 1 } } */
+
+/* { dg-final { scan-assembler-times "vesrlvb" 1 } } */
+/* { dg-final { scan-assembler-times "vesrlvh" 1 } } */
+/* { dg-final { scan-assembler-times "vesrlvf" 1 } } */
+/* { dg-final { scan-assembler-times "vesrlvg" 1 } } */
+
+typedef __attribute__((vector_size(16))) signed char v16qi;
+typedef __attribute__((vector_size(16))) unsigned char uv16qi;
+
+typedef __attribute__((vector_size(16))) signed short v8hi;
+typedef __attribute__((vector_size(16))) unsigned short uv8hi;
+
+typedef __attribute__((vector_size(16))) signed int v4si;
+typedef __attribute__((vector_size(16))) unsigned int uv4si;
+
+typedef __attribute__((vector_size(16))) signed long long v2di;
+typedef __attribute__((vector_size(16))) unsigned long long uv2di;
+
+uv16qi g_uvqi0, g_uvqi1, g_uvqi2;
+v16qi g_vqi0, g_vqi1, g_vqi2;
+
+uv8hi g_uvhi0, g_uvhi1, g_uvhi2;
+v8hi g_vhi0, g_vhi1, g_vhi2;
+
+uv4si g_uvsi0, g_uvsi1, g_uvsi2;
+v4si g_vsi0, g_vsi1, g_vsi2;
+
+uv2di g_uvdi0, g_uvdi1, g_uvdi2;
+v2di g_vdi0, g_vdi1, g_vdi2;
+
+void
+shift_left_by_scalar (int s)
+{
+  g_uvqi0 = g_uvqi1 << s;
+  g_vqi0 = g_vqi1 << s;
+  g_uvhi0 = g_uvhi1 << s;
+  g_vhi0 = g_vhi1 << s;
+  g_uvsi0 = g_uvsi1 << s;
+  g_vsi0 = g_vsi1 << s;
+  g_uvdi0 = g_uvdi1 << s;
+  g_vdi0 = g_vdi1 << s;
+}
+
+void
+shift_right_by_scalar (int s)
+{
+  g_uvqi0 = g_uvqi1 >> s;
+  g_vqi0 = g_vqi1 >> s;
+  g_uvhi0 = g_uvhi1 >> s;
+  g_vhi0 = g_vhi1 >> s;
+  g_uvsi0 = g_uvsi1 >> s;
+  g_vsi0 = g_vsi1 >> s;
+  g_uvdi0 = g_uvdi1 >> s;
+  g_vdi0 = g_vdi1 >> s;
+}
+
+void
+shift_left_by_vector ()
+{
+  g_uvqi0 = g_uvqi1 << g_uvqi2;
+  g_vqi0 = g_vqi1 << g_vqi2;
+  g_uvhi0 = g_uvhi1 << g_uvhi2;
+  g_vhi0 = g_vhi1 << g_vhi2;
+  g_uvsi0 = g_uvsi1 << g_uvsi2;
+  g_vsi0 = g_vsi1 << g_vsi2;
+  g_uvdi0 = g_uvdi1 << g_uvdi2;
+  g_vdi0 = g_vdi1 << g_vdi2;
+}
+
+void
+shift_right_by_vector ()
+{
+  g_uvqi0 = g_uvqi1 >> g_uvqi2;
+  g_vqi0 = g_vqi1 >> g_vqi2;
+  g_uvhi0 = g_uvhi1 >> g_uvhi2;
+  g_vhi0 = g_vhi1 >> g_vhi2;
+  g_uvsi0 = g_uvsi1 >> g_uvsi2;
+  g_vsi0 = g_vsi1 >> g_vsi2;
+  g_uvdi0 = g_uvdi1 >> g_uvdi2;
+  g_vdi0 = g_vdi1 >> g_vdi2;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-sub-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-sub-1.c	2016-05-11 17:12:20.000000000 +0200
@@ -0,0 +1,51 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+/* { dg-final { scan-assembler-times "vsb" 2 } } */
+/* { dg-final { scan-assembler-times "vsh" 2 } } */
+/* { dg-final { scan-assembler-times "vsf" 2 } } */
+/* { dg-final { scan-assembler-times "vsg" 2 } } */
+/* { dg-final { scan-assembler-times "vfs" 1 } } */
+
+
+typedef unsigned char     uv16qi __attribute__((vector_size(16)));
+typedef signed char        v16qi __attribute__((vector_size(16)));
+typedef unsigned short     uv8hi __attribute__((vector_size(16)));
+typedef signed short        v8hi __attribute__((vector_size(16)));
+typedef unsigned int       uv4si __attribute__((vector_size(16)));
+typedef signed int          v4si __attribute__((vector_size(16)));
+typedef unsigned long long uv2di __attribute__((vector_size(16)));
+typedef signed long long    v2di __attribute__((vector_size(16)));
+typedef double              v2df __attribute__((vector_size(16)));
+
+uv16qi g_uvqi0, g_uvqi1, g_uvqi2;
+v16qi g_vqi0, g_vqi1, g_vqi2;
+
+uv8hi g_uvhi0, g_uvhi1, g_uvhi2;
+v8hi g_vhi0, g_vhi1, g_vhi2;
+
+uv4si g_uvsi0, g_uvsi1, g_uvsi2;
+v4si g_vsi0, g_vsi1, g_vsi2;
+
+uv2di g_uvdi0, g_uvdi1, g_uvdi2;
+v2di g_vdi0, g_vdi1, g_vdi2;
+
+v2df g_vdf0, g_vdf1, g_vdf2;
+
+void
+sub1 ()
+{
+  g_vqi0 = g_vqi1 - g_vqi2;
+  g_uvqi0 = g_uvqi1 - g_uvqi2;
+
+  g_vhi0 = g_vhi1 - g_vhi2;
+  g_uvhi0 = g_uvhi1 - g_uvhi2;
+
+  g_vsi0 = g_vsi1 - g_vsi2;
+  g_uvsi0 = g_uvsi1 - g_uvsi2;
+
+  g_vdi0 = g_vdi1 - g_vdi2;
+  g_uvdi0 = g_uvdi1 - g_uvdi2;
+
+  g_vdf0 = g_vdf1 - g_vdf2;
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-vcond-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-vcond-1.c	2016-05-11 18:08:10.000000000 +0200
@@ -0,0 +1,23 @@
+/* A const vector operand is forced into a register in
+   s390_expand_vcond.
+   This testcase once failed because the target mode (v2di) was picked
+   for the reg instead of the mode of the other comparison
+   operand.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13" } */
+
+typedef __attribute__((vector_size(16))) long   v2di;
+typedef __attribute__((vector_size(16))) double v2df;
+
+v2di
+foo (v2df a)
+{
+  return a == (v2df){ 0.0, 0.0 };
+}
+
+v2di
+bar (v2df a)
+{
+  return (v2df){ 1.0, 1.0 } == (v2df){ 0.0, 0.0 };
+}
--- gcc/testsuite/gcc.target/s390/vector/vec-vrepi-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/vector/vec-vrepi-1.c	2016-05-11 17:41:29.000000000 +0200
@@ -0,0 +1,58 @@
+/* { dg-do run } */
+/* { dg-options "-O3 -mzarch -march=z13 --save-temps" } */
+/* { dg-require-effective-target vector } */
+
+typedef unsigned char     uv16qi __attribute__((vector_size(16)));
+typedef unsigned short     uv8hi __attribute__((vector_size(16)));
+typedef unsigned int       uv4si __attribute__((vector_size(16)));
+typedef unsigned long long uv2di __attribute__((vector_size(16)));
+
+uv2di __attribute__((noinline))
+foo1 ()
+{
+  return (uv2di){ 0x7f0f, 0x7f0f };
+}
+/* { dg-final { scan-assembler-times "vrepig\t%v24,32527" 1 } } */
+
+uv4si __attribute__((noinline))
+foo2 ()
+{
+  return (uv4si){ 0x7f0f, 0x7f0f, 0x7f0f, 0x7f0f };
+}
+/* { dg-final { scan-assembler-times "vrepif\t%v24,32527" 1 } } */
+
+uv8hi __attribute__((noinline))
+foo3 ()
+{
+  return (uv8hi){ 0x7f0f, 0x7f0f, 0x7f0f, 0x7f0f,
+      0x7f0f, 0x7f0f, 0x7f0f, 0x7f0f };
+}
+/* { dg-final { scan-assembler-times "vrepih\t%v24,32527" 1 } } */
+
+uv16qi __attribute__((noinline))
+foo4 ()
+{
+  return (uv16qi){ 0x77, 0x77, 0x77, 0x77,
+      0x77, 0x77, 0x77, 0x77,
+      0x77, 0x77, 0x77, 0x77,
+      0x77, 0x77, 0x77, 0x77 };
+}
+/* { dg-final { scan-assembler-times "vrepib\t%v24,119" 1 } } */
+
+int
+main ()
+{
+  if (foo1()[1] != 0x7f0f)
+    __builtin_abort ();
+
+  if (foo2()[1] != 0x7f0f)
+    __builtin_abort ();
+
+  if (foo3()[1] != 0x7f0f)
+    __builtin_abort ();
+
+  if (foo4()[1] != 0x77)
+    __builtin_abort ();
+
+  return 0;
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-dbl-math-compile-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-dbl-math-compile-1.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,67 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector --save-temps" } */
+
+/* { dg-final { scan-assembler-times "vfcedb\t" 1 } } */
+/* { dg-final { scan-assembler-times "vfchdb\t" 2 } } */
+/* { dg-final { scan-assembler-times "vfchedb\t" 2 } } */
+
+/* { dg-final { scan-assembler-times "vfcedbs\t" 2 } } */
+/* { dg-final { scan-assembler-times "vfchdbs\t" 2 } } */
+
+/* { dg-final { cleanup-saved-temps } } */
+
+#include <vecintrin.h>
+
+vector bool long long
+cmpeq (vector double a, vector double b)
+{
+  return vec_cmpeq (a, b); /* vfcedb */
+}
+
+vector bool long long
+cmpgt (vector double a, vector double b)
+{
+  return vec_cmpgt (a, b); /* vfchdb */
+}
+
+vector bool long long
+cmpge (vector double a, vector double b)
+{
+  return vec_cmpge (a, b); /* vfchedb */
+}
+
+vector bool long long
+cmplt (vector double a, vector double b)
+{
+  return vec_cmplt (a, b); /* vfchdb */
+}
+
+vector bool long long
+cmple (vector double a, vector double b)
+{
+  return vec_cmple (a, b); /* vfchedb */
+}
+
+int
+all_eq (vector double a, vector double b)
+{
+  return vec_all_eq (a, b);
+}
+
+int
+any_eq (vector double a, vector double b)
+{
+  return vec_any_eq (a, b);
+}
+
+int
+all_lt (vector double a, vector double b)
+{
+  return vec_all_lt (a, b);
+}
+
+int
+any_lt (vector double a, vector double b)
+{
+  return vec_any_lt (a, b);
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-elem-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-elem-1.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,11 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector" } */
+
+/* { dg-final { scan-assembler "nilf\t%r2,15" } } */
+/* { dg-final { scan-assembler "vlgvb" } } */
+
+signed char
+foo(unsigned char uc)
+{
+  return __builtin_s390_vec_extract((__vector signed char){ 0 }, uc);
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-genbytemask-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-genbytemask-1.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,21 @@
+/* { dg-do compile } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector" } */
+
+#include <vecintrin.h>
+
+
+vector unsigned char a, b, c, d;
+
+int
+foo ()
+{
+  a = vec_genmask (0);
+  b = vec_genmask (65535);
+  c = vec_genmask (43605);
+  d = vec_genmask (37830);
+}
+
+/* { dg-final { scan-assembler-times "vzero" 1 } } */
+/* { dg-final { scan-assembler-times "vone" 1 } } */
+/* { dg-final { scan-assembler-times "vgbm\t%v.*,43605" 1 } } */
+/* { dg-final { scan-assembler-times "vgbm\t%v.*,37830" 1 } } */
--- gcc/testsuite/gcc.target/s390/zvector/vec-genmask-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-genmask-1.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,24 @@
+/* { dg-do compile } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector" } */
+
+#include <vecintrin.h>
+
+
+vector unsigned int a, b, c, d, e, f;
+
+int
+foo ()
+{
+  a = vec_genmasks_32 (0, 31);
+  b = vec_genmasks_32 (0, 0);
+  c = vec_genmasks_32 (31, 31);
+  d = vec_genmasks_32 (5, 5);
+  e = vec_genmasks_32 (31, 0);
+  f = vec_genmasks_32 (6, 5);
+}
+/* { dg-final { scan-assembler-times "vone" 1 } } */
+/* { dg-final { scan-assembler-times "vgmf\t%v.*,0,0" 1 } } */
+/* { dg-final { scan-assembler-times "vgmf\t%v.*,31,31" 1 } } */
+/* { dg-final { scan-assembler-times "vgmf\t%v.*,5,5" 1 } } */
+/* { dg-final { scan-assembler-times "vgmf\t%v.*,31,0" 1 } } */
+/* { dg-final { scan-assembler-times "vone" 1 } } */
--- gcc/testsuite/gcc.target/s390/zvector/vec-lcbb-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-lcbb-1.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,31 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector" } */
+
+/* { dg-final { scan-assembler-times "\tlcbb\t" 4 } } */
+
+#include <vecintrin.h>
+
+/* CC will be extracted into a GPR and returned.  */
+int
+foo1 (void *ptr)
+{
+  return __lcbb (ptr, 64);
+}
+
+int
+foo2 (void *ptr)
+{
+  return __lcbb (ptr, 128) > 16;
+}
+
+int
+foo3 (void *ptr)
+{
+  return __lcbb (ptr, 256) == 16;
+}
+
+int
+foo4 (void *ptr)
+{
+  return __lcbb (ptr, 512) < 16;
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-load_bndry-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-load_bndry-1.c	2016-05-11 17:34:31.000000000 +0200
@@ -0,0 +1,80 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O0 -mzarch -march=z13 -mzvector" } */
+
+#include <vecintrin.h>
+
+signed char
+foo64 (signed char *p)
+{
+  return vec_load_bndry (p, 64)[0];
+  /* { dg-final { scan-assembler-times "\tvlbb\t%v..?,0\\(%r..?\\),0" 1 } } */
+}
+
+signed char
+foo128 (signed char *p)
+{
+  return
+    vec_load_bndry (p, 128)[0]
+    + vec_load_bndry (p + 16, 128)[0];
+  /* { dg-final { scan-assembler-times "\tvlbb\t%v..?,0\\(%r..?\\),1" 2 } } */
+}
+
+signed char
+foo256 (signed char *p)
+{
+  return
+    vec_load_bndry (p, 256)[0]
+    + vec_load_bndry (p + 16, 256)[0]
+    + vec_load_bndry (p + 32, 256)[0];
+  /* { dg-final { scan-assembler-times "\tvlbb\t%v..?,0\\(%r..?\\),2" 3 } } */
+}
+
+signed char
+foo512 (signed char *p)
+{
+  return
+    vec_load_bndry (p, 512)[0]
+    + vec_load_bndry (p + 16, 512)[0]
+    + vec_load_bndry (p + 32, 512)[0]
+    + vec_load_bndry (p + 48, 512)[0];
+  /* { dg-final { scan-assembler-times "\tvlbb\t%v..?,0\\(%r..?\\),3" 4 } } */
+}
+
+signed char
+foo1024 (signed char *p)
+{
+  return
+    vec_load_bndry (p, 1024)[0]
+    + vec_load_bndry (p + 16, 1024)[0]
+    + vec_load_bndry (p + 32, 1024)[0]
+    + vec_load_bndry (p + 48, 1024)[0]
+    + vec_load_bndry (p + 64, 1024)[0];
+  /* { dg-final { scan-assembler-times "\tvlbb\t%v..?,0\\(%r..?\\),4" 5 } } */
+}
+
+signed char
+foo2048 (signed char *p)
+{
+  return
+    vec_load_bndry (p, 2048)[0]
+    + vec_load_bndry (p + 16, 2048)[0]
+    + vec_load_bndry (p + 32, 2048)[0]
+    + vec_load_bndry (p + 48, 2048)[0]
+    + vec_load_bndry (p + 64, 2048)[0]
+    + vec_load_bndry (p + 80, 2048)[0];
+  /* { dg-final { scan-assembler-times "\tvlbb\t%v..?,0\\(%r..?\\),5" 6 } } */
+}
+
+signed char
+foo4096 (signed char *p)
+{
+  return
+    vec_load_bndry (p, 4096)[0]
+    + vec_load_bndry (p + 16, 4096)[0]
+    + vec_load_bndry (p + 32, 4096)[0]
+    + vec_load_bndry (p + 48, 4096)[0]
+    + vec_load_bndry (p + 64, 4096)[0]
+    + vec_load_bndry (p + 80, 4096)[0]
+    + vec_load_bndry (p + 96, 4096)[0];
+  /* { dg-final { scan-assembler-times "\tvlbb\t%v..?,0\\(%r..?\\),6" 7 } } */
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-overloading-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-overloading-1.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,77 @@
+/* Test whether overloading works as expected.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-march=z13 -mzarch -mzvector -fdump-tree-original" } */
+
+__vector int var_v4si;
+__vector unsigned var_uv4si;
+__vector bool var_bv4si;
+__vector long long var_v2di;
+__vector unsigned long long var_uv2di;
+__vector bool long long var_bv2di;
+__vector double var_v2df;
+
+int *intptr;
+unsigned *uintptr;
+double *dblptr;
+unsigned long long ull;
+const int *cintptr;
+long long* llptr;
+unsigned long long* ullptr;
+
+typedef __vector int v4si;
+typedef __vector unsigned int uv4si;
+
+v4si var2_v4si;
+uv4si var2_uv4si;
+
+void
+foo ()
+{
+  __builtin_s390_vec_scatter_element (var_v4si,  var_uv4si, intptr, (unsigned long long)0);
+  __builtin_s390_vec_scatter_element (var2_v4si, var2_uv4si, intptr, (unsigned long long)0);
+  __builtin_s390_vec_scatter_element (var_bv4si, var_uv4si, uintptr, (unsigned long long)0);
+  __builtin_s390_vec_scatter_element (var_uv4si, var_uv4si, uintptr, (unsigned long long)0);
+  __builtin_s390_vec_scatter_element (var_v2di,  var_uv2di, llptr, (unsigned long long)0);
+  __builtin_s390_vec_scatter_element (var_bv2di, var_uv2di, ullptr, (unsigned long long)0);
+  __builtin_s390_vec_scatter_element (var_uv2di, var_uv2di, ullptr, (unsigned long long)0);
+  __builtin_s390_vec_scatter_element (var_v2df,  var_uv2di, dblptr, (unsigned long long)0);
+
+  /* While the last argument is a int there is a way to convert it to
+     unsigned long long, so this variant is supposed to match.  */
+ __builtin_s390_vec_scatter_element (var_v4si,  var_uv4si, intptr, 0);
+
+  __builtin_s390_vec_insert_and_zero (intptr);
+  __builtin_s390_vec_insert_and_zero (cintptr);
+
+  __builtin_s390_vec_promote ((signed char)1, 1);
+  __builtin_s390_vec_promote ((unsigned char)1, 1);
+  __builtin_s390_vec_promote ((short int)1, 1);
+  __builtin_s390_vec_promote ((unsigned short int)1, 1);
+  __builtin_s390_vec_promote ((int)1, 1);
+  __builtin_s390_vec_promote ((unsigned)1, 1);
+  __builtin_s390_vec_promote ((long long)1, 1);
+  __builtin_s390_vec_promote ((unsigned long long)1, 1);
+  __builtin_s390_vec_promote ((double)1, 1);
+
+  /* This is supposed to match vec_promote_s32 */
+  __builtin_s390_vec_promote (1, (signed char) -1);
+
+  /* Constants in C usually are considered int.  */
+  __builtin_s390_vec_promote (1, 1);
+
+  /* And (unsigned) long if they are too big for int.  */
+  __builtin_s390_vec_promote (1ULL << 32, 1);
+  __builtin_s390_vec_promote (1LL << 32, 1);
+}
+
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vscef " 5 "original" } } */
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vsceg " 4 "original" } } */
+
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vllezf " 2 "original" } } */
+
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vlvgb_noin " 2 "original" } } */
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vlvgh_noin " 2 "original" } } */
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vlvgf_noin " 4 "original" } } */
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vlvgg_noin " 4 "original" } } */
+/* { dg-final { scan-tree-dump-times "__builtin_s390_vlvgg_dbl_noin " 1 "original" } } */
--- gcc/testsuite/gcc.target/s390/zvector/vec-overloading-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-overloading-2.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,54 @@
+/* Test whether overloading works as expected.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-march=z13 -mzarch -mzvector" } */
+
+__vector int v4si;
+__vector unsigned uv4si;
+__vector bool bv4si;
+__vector long long v2di;
+__vector unsigned long long uv2di;
+__vector bool long long bv2di;
+__vector double v2df;
+int *intptr;
+unsigned *uintptr;
+double *dblptr;
+long long ll;
+unsigned long long ull;
+const int *cintptr;
+long long* llptr;
+unsigned long long* ullptr;
+
+void
+foo ()
+{
+  __builtin_s390_vec_scatter_element (v4si,  uv4si, (int*)0, 0); /* ok */
+  __builtin_s390_vec_insert_and_zero (intptr); /* ok */
+
+  /* The unsigned pointer must not match the signed pointer.  */
+  __builtin_s390_vec_scatter_element (v4si, uv4si, uintptr, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  /* Make sure signed int pointers don't match unsigned int pointers.  */
+  __builtin_s390_vec_scatter_element (bv4si, uv4si, intptr, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  /* Const pointers do not match unqualified operands.  */
+  __builtin_s390_vec_scatter_element (v4si, uv4si, cintptr, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  /* Volatile pointers do not match unqualified operands.  */
+  __builtin_s390_vec_scatter_element (v4si, uv4si, cintptr, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  /* The third operands needs to be double *.  */
+  __builtin_s390_vec_scatter_element (v2df, uv4si, intptr, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  /* This is an ambigious overload.  */
+  __builtin_s390_vec_scatter_element (v4si, uv4si, 0, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  /* Pointer to vector must not match.  */
+  __builtin_s390_vec_scatter_element (v4si, uv4si, &v4si, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  /* Don't accept const int* for int*.  */
+  __builtin_s390_vec_scatter_element (v4si,  uv4si, cintptr, 0); /* { dg-error "invalid parameter combination for intrinsic" } */
+
+  __builtin_s390_vec_load_pair (ll, ull); /* { dg-error "ambiguous overload for intrinsic" } */
+  __builtin_s390_vec_load_pair (ull, ll); /* { dg-error "ambiguous overload for intrinsic" } */
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-overloading-3.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-overloading-3.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,19 @@
+/* Check for error messages supposed to be issued during overloading.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-march=z13 -mzarch -mzvector" } */
+
+__vector int v4si;
+__vector unsigned uv4si;
+
+int *intptr;
+unsigned long long ull;
+const unsigned int *ucintptr;
+
+void
+foo ()
+{
+  /* A backend check makes sure the forth operand is a literal.  */
+  __builtin_s390_vec_gather_element (uv4si, uv4si, ucintptr, 256); /* { dg-error "constant argument 4 for builtin.*is out of range for target type" } */
+  __builtin_s390_vec_gather_element (uv4si, uv4si, ucintptr, 5); /* { dg-error "constant argument 4 for builtin.*is out of range" } */
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-overloading-4.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-overloading-4.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,18 @@
+/* Check for error messages supposed to be issued during builtin expansion.  */
+
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-march=z13 -mzarch -mzvector" } */
+
+__vector int v4si;
+__vector unsigned uv4si;
+
+int *intptr;
+unsigned long long ull;
+const unsigned int *ucintptr;
+
+void
+foo ()
+{
+  /* A backend check makes sure the forth operand is a literal.  */
+  __builtin_s390_vec_scatter_element (v4si, uv4si, intptr, ull); /* { dg-error "constant value required for builtin" } */
+}
--- gcc/testsuite/gcc.target/s390/zvector/vec-splat-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-splat-1.c	2016-05-11 17:41:24.000000000 +0200
@@ -0,0 +1,42 @@
+/* { dg-do compile } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector" } */
+
+#include <vecintrin.h>
+
+vector signed char v16qi;
+vector short       v8hi;
+vector int         v4si;
+vector long long   v2di;
+
+vector unsigned char      uv16qi;
+vector unsigned short     uv8hi;
+vector unsigned int       uv4si;
+vector unsigned long long uv2di;
+
+int
+foo ()
+{
+  v16qi  = vec_splats ((signed char)0x77);
+  uv16qi = vec_splats ((unsigned char)0x77);
+
+  v8hi  = vec_splats ((short int)0x7f0f);
+  uv8hi = vec_splats ((unsigned short int)0x7f0f);
+
+  v4si  = vec_splats ((int)0x7f0f);
+  uv4si = vec_splats ((unsigned int)0x7f0f);
+
+  v2di  = vec_splats ((long long)0x7f0f);
+  uv2di = vec_splats ((unsigned long long)0x7f0f);
+}
+
+/* { dg-final { scan-assembler-times "vrepib\t%v.*,119" 1 } } */
+/* { dg-final { scan-assembler-times "vrepib\t%v.*,119" 1 } } */
+
+/* { dg-final { scan-assembler-times "vrepih\t%v.*,32527" 1 } } */
+/* { dg-final { scan-assembler-times "vrepih\t%v.*,32527" 1 } } */
+
+/* { dg-final { scan-assembler-times "vrepif\t%v.*,32527" 1 } } */
+/* { dg-final { scan-assembler-times "vrepif\t%v.*,32527" 1 } } */
+
+/* { dg-final { scan-assembler-times "vrepig\t%v.*,32527" 1 } } */
+/* { dg-final { scan-assembler-times "vrepig\t%v.*,32527" 1 } } */
--- gcc/testsuite/gcc.target/s390/zvector/vec-splat-2.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-splat-2.c	2016-05-11 17:53:39.000000000 +0200
@@ -0,0 +1,42 @@
+/* { dg-do compile } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector" } */
+
+#include <vecintrin.h>
+
+vector signed char v16qi;
+vector short       v8hi;
+vector int         v4si;
+vector long long   v2di;
+
+vector unsigned char      uv16qi;
+vector unsigned short     uv8hi;
+vector unsigned int       uv4si;
+vector unsigned long long uv2di;
+
+int
+foo ()
+{
+  v16qi  = vec_splat_s8 (-112);
+  uv16qi = vec_splat_u8 (215);
+
+  v8hi  = vec_splat_s16 (-32000);
+  uv8hi = vec_splat_u16 (64000);
+
+  v4si  = vec_splat_s32 (-32000);
+  uv4si = vec_splat_u32 (64000);
+
+  v2di  = vec_splat_s64 (-32000);
+  uv2di = vec_splat_u64 (64000);
+}
+
+/* { dg-final { scan-assembler-times "vrepib\t%v.*,-112" 1 } } */
+/* { dg-final { scan-assembler-times "vrepib\t%v.*,-41" 1 } } */
+
+/* { dg-final { scan-assembler-times "vrepih\t%v.*,-32000" 1 } } */
+/* { dg-final { scan-assembler-times "vrepih\t%v.*,-1536" 1 } } */
+
+/* { dg-final { scan-assembler-times "vrepif\t%v.*,-32000" 1 } } */
+/* { dg-final { scan-assembler-times "vrepif\t%v.*,-1536" 1 } } */
+
+/* { dg-final { scan-assembler-times "vrepig\t%v.*,-32000" 1 } } */
+/* { dg-final { scan-assembler-times "vrepig\t%v.*,-1536" 1 } } */
--- gcc/testsuite/gcc.target/s390/zvector/vec-test-mask-1.c	1970-01-01 01:00:00.000000000 +0100
+++ gcc/testsuite/gcc.target/s390/zvector/vec-test-mask-1.c	2016-05-11 17:30:06.000000000 +0200
@@ -0,0 +1,25 @@
+/* { dg-do compile { target { s390*-*-* } } } */
+/* { dg-options "-O3 -mzarch -march=z13 -mzvector" } */
+
+/* { dg-final { scan-assembler-times "vtm" 2 } } */
+/* { dg-final { scan-assembler-times "ipm" 1 } } */
+
+#include <vecintrin.h>
+
+/* CC will be extracted into a GPR and returned.  */
+int
+foo (vector unsigned int a, vector unsigned b)
+{
+  return vec_test_mask (a, b);
+}
+
+extern void baz (void);
+
+/* In that case the ipm/srl is supposed to optimized out by
+   combine/s390_canonicalize_comparison.  */
+int
+bar (vector unsigned int a, vector unsigned b)
+{
+  if (vec_test_mask (a, b) == 2)
+    baz ();
+}
--- gcc/testsuite/lib/target-supports.exp	2015-06-18 16:32:16.000000000 +0200
+++ gcc/testsuite/lib/target-supports.exp	2016-05-11 17:32:08.000000000 +0200
@@ -3800,7 +3800,8 @@ proc check_effective_target_vect_natural
         verbose "check_effective_target_vect_natural_alignment: using cached result" 2
     } else {
         set et_vect_natural_alignment_saved 1
-        if { [check_effective_target_arm_eabi] } {
+        if { [check_effective_target_arm_eabi]
+	     || [istarget s390*-*-*] } {
             set et_vect_natural_alignment_saved 0
         }
     }
